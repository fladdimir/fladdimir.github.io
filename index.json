[{"content":"Hurrying up right before an intersection just to narrowly miss the green light by a few seconds is one of the annoying things which regularly happen when cycling in the city.\nWouldn\u0026rsquo;t it be just great to know in advance whether a green phase may be catched, and to avoid useless acceleration and waiting phases?\nOver the last years, the city of Hamburg (Germany) has put some significant work into their Urban Data Platform and the geo-portal, improving the accessibility of various data sources.\nIn particular, the Traffic Lights Data Hamburg beta API provides access to real-time signal data of traffic-light systems for a large number of intersections in the city center.\nSo let\u0026rsquo;s see what we can get out of that data, and whether we may ease some pain of the daily ride.\nGreen-light optimized speed advisory The core idea of green-light optimized speed advisory (\u0026ldquo;GLOSA\u0026rdquo;) is to reduce unnecessary stops and accelerations by calculating reasonable movement speeds based on expected future green-light periods of upcoming traffic-lights.\nRecently, a couple of implementations have been developed in different cities.\nSome examples:\nGevas launched their app traffic pilot which is available i.a. in Duesseldorf, Frankfurt, Salzburg and Kassel Audi provides a GLOSA traffic assistant for their cars in selected cities (i.a. Ingolstadt, Duesseldorf, and New York (US)) the PrioBike project in Hamburg includes the PrioBike app as well as the PrioBike-Säule (see photo below; a fixed post comprised of a sensor and display to show speed advice to cyclists passing by).\nThe PrioBike app is developed by the TU Dresden and is currently (06/23) in closed beta (unfortunately, my registration mail has not been answered yet, however, an official statement confirms a planned release later in 2023).\nThe fixed post has been developed together with the company yunex traffic. Unfortunately, currently (06/2023) none of the app solutions is publically available in Hamburg.\nOne \u0026ldquo;PrioBike-Säule\u0026rdquo; has been installed close to the Hamburg-Dammtor train station, and judging from limited own experience it seems to work pretty well!\n(3 out of 4 times it actually showed a fully correct recommendation to catch the next green traffic-light. Sadly, it\u0026rsquo;s not part of my regular cycling routes.)\nAvailable traffic-lights data in Hamburg The geo-portal is an interactive map which can visualize some of the data made available by the city of Hamburg.\nThe screenshot below shows real-time traffic-light data of the intersection Fruchtallee / Doormannsweg:\nColors indicate the current signal of traffic-lights of different lanes.\nCorresponding data is provided for about 800 of 1770 traffic-lights in Hamburg:\nThe data is made available on https://tld.iot.hamburg.de/v1.1 via the standard OGC SensorThings API which is described in detail e.g. here (usage guide).\nThis paper gives an excellent overview on the system architecture and underlying infrastructure.\nThe map below shows a part of the route I used to cycle to work as well as markers for all traffic-lights for which real-time data is currently available:\nOverall, 13 out of 21 traffic-lights on the route provide real-time data (some are currently disabled due to ongoing construction work, and some may just be too small to be prioritized when planning corresponding IoT equipment. Nevertheless, having speed recommendations for almost two thirds of the encountered traffic-lights could already be a nice thing.)\nTo give an impression of provided traffic-light real-time data, the following bokeh-chart shows some historical data for the relevant primary signal (south -\u0026gt; north) of the above-mentioned intersection Fruchtallee / Doormannsweg (the most westward on the route, with ID 813_19, Datastream-ID 50850), on a typical morning (Friday, 19.05.2023, 6:00-8:00 UTC):\n(\u0026gt; interactive chart)\nThe y-axis shows the duration of a phase over time (x-axis). Blue indicates the length of complete cycles (red-to-red), and is calculated from the sum of the durations of the different phases, drawn in their corresponding colors. Dashed lines indicate average values over the time-period (arithmetic mean).\nOn average, a complete cycle took about 90s and the traffic-light showed red about 63s, green for about 23s, and the transitions (amber / red-amber) only account for 3s and 1s respectively.\nThat makes sense, given that the analyzed traffic-light is pointing towards the lanes going from south to north, while the majority of (car-)traffic at that time can be expected to go from west to east towards the city center.\nThe observations show a number of larger deviations from the mean, with green phases being almost twice as long as normal (over 40s, at 06:30 and 07:20), followed by accordingly shorter red phases (only slightly longer than 40s).\nThese deviations may be explained by a dynamic traffic-lights control system, which may extend certain phase durations e.g. to account for bus signal requests (\u0026ldquo;bus priorization\u0026rdquo;), or when detecting extraordinarily high volumes of car traffic going into a specific direction.\nAccording to a small sample of own observations, recent data is online available via the API in about 1-2s after a signal phase change.\nIn addition to the described real-time traffic-light data for car signals, the API also provides further information e.g. on car detectors, bus signal requests, pedestrian signals, and pedestrian signal requests.\nProof-of-Concept: A simple GLOSA web-app Given quality and quantity of the available data, it does not seem impossible to imagine using it to forecast relevant future signal-phase changes and to calculate reasonable movement speeds from that information.\nBelow, a small application is sketched which allows a mobile web-client to request speed recommendations:\nThe main flow of information looks as follows:\na client requests a speed-advice, submitting its current location, movement speed, and trip-id based on the trip-id the server can lookup the predefined route and determine the next upcoming traffic-light the latest signal-phase change times can be queried from the traffic-lights-data API (encapsulated by a locally running proxy-service) based on the received observations, a forecast of future phase changes can be created (also outsourced to a dedicated service) the forecast is used to determine the movement speed necessary to catch the next reachable green phase and that information is returned to the client whenever the client location updates, a new GLOSA-request is sent (see step 1., limited to at most 1 request per second) For the proof-of-concept, the initial route configuration requires some manual effort for importing a defined route (in form of a .gpx file), and for identifying relevant traffic-lights (IDs) with help of the geo-portal.\nThe next screenshot shows a minimal web UI which provides information about the current location of the device, the next traffic-light (currently showing red), as well as the speed recomendation (\u0026ldquo;slow down, its not getting green anytime soon..\u0026rdquo;):\nA couple of seconds later, the signal switched to green, and the current speed is sufficient to catch the phase:\nGPS-independent testing To facilitate manual testing without being reliant on actual GPS signals, the UI allows for the simulation of movements by dragging the user marker on the map:\nThere should have been a video here but your browser does not seem to support it. For manual testing, the current speed and the accuracy are set to 20 km/h and 10m.\nUsed technologies The client is built with React Material UI components, react-router, Leaflet for map display and interaction (via react-leaflet), and the browser Geolocation API (consent and secure context required).\nSince the Geolocation API does not work in the background a wakelock can be requested.\nAt the server-side, nginx provides basic authentication, ssl-termination, serves static frontend files, and forwards /api-requests.\nA \u0026ldquo;main backend service\u0026rdquo; is used for central calculations and the \u0026ldquo;orchestration\u0026rdquo; of auxiliary services, built with Spring Boot (which recently released the amazing support for compiling to a native-image, even though some stumbling blocks seem to remain).\nAccess to the latest traffic-light data is implemented using express running on node.js.\nFuture phase change times are forecasted based on the naive expectation of average-length phases, implemented in Python with pandas and exposed via flask.\nFor simple testing during development, the public localtunnel service can be used. This way, a server running at home can be accessed e.g. via mobile network (although they recently had to add some annoying password inquiry on their consent page).\nThe repository is available on github.\nGPS accuracy and \u0026ldquo;route snapping\u0026rdquo; One of the interesting aspects relates to the fact that the position the client detects is always somehow offset from the predefined route, be it due to measurement inaccuracies (commonly up to a couple of meters when using GPS), or inaccurate route definitions.\nConsider the following part of a piece-wise linear route R1-R4 and the corresponding measurement L:\nL indicates the current location sent by the client device, moving along the predefined route R1-R2-R3-R4. To give a speed recommendation, the next traffic-light (TL1 at R4) needs to be identified, and the correct distance along the given route must be calculated.\nTo determine the current location on the route, potential \u0026ldquo;snap locations\u0026rdquo; (SL1=R1, SL2, SL3) are evaluated by calculating the distance to the closest point on each segment (SL2 in the example above). The performed \u0026ldquo;route snapping\u0026rdquo; of the sent location is similar to (although simpler than) what some maps APIs offer, e.g. OSRMs match service.\nTo speed up the calculation, each segment is assumed to be linear, so that the JTS Topology Suite Java library\u0026rsquo;s DistanceOp can be used (with converted Mercator-coordinates as a fast simplification).\nPerformance considerations As the sketched system relies on frequent client-server-communication (~1 request per client per second), performant request handling is of high importance.\nFor basic analysis of server-side request processing, the open telemetry project provides standard tooling which makes it easy to gather tracing information. This is especially interesting since a client request causes multiple requests to different backend services (database, traffic-lights data API, forecasting service).\nInstrumentation support for automatic trace collection and forwarding to a tracing analysis backend are available for many popular languages / frameworks. For the main Java service, the Java instrumentation agent is used, which injects bytecode so that tracing information is automatically collected for popular frameworks / libraries without any code changes. Similar auto-instrumentation functionality is available for the node.js based traffic-lights data API proxy service, and for the Python-based forecast-service.\nThe following screenshot shows the processing of a single GLOSA-request:\nThe complete POST to the main backend (locationtracking-0.0.1-SNAPSHOT) takes 105ms to complete, starting with some database accesses (first ~10ms), followed by a long-running 69ms call to the traffic-lights data API proxy service (tld_api), and a 14ms call to the forecasting service (tld_forecast). As we can see, the main share of the overall request processing time is caused by the call to the external traffic-lights data API (68ms), out of which creating the TCP connection and completing the TLS handshake alone require 39ms.\nTo improve performance and efficiency of the external data API access, we can use a second feature of the sensorthings API. Apart from the normal HTTP-API to access historical data of traffic-light signal phase changes, a (websocket-based) MQTT-API allows to subscribe to new updates of specific traffic-lights. This way, we can extend the functionality of our API proxy service as follows:\nan initial request of the last N signal phase change timestamps causes a regular HTTP request to the external API to fetch the required observations (which are then returned to the client) furthermore, the received data is cached, and an MQTT-subscription for future updates is created each received update is used to update the cache (evicting the oldest entry) each following client-request for the same traffic-light data can now immediately be answered with locally cached data, without causing any external API request after a configurable time without any client requests for data of a specific traffic-light (e.g. 60s), its subscription is canceled and its cache is cleared (a following client request is again causing an initial external request (1.)) The following screenshot shows the processing of a second client request for the same traffic-light data:\nAs we can see, the request to the tld_api is processed within 3ms, without any external call.\nAssuming that data is requested over the last 1km when approaching a traffic-light, going with 20km/h (3 min travel time), and a frequency of 1 request per second, this means that 180 client requests only cause 1 initial external request and 179 subsequent requests are answered from the cache which is updated by push notifications received via MQTT. Consequently, the average request processing time for clients can be reduced by more than 50% (68/105ms), and the load for the external API server can be drastically reduced (even more when assuming multiple clients request the same traffic-light data).\nAs a further optimization, a server-push technology may be used to avoid polling and to allow for immediate client-notifications on the arrival of new data which causes changes to the last pushed recommendation.\nAs a disadvantage, the API proxy service now occupies a long-running MQTT-subscription / web-socket connection to the external API server (which should not cause much performance problems, as modern web-servers / MQTT brokers can be expected to easily handle large numbers of concurrent of connections, even on commodity hardware). Additionally, a subscription is kept alive some time after the last client request occured (as we cannot know whether there will be more), so that a couple of updates can be expected to be received unnecessarily - however, that disadvantage should be more than counterbalanced by the amount of saved external requests.\nApart from the external request handling, further optimization possibilities include database access and calculations at the stateless main backend service. Instead of loading the predefined route from the database on each client request, corresponding accesses can also be cached, e.g. using Springs built-in caching capabilities. As there is currently no way to modify predefined routes, their immutability avoids the need for cache invalidation. Even though there were no detailed measurements, it seems that a couple of milliseconds of request processing time may be saved this way.\nTo reduce the calculation effort of the \u0026ldquo;route snapping\u0026rdquo;, the index of the last reached route-segment is submitted back to the client as part of the response, so that this information can be included into a following request. This way, the number of checked route-segments can be limited, based on the assumption that a client is not expected to move backwards on a route. Additionally, checking further upcoming route-segments can be aborted once the closest distance to the current location stops to decrease (or does increase over a threshold), so that checking all upcoming route-segments can be avoided (at the expense of possibly missing an even closer segment). Other optimizations may include reducing the number of segments on a route by combining adjacent segments with (almost) identical direction into a single segment.\nWhile the described calculation optimizations may be relevant for longer routes with more segments (i.a. GPS waypoints), there was no measurable difference on the test route when reducing from 522 to 310 waypoints.\nExperimental evaluation To be able to quantify the effects of using the GLOSA app, the geolocation and speed data sent with each request is persisted for analysis.\nThe following chart shows the speed profile of a bike ride to work without following the GLOSA speed recommendations:\nThe trip had a duration of 00:32:16, and during this time 1,937 samples were collected (almost exactly one GPS update per second).\nThe theoretical distance measured on a map is 10.7 km, the total sum of the distances between all sampled locations is 11.1 km (slightly longer, probably due to GPS inaccuracies and seemingly \u0026ldquo;less straight\u0026rdquo; movements).\nGiven the duration and (theoretical) distance of the trip, an average speed of 5.53 m/s (19.90 km/h) can be calculated. This is close to the average speed calculated from the 1,937 speed samples (5.38 m/s), so the measurements seem to be quite accurate.\nThe maximum measured speed was 9.30 m/s (33.47 km/h), the share of samples with a measured speed below 1 km/h (likely stops) was 10.8%.\nThe next screenshot shows a trip when trying to follow the GLOSA recommendations:\nTo my disappointment, the share of speed measurements below 1 km/h is still 9.2% - a difference of 1.6 percentage points which may not be interpreted to be of any significance.\nObviously, any serious evaluation should be based on a larger number of trips. Furthermore, the experiment itself was probably also negatively influenced by existing personal experience on the common phase lengths. (Infact, some traffic lights along the route are already quite well-known, so that no external GLOSA recommendation is needed. Any serious evaluation should of course be performed on routes which are unknown to the testers.)\nWhile I did not analyse any more specific data, it seems that the effect of using the GLOSA app is negatively influenced by:\nbad traffic light data for some traffic lights (the \u0026ldquo;real-time\u0026rdquo; data is so old that no meaningful predications can be made) badly correlated traffic lights (no matter how slow/quick you try to go, between some traffic lights stops are almost inevitable; the phases seem to be optimized for a limited number of cars driving 50 km/h) \u0026ldquo;Bedarfsampeln\u0026rdquo; (one traffic light on the route only switches to green upon manual triggering, thus also causing unavoidable stops) bad predictions (forecasts purely based on historical data tend to be rather inaccurate, given the dynamic traffic control with adaptive phase lengths) Especially the last point causes a particularly frustrating experience in case a green phase could actually have easily been catched when not following a recommendation.\nConclusion While digging into the topic, it was quite surprising how and what data is made publically available by the Urban Data Platform of Hamburg. It\u0026rsquo;s good to see the municipality investing into the digitalization of the traffic-light infrastructure, and also into an IT infrastructure which may make large-scale applications possible.\nConcerning GLOSA applications, the practical benefit really depends on the decisive details - predicting the correct phase change times down to the second is a crucial aspect which seems quite hard to get right in this dynamic environment. Nevertheless, I am looking forward to the PrioBike app and may give it a try after its publically released (hopefully open-source).\nPotential extensions and improvements Based on the sketched proof-of-concept, numerous improvements are imaginable.\nStarting with the annoying manual setup step to define routes and to identify relevant traffic-light signal head IDs, the identification step could be automated as shown by the PrioBike-app developers from the TU Dresden in their selector implementation.\nA main challenge relates to the prediction of future signal phase changes, which could e.g. be extended by incorporating already available data such as bus signal requests.\nAnother potential for improvement relates to recommendation algorithms, which could e.g. include information on current/past riding speeds, as well as multiple upcoming traffic-lights (maybe the traffic-light right after the next one will be missed anyway).\nFinally - to generally improve the cycling experience and the quality of life in the city as a whole - other measures like optimizing traffic-light controls towards an average speed of 20-25 km/h, as well as simply extending the cycling lanes can obviously be expected to have a much greater impact than any GLOSA application.\n","permalink":"https://fladdimir.github.io/post/glosa/","summary":"A web-app to reduce traffic-light waiting time on the daily bike ride to work","title":"Green-Light Optimized Speed Advisory for Cycling in Hamburg"},{"content":"Automated testing has become an essential part of software development. Good test support is a core feature of many modern application frameworks, and there is rarely a debate about whether tests should be written or not, but rather on how and which tests should be written.\nHowever, there is also a reality of some existing systems having few - if any - automated tests in place. While this might not be a problem per se, a degrading understanding of a system can make it harder to confidently make necessary changes - up to a point where no one dares to touch anything anymore, in fear of not noticing that a change breaks existing functionality. Having a suite of automated tests in place can greatly reduce that risk and thereby enable the continuous evolution of the system.\nTo explore this in some more detail, three questions will be discussed in the following:\nWhat are properties that automated tests should have in order to maximize the value they provide over the lifetime of a system? How can systems be designed to facilitate the creation of useful tests? What can be done if a system is not (yet) designed in a way which allows to accomodate such tests? Most of the aspects reflected on below are described in-depth i.a. in the following books:\nWorking Effectively with Legacy Code, M. C. Feathers, 2005 - an often-recommended classic motivating why to create automated tests, and how to make it possible [Feathers, 2005] Software Engineering at Google, T. Winters, T. Manshreck, and H. Wright, 2020 - devoting several chapters to the way tests are used in that huge software organization [Winters et al., 2020] Clean Architecture, R. C. Martin, 2017 - containing general advice on how to structure (testable) systems [Martin, 2017] Object-Oriented Reengineering Patterns, S. Demeyer, S. Ducasse, O. Nierstrasz, 2003 - a collection of approaches for restructuring existing systems, showing ways to improve the design, and describing strategies to use and grow test-suites [Demeyer et al., 2003] 1. Working with legacy code Most relevant software lives for long periods of time and the continuous adaption to new requirements is a relevant challenge.\nWith growing size, age, and complexity of a system, it seems not to be unusual for even seemingly simple changes to take longer and longer to be implemented, and to carry increasing risks of breaking existing functionality. Of course, this holds especially true for systems which are developed just relying on \u0026ldquo;working-with-care\u0026rdquo; (sometimes also known as \u0026ldquo;professionalism\u0026rdquo;). When it comes to making further changes, [Feathers, 2005] describes developers to then \u0026ldquo;edit and pray\u0026rdquo; that the change does not break anything, building confidence upon their experience with the system and some exploratory manual testing.\nObviously this only gets riskier, once different people start to extend different parts of the system, and once experienced contributors leave the team while new developers join. Slowly, the architectural vision of the design starts to blur, and the understanding of how the systems works gets lost up to the point where no one really knows anymore what is going on. Needless to say, that just continues to increase the risk of breaking changes (ultimately, how could one do any regression testing when not even knowing at all how the system is supposed to behave?).\nAs an alternative way, Feathers describes an approach of covering existing functionality with automated tests, providing a \u0026ldquo;safety net\u0026rdquo;, which allows for controlled refactorings as well as the controlled addition of new features or fixes. This way, the chance to break existing functionality is reduced and all developers (experienced or not) are provided with confidence into the correctness of the software.\nSo how could those automated tests look like?\n2. Properties of useful automated tests [Winters et al., 2020] mention different dimensions to classify automated tests:\nscope (describing the amount of validated code) narrow (e.g. class, or even single function) medium (e.g. multiple classes) large (e.g. system / end-to-end tests, verifying the interaction of sub-systems) size (describing the amount of resources the test needs) small (the test and its dependencies all run inside a single thread) medium (everything runs locally, in different processes, network calls to localhost, or file-system access are allowed) large (calls to external systems are allowed) Fast and deterministic The execution time of a test is typically determined by its size, and the larger the test the more flaky it tends to be, since e.g. network calls to external systems may timeout (flakiness - the extent to which a test tends to fail sometimes, not being caused by any actually problematic code change).\nTo keep a test suite fast and deterministic, it is recommended to rely on a majority of small tests whenever possible (not leaving room for any flakiness, and even providing the option for simple parallel execution by multiple threads). Furthermore, [Winters et al., 2020] emphasize the fact that only fast, small tests are practical to be run as part of the normal development workflow, and experience shows that longer-running tests tend to be not executed. However, the importance of larger tests is also acknowledged in order to cover aspects that small tests cannot verify.\nRobust and maintainable When it comes to scope, there are different aspects to keep in mind:\nOn the one hand, narrow-scoped tests (e.g. testing a single implementation class) usually allow for a quick and precise analysis of the root-cause in the event of failure. On the other hand, narrow-scoped tests tend to be brittle (they break on unrelated changes), since a simple redistribution of some logic between collaborating classes can already cause a lot of test failures. [Winters et al., 2020] recommend to test business-relevant behaviour via the public API instead of directly depending on implementation details, to avoid frequently changing the tests (\u0026ldquo;Don\u0026rsquo;t depend on volatile things!\u0026rdquo; - [Martin, 2017] even recommends to use a dedicated testing API to shield tests from changing implementation details).\nPure refactorings should not break tests, and doing so may indicate an innapropriate level of abstraction (test behaviour - not methods/classes). The same holds true for changes that introduce new features or fix bugs, which also should not require an adjustment of existing tests. So with respect to scope, following these recommendations may typically result in testing at least several related classes together.\nAnother interesting aspect of scope is that it is defined by the amount of validated code, as opposed to executed. In particular, [Winters et al., 2020] argue that - if possible - a test should stick with the real implementations of the dependencies of the tested code, instead of replacing them with test doubles by default (preferring classical over mockist testing):\n\u0026ldquo;Using real implementations can cause your test to fail if there is a bug in the real implementation. This is good! You want your tests to fail in such cases because it indicates that your code won’t work properly in production.\u0026rdquo;\nThis is especially true when the dependency itself is not properly tested on its own.\nObviously, dependencies on things running outside the test thread (e.g. external services, databases) must be replaced by test doubles in order to keep the test small. Here, [Winters et al., 2020] prefer the usage of lightweight fake implementations over mocks to be able to test state instead of interactions.\nWith respect to scope, [Feathers, 2005] also mentions the importance of narrow-scoped tests, i.a. because of the simplicity with which failure causes can be located.\nWhile I certainly also like this property, I also fear that it tends to motivate me to slavishly create tests for each and every single class, leading to the brittleness problems described above. More often than not it should be easy to spot the root cause of a problem even among a couple of collaborators. Nevertheless, this does of course not mean that I would oppose occasionally making some pure function package-private to be able to quickly test some nasty regex in isolation.\n3. How to make code untestable While it may sound just great to have a blazingly fast, deterministic, robust, and maintainable test suite in place, it really needs to be kept in mind that this must not be an afterthought during development. If not carefully taken into account, it is surprisingly easy to end up with code which makes it incredibly hard to add any useful (fast, small) automated tests ex-post (an endavour hard to describe other than painful).\nConsider the following Java sample:\n@Service class CalculationServiceImpl implements CalculationService { /** * @return true, if successful */ @Override public boolean calculate(int input) { Result result = new FirstCalculator().calculateFirstPart(input); // 1. SecondProcessor.calculateSecondPart(result, input); // 2. SessionContext.store(\u0026#34;myResult1\u0026#34;, result); // 3. boolean isSuccessful = result.getValue32() == 13; // 4. if (isSuccessful) { ThirdProcessor.calculateThirdPart(); // 5. NotificationService.sendKafkaMessageToCalculationsTopic(); // 6. } return isSuccessful; } } class FirstCalculator { Result calculateFirstPart(int input) { int baseValue = new MariaDbDatabaseAccess().getBaseValue(); // 1.1 // [...] some calculation logic return result; } } class SecondProcessor { static void calculateSecondPart(Result result, int input) { int extraInfo = CalculationHelperWebService.getExtraInfo(); // 2.1 // [...] some calculation logic } } class ThirdProcessor { static void calculateThirdPart() { Result result = (Result) (SessionContext.get(\u0026#34;myResult1\u0026#34;)); // 5.1 // [...] some calculation logic } } // [...] various other classes Imagine that we\u0026rsquo;d want to write a test for the calculate functionality of the CalculationService (public API).\nWe notice that the actual calculation logic seems to be distributed over at least three different collaborator classes, but the distribution seems rather arbitrarily. In order to avoid creating a brittle test which is broken by the first upcoming refactoring, we avoid the temptation to test any of the three calculation parts in isolation.\nnew FirstCalculator().calculateFirstPart(input)\nFirst, there is a call to a collaborating class (FirstCalculator), which in turn collects some additional information from a database (1.1):\nint baseValue = new MariaDbDatabaseAccess().getBaseValue();\nThis may already become a first problem, since there is no simple way to replace the MariaDB with a lightweight fake implementation, so that we would need to either run the complete test against a real MariaDB (making the test larger), or resort to advanced mocking library features (which are hopefully available).\nSecondProcessor.calculateSecondPart(result, input);\nThen the collaborator SecondProcessor is called, which first fetches some needed information from a remote service via network before running its calculations (2.1):\nint extraInfo = CalculationHelperWebService.getExtraInfo();\nAgain, replacing this static call may require advanced magic to be available (again also including possibly surprising side-effects in case the static mock is not properly cleaned up at the end of the test).\nSessionContext.store(\u0026quot;myResult1\u0026quot;, result);\nThis shows another possibly hard-to-replace static call that depends on some framework-provided (thread-local?) session state to be setup, so that later parts of the calculation logic can then retrieve that state (e.g. 5.1 and 6). Relying on side-effects like this is just a great option to create a sufficiently confusing data flow which does not ease creating relevant test cases in general.\nWhy would one ever resort to using some sort of SessionContext at all here? Well, at least it allowed us to share information between different places without having been required to refactor much existing logic which would have been risky to touch..\nboolean isSuccessful = result.getValue32() == 13;\nAt 4. the orchestrating logic of the calculate method is suspended by some core business logic, breaking with the abstraction level of the method. Apart from making it harder to understand the now even more widespread calculation logic, this does not really hinder creating the test. It merely serves as an example of needed refactoring and is probably again a symptom of missing tests, since it was probably too risky to put it elsewhere in the first place. This also illustrates the importance of testing via stable interfaces (public API), since implementation details such as the distribution of logic among collaborating classes may be likely to change - especially in systems where missing tests prevented adding new features at the right places.\nFinally, 5. just represents some dependency on the SessionContext, and 6. another hard-coded dependency on external systems. Obviously the overall sample is kept short and the length of realistic methods won\u0026rsquo;t contribute a lot to make writing tests any easier.\nSumming up some general problems which may add up over time and result in hard-to-test code:\nhard-wired dependencies to external systems (hard to fake/mock) dependencies hidden deep in the core business logic behavior relying on side-effects and arcane features of the used framework mangling different aspects and abstraction levels, just to avoid changing code at other places 4. Design for testability As shown in the previous part, testability should be a key aspect during system design/implementation. So how can code be structured to allow for useful tests?\nProviding Seams The Seam is a central concept of [Feathers, 2005], described as \u0026ldquo;a place to alter behavior without editing that place\u0026rdquo;. In particular, Object Seams are recommended, i.e. providing places to allow replacing problematic dependencies with subtypes.\nDependency injection (DI) is a central functionality of many popular frameworks such as Spring or Quarkus (just to give some Java examples), and both DI containers allow to replace existing bindings of (problematic) implementation classes with mocks or own fake implementations. However, when using constructor injection it is also simply possible to construct instances of the classes under test by hand, without relying on any DI framework functionality. Of course, this manual construction has the disadvantage of having to manually create the complete dependency graph, but may speed up test execution significantly.\nFor the problematic FirstCalculator above, a better testable version could look like this:\nclass FirstCalculator { private final MariaDbDatabaseAccess dbAccess; // constructor allows to provide fake/mock dependencies FirstCalculator(MariaDbDatabaseAccess dbAccess) { this.dbAccess = dbAccess; } Result calculateFirstPart(int input) { int baseValue = dbAccess.getBaseValue(); // [...] some calculation logic return result; } } This way, a fake-implementation of MariaDbDatabaseAccess could be provided (subclassing it and overriding problematic behavior).\nAdditionally, the CalculationServiceImpl would also need to offer a seam by providing a constructor allowing to inject the FirstCalculator instance with the faked database access dependency:\n@Service class CalculationServiceImpl implements CalculationService { private final FirstCalculator firstCalculator; // constructor allows to provide test-specific dependencies CalculationServiceImpl(FirstCalculator firstCalculator) { this.firstCalculator = firstCalculator; } @Override public boolean calculate(int input) { Result result = firstCalculator.calculateFirstPart(input); // [...] } } (Luckily, lombok may save us at least some of this constructor boilerplate.)\nClean architecture [Martin, 2017] describes clean architecture, a general approach to structure a system so that central business logic is properly separated from external dependencies. The main idea is also at the core of similar concepts like hexagonal architecture or onion architecture.\nIn particular, a Dependency Rule is formulated which states that:\n\u0026ldquo;Source code dependencies must point only inward, toward higher-level policies.\u0026rdquo;\nWhen core business logic needs to invoke functionality from outer layers, this dependency must be inverted (Dependency Inversion Principle), so that the source-code dependency still only points inward, opposing the flow of control.\nConsider the sample above:\nA cleaner version of the code sample shown above could be realized as follows, replacing the low-level MariaDB dependency of the core logic with the abstract interface BaseValueProvider:\n// core business logic class FirstCalculator { private final BaseValueProvider baseValueProvider; // abstract dependency // constructor allows to provide fake/mock dependencies FirstCalculator(BaseValueProvider baseValueProvider) { this.baseValueProvider = baseValueProvider; } Result calculateFirstPart(int input) { // core logic directly invokes functionality from outer layers, // but has no source-code dependency int baseValue = baseValueProvider.getBaseValue(); // [...] some calculation logic return result; } interface BaseValueProvider { // part of the core business logic int getBaseValue(); // abstract functionality needed by the core logic } } // --- // implementation detail, outside the core logic class MariaDbDatabaseAccess implements BaseValueProvider { @Override public int getBaseValue() { // [...] actual MariaDB access logic } } As a result, central business logic can be kept independent from external influences and low-level details, be it frameworks, UI, or databases. This does not only make it simpler to replace a specific database or UI technology when a popular new one emerges, but also makes it simple to provide seams which allow to swap out problematic dependencies with mocks or lightweight fake implementations. Consequently, the creation of small, useful automated tests is facilitated.\nWhen implementing clean architectures it may also be helpful to enforce the dependency rule e.g. in Java with help of ArchUnit tests.\nSidenote: on the over-usage of Java interfaces Another, somehow related and sometimes observed problem in Java is a general tendency towards the over-usage of interfaces. Consider the sample introduced above:\n@Service class CalculationServiceImpl implements CalculationService { @Override public boolean calculate(int input) { // [...] } } Imagine the calculate method being called exclusively by some CalculationRestController upon an HTTP-request initiated by user interaction. In that case, there would not be any problem having a direct dependency from the CalculationRestController (low-level detail) towards the CalculationServiceImpl (core business logic). Infact, we might just be able reduce some clutter by removing the useless CalculationService interface as well as the annoying Impl postfix. In many cases using interfaces is not required and should be a conscious decision rather than the default.\n5. Reengineering for testability Given some system that was not designed with clean architecture and useful automated tests in mind, how can we still get there? [Demeyer et al., 2003] give a number of useful recommendations on reengineering, i.e. on how to restructure systems in an improved form.\nOne chapter touches on the question of which parts of a system to prioritize. Understandibly, reengineering efforts should not focus on stable, flawlessly working parts, but rather on the faulty ones, which require change and suffer the worst from reliance on outdated technologies, developer fluctuation, insufficient documentation, duplicated code, or tangled structure.\nStarting with the most problematic parts, the core business functionalities as well as dependencies and auxiliary functions need to be analyzed to identify a cleaner, more testable target design as well as the corresponding target scope of useful automated tests.\nIn order to safely make the necessary code changes (e.g. introducing seams to break and invert dependencies), [Demeyer et al., 2003] recommend to incrementally introduce tests for the parts of the system which are changed.\nHowever, isn\u0026rsquo;t there the chicken-and-egg problem of already requiring tests to safely do changes which only enable the creation of tests?\nYes, that\u0026rsquo;s what [Feathers, 2005] calls the legacy code dilemma. To alleviate this problem, a two-step approach can be taken:\nstart with larger-sized tests which allow to keep as many dependencies in place as possible, minimizing the amount of necessary code changes to create the tests refactor the covered code so that creating small tests of the core business logic becomes feasible First, this may e.g. involve running tests against an existing database or other external services. Even though running the larger tests may take time and will be subject to flakiness, it will still provide the necessary safety net to incrementally move towards a cleaner design with faster tests.\n[Demeyer et al., 2003] advise to start with black-box tests of big abstractions, focusing on business values, instead of individual sub-components. In particular, one recommendation is to record business rules as tests, aiming to represent core functionality by a set of canonical examples with well-defined actions and clear, observable results. Since covering all rules may not be feasible (depending on their number and the runtime of the larger tests), it is suggested to start with essential cases. The 80/20 rule may apply here as well, maybe 80% of production cases only exercise 20% of the business logic?.\nHaving the larger-sized tests in place, the necessary refactorings can be done to break the problematic dependencies and introduce a cleaner architecture. Subsequently, the implemented test scenarios of the larger-sized tests can be nicely reused to create fast-running small tests, swapping out problematic dependencies with leightweight fake implementations. Furthermore, more small tests can be added to further increase the amount of covered business rules.\nWhile the small tests can be run quickly and often as part of the local development workflow, the larger-sized tests should still be run on a regular basis in order to verify the functionality against the actual dependencies. (Drawing from own experience, errors seem to come as often from self-developed business logic as from unexpected behavior of dependencies - be it caused by actual bugs or just by unclear documentation.)\nSumming up To be fast and not flaky, automated tests must be small in size (running inside a single thread). To be maintainable and not brittle, automated tests should test through stable interfaces (public API), focusing on business requirements instead of being too narrow-scoped. This leaves room to freely refactor the internal implementation. Being able to build useful automated tests requires conscious management of source code dependencies. This needs to be kept in mind when designing and implementing a system. Adding useful automated tests to a grown system ex-post can be a laborious - yet worthwile - endavour, which may benefit from first creating larger-sized tests as an intermediate step towards fast and deterministic smaller tests. ","permalink":"https://fladdimir.github.io/post/testing-legacy-code/","summary":"On useful automated tests and how to get there when not developing a system from scratch","title":"Testing Legacy Software"},{"content":"Did you ever have the feeling that you know for sure that you did read about something, but you just cannot remember where? Sounds like it would be great to be able to index and search some stuff using a full-text search-engine..\nSimple design Consider the following simple design for an application\npersisting an uploaded document in an object-storage, saving metadata to a database, and indexing the document via a dedicated search-engine: After an upload request succeeded the search-index can be queried and relevant documents can be viewed/downloaded.\nAnd if something goes wrong? As described by Martin Kleppmann in his awesome book Designing Data-Intensive Applications, every component of a system should be expected to fail at any time (in particular when they are connected via network).\nThus, while the design shown above is simple, it may suffer from a couple of issues related to:\nAvailability: In the design shown above, all three data sinks need to be available to successfully process a document upload. If either one of them is (temporarily) unavailable, the request fails. In particular, the expensive indexing operation may become a bottleneck when processing concurrent requests. Consistency: Furthermore, only some of the three storage requests may actually succeed while others may fail. No matter how the three storage requests are ordered, a failure may leave the system in an inconsistent state, e.g. being able to find a document when searching via the index, but not when looking for the data in the object-storage (or vice-versa). These are known as dual-write issues. Decoupling components to improve availability Since the indexing is just relying on data which is also saved to the other stores, it could be postponed so that an upload request can succeed without requiring the search index to be available. In the following design, a message-queue is used to buffer indexing requests for asynchronous processing:\nThis may improve performance characteristics, since the indexing can limit the number of concurrently executed tasks by limiting the number of concurrently processed messages. Furthermore, the asnychronous processing of an indexing request can be automatically retried in case of temporary failures until it is successfully completed.\nHowever, upload requests will now succeed without the document actually being ready to be searched, which may be perceived as a kind of (temporary) inconsistency.\nAnd another problem: this still requires the message-queue to be available at upload-time and may suffer the same inconsistency issues as described above, since sending a message to the queue may also succeed or fail independently of the other storage requests.\nKeeping resources in sync One way to avoid having only some of the requests succeed is to let all resources participate in the same global, distributed transaction, using the 2-phase-commit protocol.\nWhile this is a common approach, it may not be supported by all resources, and it carries some complexity as it may entail manual recovery procedures (such as \u0026ldquo;system manager intervention\u0026rdquo; in case of deviating heuristic decisions after communication breakdowns).\nAnother way to avoid dual-write inconsistencies without resorting to 2-phase-commit is to persist the requirement of updating other resources as part of a local transaction, e.g. by using the transactional-outbox pattern.\nIn the following design, the indexing requirement is saved as part of the metadata storage transaction into an outbox table, which is monitored by a message relay component then creating messages for later processing to update the search-index:\nThis way, it is ensured that indexing messages are only created in case the transaction spanning the outbox-table-insert successfully committed.\nGiven a relay component that can asynchonously monitor the outbox table, there is also no need for the relay component or the message-queue to be available at upload-time to let a document upload request succeed.\nNevertheless, one dual-write remains in the system: during the initial processing of the upload request, the document is first stored in the object-storage, and the metadata is then stored together with the indexing request inside the database. In case the object-storage successfully completes the request but the database insertions fail, an orphan document will remain in the object-storage. This problem still needs to be mitigated, e.g. by periodically checking the object-storage for orphan documents which have no persisted metadata.\nAnother thing which needs to be taken into account is the possibility of failures during the processing of outbox entries and messages, since:\nthe relay component needs both to successfully create a message and to acknowledge the processing of the outbox entry the message both needs to be processed successfully and the successfull processing needs to be acknowledged Depending on whether the acknowledgements are done after or before the processing, this results in at-least-once or at-most-once processing.\nChoosing at-least-once to make sure that no documents remain non-indexed, repeated processing of indexing messages can occur. In our case, this should not be a problem since the processing may be made idempotent by first querying the index to check for already indexed documents, or the indexing could just be executed again since it can also be considered idempotent already.\nCDC with Debezium and the transactional-outbox pattern To implement the message relay component, microservices.io lists two options: polling the outbox table for events to be published as well as tailing the transaction log. The latter option is described as \u0026ldquo;relatively obscure although becoming increasingly common\u0026rdquo;, which is where Debezium comes into play, allowing for a convenient setup of the log-tailing approach.\nDebezium is a platform for change data capture, which is able to reliably capture data changes from a variety of sources (such as popular relational databases), and to emit these changes as an event stream via a variety of messaging infrastructures.\nIn our sample app, we use PostgreSQL for saving metadata on uploaded documents (and for the outbox table), Minio, an S3-compatible object-storage for the documents themselves, and OpenSearch for indexing and searching (an open-source fork of Elasticsearch). Redpanda is used as an Apache Kafka-like message broker, and Quarkus for implementing the backend of the user-facing application.\nAll required components can be setup locally using docker and docker-compose. Just clone the repository from Github and run the following commands from within the directory:\n# start needed services: # (postgres, minio, opensearch, kafka, debezium) docker-compose up # run the app, listening at http://localhost:8085 # (this will first build the app in a separate container) docker-compose -f docker-compose.app.yml To point it to a running Kafka instance, the Debezium Docker container is supplied an environment variable:\ndebezium: image: debezium/connect depends_on: - kafka environment: BOOTSTRAP_SERVERS: \u0026#34;kafka:9092\u0026#34; And for setting up Debezium to tail the transaction log of a specific table in a Postgres database, one request is needed:\ncurl --request POST \\ --url http://localhost:8083/connectors \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --data \u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;upload-service-outbox-connector\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;connector.class\u0026#34;: \u0026#34;io.debezium.connector.postgresql.PostgresConnector\u0026#34;, \u0026#34;plugin.name\u0026#34;: \u0026#34;pgoutput\u0026#34;, \u0026#34;database.hostname\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;database.port\u0026#34;: \u0026#34;5432\u0026#34;, \u0026#34;database.user\u0026#34;: \u0026#34;postgres\u0026#34;, \u0026#34;database.password\u0026#34;: \u0026#34;postgres\u0026#34;, \u0026#34;database.dbname\u0026#34; : \u0026#34;postgres\u0026#34;, \u0026#34;table.include.list\u0026#34;: \u0026#34;public.outboxevent\u0026#34;, \u0026#34;topic.prefix\u0026#34;: \u0026#34;upload-service-outbox\u0026#34;, \u0026#34;transforms\u0026#34;: \u0026#34;outbox\u0026#34;, \u0026#34;transforms.outbox.type\u0026#34;: \u0026#34;io.debezium.transforms.outbox.EventRouter\u0026#34;, \u0026#34;topic.creation.default.partitions\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;topic.creation.default.replication.factor\u0026#34;: \u0026#34;1\u0026#34; } }\u0026#39; Publishing and consuming messages with Quarkus For simple setup during development, the request shown above may also be issued automatically during Quarkus startup, e.g. using a microprofile RestClient:\n@IfBuildProfile(\u0026#34;dev\u0026#34;) @ApplicationScoped public class SetupDebeziumOnStartup { @Inject @RestClient DebeziumSetupService service; public void onStart(@Observes StartupEvent ev) throws Exception { try { service.setupConnector(getBody()); } catch (ConflictException e) { // ok, already setup } } @IfBuildProfile(\u0026#34;dev\u0026#34;) @ApplicationScoped @RegisterRestClient(baseUri = \u0026#34;http://debezium:8083\u0026#34;) // configurable @RegisterProvider(ConflictDetectingResponseMapper.class) public static interface DebeziumSetupService { @POST @Path(\u0026#34;/connectors\u0026#34;) @Consumes(MediaType.APPLICATION_JSON) void setupConnector(JsonObject body); } } Having this configuration in place, the Quarkus app can then start to write to the outbox table, which can be further simplified by including the corresponding Quarkus extension as a Gradle dependency:\nimplementation \u0026#39;io.debezium:debezium-quarkus-outbox\u0026#39; This allows to write to the outbox table by simply firing a sub-classed ExportedEvent:\n@Inject Event\u0026lt;IndexingRequiredEvent\u0026gt; outbox; @Transactional(value = TxType.MANDATORY) public void indexAsync(DocEntity doc) { IndexingRequiredEvent event = new IndexingRequiredEvent(doc.getFilename(), doc.getFilename(), Instant.now()); outbox.fire(event); } If configured, the Quarkus extension will also take care of immediately deleting entries from the outbox table to prevent it from growing over time.\nDebezium will then start to capture the data changes and publish corresponding messages to the configured Kafka topic.\nReading messages from a Kafka topic with Quarkus is also straightforward as described in the corresponding guide, using Smallrye Reactive Messaging:\n@Incoming(\u0026#34;indexing-events\u0026#34;) // configured to map to a specific Kafka topic @Blocking(ordered = false) // messages only affect distinct documents, so ordering is expendable public void receive(String message) { String payload = getPayload(message); indexingProcessor.processIndexingRequest(payload); } The message processing should be extended by automatically retrying message processing in case of transient failures (e.g. caused by unavailable upstream services), as well as by eventually handling persistent problems, e.g. in a way described in this article (my personal favorite being the Robinhood approach which allows for the simple implementation of a dedicated UI for flexibly triggering a reprocessing of messages e.g. after bugfixes have been deployed).\nWhen processing an indexing request message, the document to be indexed is first downloaded from the Minio object-storage using the Quarkus Minio extension which provides convenient access to the Minio Java API.\nSubsequently, an indexing request is send to OpenSearch using the low-level RestClient. To be able to index PDF-documents, a special ingest-plugin for attachment processing needs to be installed:\n# custom OpenSearch Dockerfile to install the ingest-attachment plugin FROM opensearchproject/opensearch:2 RUN /usr/share/opensearch/bin/opensearch-plugin install --batch ingest-attachment Further OpenSearch configuration is automatically done on app startup via the RestClient.\nResult The following screenshots show how uploading and searching documents looks like and how the data processing proceeds:\nPDFs can be uploaded by dropping them onto the page:\nFiles are uploaded to Minio, as shown by the built-in UI:\nUpload-requests complete successfully, however no documents have been indexed yet:\nMetadata was saved to Postgres, together with the outbox table entries (automatic deletion disabled):\nDebezium captures the inserts and creates corresponding Kafka messages, which can be inspected e.g. using the Redpanda Console:\nThe app then processes the messages and indexes the documents, which can then be queried. Results also include highlights showing the context in which search terms occured, and the document can be viewed:\nThere should have been a video here but your browser does not seem to support it. Obviously, the presented example system is overengineered. Just using PostgreSQL for storing the documents as well as using it\u0026rsquo;s built-in full-text search capabilities would probably have resulted in a sufficient user-experience without ever having to worry about more than one single, local, ACID transaction.\nNevertheless, the example could still be extended in numerous ways, e.g. by adding dead-letter handling, taking full advantage of OpenSearch\u0026rsquo;s various search options, or properly splitting the uploading-related and the indexing-related code into two separate services, which would further decrease the coupling at the code-level and could increase the maintainability. Another interesting extension could include a user notification right after the indexing of an uploaded completed, implemented e.g. by publishing corresponding events and subscribing a dedicated backend service which could then ping the user e.g. via a server-sent event.\n","permalink":"https://fladdimir.github.io/post/document-search/","summary":"Debezium, Quarkus, Kafka - the Transactional Outbox pattern in action","title":"Availability and Consistency of an App with Multiple Datasources"},{"content":" Update: as of 11/2022 Heroku suspended their free tier offer\nmorling.dev describes render to come close to the Heroku offer, with Postgres databases expiring 90 days after creation\nWhether its about trying out a new framework or sketching your next big startup idea - its always great to get the code running somewhere. And its even better if you can be sure that you will never be forced to pay anything at all.\nMany platforms offer a free-tier for freely using some of their services, sometimes just for a limited period of time, and/or only after presenting a valid credit-card. Furthermore, especially services based on vendor-specific technologies tend to be offered freely.\nAWS for example offers one free EC2 compute instance as well as a relational database instance for the first 12 months after the registration. In the \u0026ldquo;free-forever\u0026rdquo; offer are vendor-specific technologies such as Lambda-invocations and DynamoDB-storage.\nWhile this obviously makes sense from a marketing perspective, it is not great to get locked into vendor-specific technologies, or to risk financial loss just because of configuration mistakes (or because of security breaches).\nThis post describes a couple of tools, technologies and services which may be used for developing and operating a full-stack web-application, without the need to show a credit-card anywhere, as of 02/2022. Vendor lock-in\u0026rsquo;s are avoided as much as possible by aiming to build on established standards.\nSample app \u0026ldquo;Collective Color Picking\u0026rdquo; is a small toy app illustrating the used technologies. It allows users to pick their favorite color and share it with all other connected users in real time. A default color, which is assigned to the first connecting user after application startup, is persisted and can be changed via HTTP.\nThe app consists of a minimal browser-based frontend, a backend which holds the state (the picked colors of all connected users), and a database which stores the default color.\nGitHub repository: https://github.com/fladdimir/collective-color-picking\nSample-App on Heroku: https://collective-color-picking.herokuapp.com\n(first request may take some seconds, due to a possibly sleeping Heroku dyno)\nApplication structure The app has a standard 3 tier structure.\nClient-side: Vue.js with Typescript, using the cool radial-color-picker and a canvas to display the picked colors. Websocket connection for sending and receiving color updates.\nServer-side: Quarkus, a growingly popular Java framework with a number of cool features (e.g. dev-mode, native-image compilation), built on JEE standards like CDI, JPA, Bean Validation. Websocket support is available as well. Seamlessly usable with mapstruct, lombok, AssertJ, and others that make Java programming even more fun. PostgresQL, widespread and compatible database, locally runnable e.g. via docker.\nFor local development, the vite frontend dev-server can proxy incoming requests to a locally running backend.\nThe actual deployment uses nginx to serve the static client files and proxies /api requests to the backend.\nTests \u0026amp; CI Both Vue and Quarkus offer good test support. An especially useful feature is the option to run Quarkus API tests not only against the application running on Java but also against a native-image, which helps to discover problems like missing @RegisterForReflection annotations on DTOs.\nStatic code analysis tools like sonarqube further help to ensure code quality. Sonarqube analysis can be incorporated into the build via a gradle-plugin, and also supports visualizing the test coverage, which can be obtained by integrating the Quarkus-jacoco-extension.\nThe test and static analysis can be run e.g. on every push to or PR at a remote repository. git-services like GitHub (actions) and GitLab (pipelines) offer free build time on their runners for public projects.\nAnalysis and coverage results can then be submitted to sonarcloud, a sonarqube service which is also freely available for public projects (and offers shiny badges, too).\nHosting \u0026amp; Deployment Different to major cloud platforms, Heroku offers not only free compute instances (dynos, 512 MB RAM), but also free standard PostgresQL databases (limited rows, size, connections).\nTo be able to switch platforms without extensive configuration changes, the application is provided in form of a docker image, which is pushed to the Heroku registry. Database credentials are provided to a started container via an environment variable (which may profit from further pre-processing so that the backend can connect successfully).\nThe application docker image builds on top of the official nginx image and includes the frontend files and the backend in a single image, which can conveniently be created using a multi-stage docker build, so that no build dependencies need to be included into the final image.\nSince free RAM is limited - and to reduce wake-up time after Heroku dynos were put to sleep - compiling the application backend to a native-image can be beneficial. The native-image compilation requires a large amount of memory (~6 GB for the simple app), but luckily the free build runners of GitHub are able to cope with this.\nThe port on which the application is supposed to listen is also provided by Heroku via environment variable. To tell nginx, the config is generated at runtime from a template with help of envsubst before the start.\nOn update of a dedicated branch, a corresponding GitHub action automatically builds and deploys the image, also including a health-check request after application update.\nMonitoring: Logs \u0026amp; Metrics For observing the state of a running application, Heroku offers dedicated add-ons for collecting and analyzing logs and a standard metrics collection, which are only available after registering a credit-card.\nTo circumvent this issue, the app uses a dedicated logging and metrics collection setup. All logs \u0026amp; metrics are sent to a free-tier grafana-cloud instance.\nApplication logs are sent by nginx and the Quarkus backend via syslog to a local grafana-agent, which forwards them to the grafana-cloud loki log database. Since the grafana-agent receives syslog logs only via TCP, and nginx sends syslog logs via UDP, rsyslog is additionally used for \u0026ldquo;protocol translation\u0026rdquo;.\nApart from log-forwarding, the local grafana-agent is also a slimmed-down prometheus in agent mode, which scrapes metrics information from a specified endpoint created by the Quarkus metrics extension (/q/metrics). The scraped metrics are then forwarded to the Prometheus instance of grafana-cloud.\nBesides forwarding logs and metrics, the grafana-agent acts as an OTLP trace receiver which can forward traces created by the Quarkus OTLP extension. Grafana-cloud free-tier includes a Tempo tracing backend.\nThe grafana-cloud connection credentials are provided via environment variables.\nThe complete setup is shown below:\nThe observability backend including grafana components can be setup locally, e.g. using docker-compose, so that the complete setup can be tested locally which is especially important due to the amount of involved configuration.\n","permalink":"https://fladdimir.github.io/post/stack-2022/","summary":"A no-credit-card way of developing, deploying, and monitoring a web application","title":"Credit-Card-Free Web Application Stack for 2022"},{"content":"Back at university we had a great time playing around with miniaturized Fischertechnik conveying systems. We learned about the challenges of PLC programming and the joy of using Arduino micro-controller boards to bring conveyor belts and turntables to life. We were learning about simulation-based controls testing using Emulate3D (awesome 3D physics engine, robot emulations) and Plant Simulation (also great for testing higher-level controls).\nHowever, the restrictive licensing - USB dongles and notoriously limited pool-licenses - made it challenging to create a simple and accessible controls development workflow suitable for all project team members.\nLet\u0026rsquo;s sketch the idea of a material flow simulation environment for developing, debugging and testing C-code inside a Python-based simulation model - non-proprietary and ready to be run anywhere, e.g. as part of CI.\nPublic repo: https://github.com/fladdimir/material-flow-control-c-emulation\n1. Scenario \u0026amp; Scope The \u0026ldquo;turntable\u0026rdquo; (image below) represents a typical Fischertechnik conveying module. It consists of 2 motors and 3 sensors:\nconveyor belt (motor 1) proximity sensor or photoelectric barrier to identify whether some item is present on the module (sensor 1) turning unit for changing the belt rotation by 90 degrees (motor 2) stop position sensors for checking whether the module fully rotated (sensors 2+3, both at 0 and 90 degrees) Even though the motors typically require 24V, relays can be used to control a module with a micro-controller boards typically working at 3-5V pin voltage. Micro-controller boards such as Arduinos also offer further options for interfacing with other systems via serial communication.\nPutting together multiple turntable modules, grid-like conveying layouts can be realized.\nHowever, writing control logic for Arduino can be challenging, especially since debugging code running on a connected board is not as simple. Additionally, central material flow control software also needs to be tested in interaction with module controls.\n2. System Structure A structure for controlling the flow of goods through a grid of connected turntable modules could look like this:\nevery turntable\u0026rsquo;s sensors and actors are controlled by a micro-controller (I/O) every micro-controller has a serial connection to communicate with a central material flow control server the server logic consists of different parts dedicated to communicating with connected modules, as well as central functions e.g. for pathfinding/routing of items through the system While systems such as Flexconveyor claim to work in a decentralized way without the need for a central \u0026ldquo;routing brain\u0026rdquo;, the described structure tries to shift all functions without hard real-time requirements from the micro-controllers to a central server (which makes particular sense when running on low-budget, resource-constrained boards such as the Arduino Uno).\n3. Emulation Interfaces Within the described setup, every micro-controller interacts with its environment via sensor-reads, actor-writes, and byte-wise serial-reads/writes. To be able to run the control logic on a development computer, the actual hardware-dependent read/write calls need to be replaced by interactions with simulated counterparts.\nOne way of achieving that using a Python-based emulation environment and exploiting the simple interfacing with C code could look like this:\nmodule control C code is called from a Python runtime with help of a generated CFFI wrapper device-dependent interaction methods are setup to call corresponding Python functions providing simulated inputs/outputs sensor-reads \u0026amp; actor-writes are run against a shared memory, which is also read/written by a simulation of the physical modules serial reads \u0026amp; writes are going to a pipe, which represents a connection to the server-side module agent a dedicated sub-process is created for every module control so that no global C variables are shared between different module controls CFFI allows for the simple creation of functions which are declared in C and called from Python, as well as the other way around. When generating the CFFI wrapper code, C header files can be used to prepare C functions to be called from Python. To be able to implement forward-declared C functions in Python, the follwing definition can be set:\nextern \u0026#34;Python+C\u0026#34; bool _light_barrier(); // provide info whether sensor is triggered This way, an implementing Python function can then be wired:\n@ffi.def_extern() def _light_barrier() -\u0026gt; bool: return shared_array[LIGHT_BARRIER_IDX] # simulated value from memory instead of actual sensor 4. 2D-Simulation To be able to emulate physical interactions of the module control code, real system components need to be simulated. While there are Python wrappers for awesome (3D) physics engines such as chrono, this implementation just uses a minimal 2D engine with very limited physics.\nSupported features include:\nnested element structures in 2D cheap collision//overlap detection for certain combinations of simple geometries (rectangles, straights, points) relative transition and rotation movements of elements and child-elements movement limits (e.g. to realize max. 90 degrees of rotation for turntables) While an elements global position is determined by its local position relative to the parent-element it is attached to, the global position is cached to avoid recalculation for nested objects in case no position changes occured for any parent element.\nThe collision detection uses a simple 2-phase check, first checking cheap circumcycles to identify potentially colliding objects before performing a more expensive cohen-sutherland evaluation. Collisions are used in the simulation e.g. to identify when a box is hitting a light barrier, or which conveyor belt movements are changing a box\u0026rsquo;s position.\n5. Putting it all together The screencast shows an animation of a small sample test scenario consisting of several modules forwarding one box towards randomly chosen target modules (indicated by \u0026lsquo;X\u0026rsquo;):\nThe visualization is created with help of the Arcade game engine, which allows to simply step forward the simulation inside the game loop and animate the current state. In addition to the advent of the simulated time, the (C-coded) control loop of each module is invoked sequentially to allow a reaction to potential sensor state changes.\nOn the server-side of the control hierarchy, the forwarding/receiving of boxes between modules and the awaiting of modules to reach readiness is realized using asyncio coroutines.\nThe determination of which modules to visit in order to reach a distant target module is done with help of networkx and a corresponding graph constructed from the module locations.\nThe cool thing about all this: we can debug every single layer of the control code at any point in time!\nIn addition to normal Python debugging (part of the VS Code Python extension) of the server-side control code, we can attach a C (gcc) debugger to any of the module sub-processes running the C code.\nThis also works great from within VS Code, where we can even run different debugging sessions at the same time.\nBeing able to \u0026ldquo;look inside\u0026rdquo; the current state of a C module control makes it amazingly easy to discover bugs, such as the bad evaluation of commands received via serial communication, confirmation serial writes at bad times, plain wrong if-conditions, misplaced early returns etc..\nAs animation is fully optional, running a \u0026ldquo;headless\u0026rdquo; simulation can be easily used for validating checked-in control code agains a set of defined test scenarios / module layouts e.g. as part of a CI pipeline.\nLimits and Shortcomings Despite the good level of support for development and debugging, there are serious shortcomings of the presented approach, so that there still remains the need for properly testing the complete physical system:\nC code is run on a normal PC instead of a resource-constrained micro-controller, so e.g. memory usage should be carefully checked the defined \u0026ldquo;hardware abstraction layer\u0026rdquo; hides the complexity of actual communication hardware and technologies (pins/bus systems/rxtx/i2c/\u0026hellip;) all control code is invoked sequentially and the simulation waits for all control loops to finish before the next time step, so real-time requirements should be checked with special care on the target hardware ","permalink":"https://fladdimir.github.io/post/material-flow-control-emulation/","summary":"Running C code as part of a Python simulation using CFFI","title":"Python-based Emulation for Developing Material Flow Controls in C"},{"content":"Wouldn\u0026rsquo;t it be cool to be able to share your location with friends, but without relying on established service providers, which most probably already know way more about you than they should?\nThis post will look at a small web-app implementing essential basics of that functionality, using React and Leaflet at the frontend, Micronaut at the backend, Okta as identity provider, and Heroku for free hosting.\nGet the repo: https://github.com/fladdimir/locsharex.\nTL;DR - just show me the app!\n(first request may take some seconds, due to a possibly sleeping Heroku dyno)\nUpdate: as of 11/2022 Heroku suspended their free tier offer\nFeatures and UI Login screen To get a first impression without sign-up, 3 pre-defined test-users allow you to quickly see the app in action. Just click to login and play around.\nOkta-Login To create your own user, sign-in via Okta, a free Open-ID-Connect identity provider.\n(Well, free for any app as long as there are no more than 15k monthly users.)\nThe Map Main screen of the app, where you can see your and your friends\u0026rsquo; locations.\nEdit your location by dragging the marker onto the map, stop sharing your position, or rely on your browser/device location (consent needed). Filter for username in case of searching for someone specific.\nSettings The contact-administration page, with lots of space for more friends ;)\nLook for not-yet-connected people by username, change the name by which people may find you. Sign-out to try another test-user or check back later.\nTechnologies Frontend Single-page react-app, created with create-react-app and Material-UI. Open-street-maps integration with help of leaflet and react-leaflet for simple usage within react components with great typescript support. VSCode.\nWorth mentioning might be the awesome create-react-app tooling, including a proxy-mode for forwarding of /api requests to another locally running server (no cors hustle), and the HTTPS support required for using the geo-location browser api.\nBackend Micronaut, a popular JVM-based microservice framework, aiming for lower memory footprints and faster start-up times than Spring-Boot, and (at least currently) still with better support for creating native images with GraalVM (for further start-up time reduction). Thanks to the java-extension, VSCode also provides great support for backend development.\nConvenient entity persistence with Data Repositories and Hibernate. Below is a simple entity-relationship-model for the app, showing the AppUser with it\u0026rsquo;s app-internal ID, name, (embedded) position, and associated users (m-n self-reference):\nIn addition to the REST-API, Micronaut also provides the possibility to serve static files.\nIdentity Provider Micronaut Security offers a variety of options for securing an app, including support for OpenID Connect-based authentication flows. To link persisted user-entities with actual users authenticating via OIDC, the entity also contains information on the identity provider and the sub (a user ID from the identity provider). That information is only used once during login to identify the logged-in user before issuing a new JWT. No info from the identity-provider is given anywhere but to the database.\nMicronaut Security includes support for a couple of different OIDC providers out of the box (e.g. Auth0, AWS-Cognito, Okta, Keycloak).\nKeycloak is a popular open-source identity and access management solution, lately starting to move from Wildfly to Quarkus - Keycloak-X. Great for just spinning up a container for local development.\nAlternatively it is simple to switch to a managed IDP such as Okta.\nShown below is an overview of the application components and their interaction during OIDC login:\nThe user decides to login and issues a request to initiate the flow The response redirects to the identity provider The IDP login is loaded, where the user enters credentials Upon successful IDP login, the user is redirected to a callback-endpoint of the app (including a one-time code) The user issues the callback request (including the one-time code) The app backend makes a secure back-channel request to exchange the user-related one-time code for an ID-token (together with an app-specific, pre-configured client-secret) The IDP returns an ID-token containing user information The app backend creates a new JWT containing the app-internal user ID and returns the JWT as an http-only cookie as part of a redirecting response The browser then sends the cookie with every API request, allowing the backend to validate the JWT and the permission of the user to access requested resources During logout the user needs to clear the existing IDP session as well as the app session, which can be achieved by a series of redirects between the logout endpoints.\nSee e.g. this presentation for an in-depth explanation of OAuth 2 and OIDC.\nHeroku Heroku is a platform-as-service which offers a simple way to run an app in the cloud, including a free tier of 550 micro instance-hours per month and a postgres-database with a 10k row limit. Applications can be provided e.g. in form of jars or docker images (the latter may require custom processing of the environment variable storing the Database credentials). After 30 mins of inactivity without incoming requests, an instance is put to sleep and stops consuming instance-hours. To quickly respond to a request when waking up an instance, the usage of a native image of the app can be beneficial (reducing the app startup time from ~4.1 seconds to ~0.24s).\n","permalink":"https://fladdimir.github.io/post/location-sharing/","summary":"A simple web-app for sharing geo-locations with friends","title":"Playing around with React, Leaflet, Micronaut, Okta, \u0026 Heroku"},{"content":"When creating Python-based simulation models with Casymda /SimPy, a frequent remark is that Python - being a slow, dynamic, interpreted language - would be a bad choice for this type of endeavour.\nSo could compiled, statically-typed SimPy alternatives make model execution faster, therefore being a better choice for development?\nThis post will have a look at Uia-Sim and SimSharp, two SimPy ports for discrete event simulation in Java and C#. As a proof-of-concept, two small block-based modeling libraries were created, similar to Casymda for SimPy, and their execution speed was compared with help of a simple benchmark model.\nUia-Sim: SimPy for Java Uia-Sim is a recently published Java port of SimPy, published by UIA Java Solutions. Since Java does not provide generators/coroutines out of the box, the creator developed a \u0026ldquo;yield-like API\u0026rdquo; to enable blocking and resuming of simulation processes with help of threads.\nBased on Uia-Sim, a Casymda-like library for block-based composition of discrete event simulation models was created as a proof-of-concept. Csa4j implements some of the ideas from Casymda:\na Block as the basic class to compose simulation models is executing processing logic for received entities, and forwarding to successor-blocks can be extended for custom processing logic (e.g. elapsing time, as Delay) Source-blocks spawn entities and initiate their processing Sink-blocks end entity processing and cause removal from the simulation basic animation capabilities can visualize the entity flow between model blocks for debug / presentation As an improvement over Casymda, animation-related behavior and data was properly seperated from the simulation-related behavior of basic blocks, which just provide notifications whenever block-states change or entity movements occur.\nSimilar to Casymda\u0026rsquo;s debug animation based on Tkinter, corresponding visualizations can be created in Csa4j with JavaFX. While browser-based animation was out of scope for this PoC, the multitude of existing (micro-) web-frameworks for Java would make it easy to provide corresponding functionality. Further missing features which are present in Casymda include the parsing of .bpmn files to generate simulation model classes, the state-tracking of blocks, and tilemap-movements.\nThe project uses a basic gradle setup with Java 14 and JUnit-tests. Coverage info and static code analysis can be obtained with help of jacoco, sonarqube, and a sonarqube gradle plugin. Since Uia-Sim seems to be not yet available via maven-central, this dependency can be built locally and primarily be retrieved via a local maven repository. VSCode and its Java extension pack provide a great development experience.\nSimSharp: SimPy for .NET SimSharp is a .NET port of SimPy which is developed by the research group \u0026ldquo;Heuristic and Evolutionary Algorithms Laboratory\u0026rdquo; (HEAL) from Austria (also known for the HeuristicLab optimization framework). Similar to SimPy (and different to Uia-Sim), SimSharp is using iterators for resumable simulation-processes.\nCreated for this PoC, the Csa4cs-library provides the same features as \u0026ldquo;Csa4j\u0026rdquo; (described in the previous section), including a simple canvas animation based on skiasharp and gtksharp.\nThe project is based on the .NET 5.0 SDK, using the XUnit test framework. Similar to the Java project, coverage info and static code analysis can be obtained conveniently with sonarqube (even though the scan step requires a bit more setup effort compared with the gradle plugin usage in Java). Thanks to its C# extension, VSCode can offer great development support.\nPerformance Comparison To evaluate the execution speed of the simulation libraries, the same model was implemented using Casymda/SimPy, Csa4j/Uia-Sim, and Csa4cs/SimSharp. Additionally, the SimPy-model was executed using CPython and PyPy. Identical assertions on the results of the simulations verify the correctness of the model implementations.\nThe benchmark model is made of typically used, basic processing blocks and is shown below.\nOne source produces entities with a given inter-arrival-time and forwards them to a gateway, which alternatingly chooses either a delay-block with infinite capacity (parallel processing), or a buffer which is placed before a delay-block with capacity 1 (sequential processing). A second gateway joins both entity flows.\nDepending on the inter-arrival-time, 2 main scenarios can be simulated:\ninter-arrival-time \u0026gt; processing time not causing any queues or actually parallel processing, representing a plain processing of entities with a short event queue inter-arrival-time \u0026lt; processing time leading to a queue before the sequential processing (up to n/2 entities waiting), and a simultaneous processing of the other n/2 entities Different experiments simulate the processing of 10 to 200_000 entities and were carried out on an Ubuntu notebook with i5 processor.\nThe diagram below shows the execution time of the simulation runs for the first scenario where no queing occurs, depending on the number of created entities:\nAs we can see - and as one might expect - the execution time linearly grows with the number of processed entities on all platforms. Interestingly, the Java model (red) is considerably slower than the Python and the C# versions, so that the longer experiments were omitted. This performance drawback could be explained by fact that one os-thread is created per entity-process, combined with a high computational overhead of threads compared to generator/iterable-based coroutine-objects. Even for short runs, the PyPy JIT compiler (orange) can reach an impressive speed-up compared to CPython (blue). The .NET model outperforms even PyPy by a factor of ~5.\nThe second chart shows the queuing scenario, with up to one half of the entities waiting for a shared resource, and the other half being simultaneously processed:\nAs before, the longer runs of the Java model were omitted due to their duration. While CPython and .NET show an again seemingly linear growth, the exponential development of the PyPy execution time reveals a rather surprising slow-down in the long run compared to CPython.\nSummary Static typing alone does (surprisingly) not guarantee any execution speed advantage. SimSharp did prove to work great and might definitely be worth further evaluation. The Java-based library should probably not be used when creating many simulated processes (e.g. one process per entity, with many rather short-lived entities as in the sample model). PyPy can provide significant speed-ups over CPython, closing the gap between Python and C# - however, that depends. The shown figures were created from a first, tentative, prelimenary comparison, delivering results which are not even fully comparable (especially due to the omitted features in the Java and C# libraries). Apart from execution speed, the eco-system remains as a strong plus for creating discrete event simulation models with Python.\n","permalink":"https://fladdimir.github.io/post/csa4j+cs/","summary":"Performance Comparison of Casymda-Ports for Uia-Sim and SimSharp","title":"Block-based Modeling with SimPy - in Java \u0026 C#"},{"content":"In his highly interesting, recently published PhD thesis (German), Toni Donhauser from the University Erlangen-Nürnberg gives an excellent example on how a production-synchronous digital twin can be used for automated, simulation-based order scheduling in masonry plants.\nAs a core feature, the developed simulation allows to initialize the work-in-process of the manufacturing system to precisely mirror the current state and create accurate short-term forecasts, which serve as a basis for comparing alternatives and optimizing production plans in case of unexpected disruptions. Tecnomatix Plant Simulation (Siemens) is used for the implementation of the simulation model. Manufacturing data is fetched via the built-in OPC-UA interface from an OPC server and via ODBC from an MS Access database. Simulation runs can be triggered manually by an operator using a management application written in C#.\nSince Plant Simulation is known for extensive features as well as for extensive licensing fees, this blog post will present an alternative implementation of such a production-synchronous digital twin, based on open-source frameworks and building on easy-to-operate, pay-per-use AWS infrastructure.\nThe complete setup can be deployed and tested locally using Docker, LocalStack and Terraform (no AWS account required).\nGet the repo from github: https://github.com/fladdimir/csa-simulation-based-sc-forecast\nSlides: https://fladdimir.github.io/presentations/des-on-aws.html\nPaper on researchgate\nScenario \u0026amp; Scope The chart below shows a fictive and simplified order manufacturing process, serving as a minimal example to illustrate how a digital twin of the system can be implemented.\nAfter being created, orders are received and accepted by the company (\u0026ldquo;ingest\u0026rdquo;-step), and the order-specific raw material is ordered (\u0026ldquo;order_material\u0026rdquo;), leaving the order waiting until the corresponding material arrives (\u0026ldquo;wait_for_material\u0026rdquo;). When the material is delivered, the order proceeds to a queue (\u0026ldquo;wait_for_sop\u0026rdquo;), waiting to be processed in a capacity-constrained \u0026ldquo;production\u0026rdquo;-step, which is only able to process one order at a time. Eventually, the finished order gets delivered to the customer and leaves the system.\nWhenever material for an order is requested, an initial estimated time of arrival (ETA) is assigned. However, unexpected supplier-specific process deviations or other delivery problems may introduce delays at any point in time, so that ETA-updates are possible during this step. Since the production step uses a capacity-constrained resource and represents a possible bottleneck of the system, any unplanned under-utilization here may delay every upcoming order and diminish the system throughput (depending on how tight the schedule looks like). Therefore, it is desirable to be able to quantify the effects of any shift in time as soon as an ETA-update for an order occurs.\nSynchronized Digital Twin: Concept and Implementation The next figure shows a simple event-processing pipeline, able to ingest defined events and to persist the system state (event tracking), which in turn enables the simulation-based creation of forecasts for expected order completions times and delays (event analytics). A simple web-dashboard will be used to visualize the results.\n1. Publishing events of data producers During the processing of an order in the physical system, data producers such as sensors and IoT-devices are capturing information on the progress, i.e. events of state-changes as e.g. start or finish of the production step of an order. These order updates are published to a defined endpoint where they are collected and processed (2.). While those events would actually be happening in the physical manufacturing system, a simulation model might be used to create test-data for the digital twin (see the post on Virtual Commissioning for another example of this use-case for simulation).\n2. Capturing events with AWS Kinesis Kinesis is an AWS service for continuous buffering and real-time processing of streaming data. A Kinesis stream decouples data producers and consumers and consists of a configurable number of shards, each of which is able to ingest up to 1 MB or 1000 records of data per second. Each record is put into one shard based on it\u0026rsquo;s specified partition key value, which gets important since in-order processing of records is guaranteed only on partition key level.\nIn the described scenario in-order processing becomes critical for ETA-updates of orders, since the message of an expected delay must not be processed before an earlier submitted update.\nNew records can be put to the stream e.g. using the AWS SDK, which is available for various languages, including Python which is used for the emulated test client.\n3. Processing events with AWS Lambda Lambda is the function-as-a-service offer of AWS, which allows to run code on-demand, paying for the number of invocations as well as for execution time. Lambda functions can easily be integrated with other services such as SQS and DynamoDB. Since AWS provisions the function runtime on-demand, the short cold-start times of NodeJS and Python make them a popular choice for implementing lambdas, while \u0026ldquo;heavier\u0026rdquo; alternatives such as Java are less common (the JVM would need multiple invocations for the JIT-compilation to boost performance).\nThe lambda implemented for processing order updates is simple and just updates the corresponding item of the affected order in a specified DynamoDB table with data from the event provided as part of the invocation.\n4. Persisting the system state with DynamoDB DynamoDB is used as a fast, flexible and managed NoSQL database. While this type of database by design lacks some of the amenities of relational databases (such as proper means to enforce referential integrity on the database level, or the availability of sophisticated ORMs and schema management tools), it is fine for our simple use-case which just involves updating single items and basic queries. DynamoDB requires a hashkey and optionally a partition key, both of which are used in combination to uniquely identify a stored item. For orders the string id can be used as the hashkey. A nice feature of DynamoDB is the option to enable streams, automatically providing information on table-updates. This way, order ETA-updates can trigger new forecasts.\n5. Simulating the future AWS allows to use Lambda functions as DynamoDB stream event consumers, so that simulation runs can forecast future order completion times on every state change.\nFor each run, the complete system state is fetched from the DynamoDB (which might actually need multiple requests, since a single scan might only return a page of up to 1 MB of data).\nBased on the registered process timestamps, the currently relevant process step of each order can be identified.\nThe simulation model is generated from the process diagram shown above using Casymda. For the sake of simplicity of this proof of concept, processing times are assumed to be deterministic (even though stochastic behavior could be easily modeled, it would require averaging multiple runs). Model blocks are implemented to account for already elapsed processing time of work-in-process-entities at the start of the simulation (one of the possibilities to initialize online simulation models discussed in the often-cited paper of Hanisch and Tolujew, 2005, further explored by Hotz, 2007). During the execution, forecast metrics are collected in form of predicted process step completion times. Currently, AWS allows Lambda function executions to take up to 15 minutes, so that even complex models can be run this way. However, frequent and long running calculations might make it more attractive to create a dedicated service.\n6. + 7. Forecast persistence and visualization At the end of each run, the gathered results are persisted in a second DynamoDB table, from where a dashboard application can access and visualize the data.\nPlotly Dash is a popular framework for analytics web-apps. It enables the quick creation of dynamic dashboards just by writing Python code. Under the hood, it uses flask to serve React websites with plotly charts to a browser. Data queries and analysis are done on the backend using Python. The implemented dashboard just contains a simple gantt-chart (and serves only as a very basic example, leaving lots of room for extension). Automatic dashboard refreshes are implemented using an interval-callback to cyclically poll the database for updates.\nA dashboard\u0026rsquo;s Docker container could be run on AWS (e.g. ECS/Fargate, but since the free version of LocalStack does not include this it will just be run locally for demonstration).\nResult To run the setup locally from within the cloned repository, Docker and Terraform need to be installed.\nEven though the performance is not comparable to the actual cloud service, LocalStack is an awesome option to serve a multitude of AWS services locally, including Kinesis, Lambda, and DynamoDB. LocalStack can be started in a privileged Docker container, spawning more containers as needed, e.g. for executing Lambdas. It can be started via:\ndocker-compose up localstack Before the Lambda functions can be deployed, the function code and its dependencies need to be packaged:\ndocker-compose up package-ingest-lambda package-simulation-lambda Terraform is a great and widespread tool which can automatically provision infrastructure resources described in configuration files (however, have a look at this article for a more nuanced analysis). To create all required resources, two terraform commands are needed:\ncd terraform terraform init # required once terraform apply # enter \u0026#39;yes\u0026#39; when prompted to confirm the changes (or use -auto-approve) cd ../ # return to project root (To prevent 404 errors when calling apply after a restart of LocalStack without calling terraform destroy, first delete the terraform.tfstate files next to main.tf.)\nAfter the successfull creation, two more containers can be started - one serving the dashboard and one running a simulation model to emulate real event producers:\ndocker-compose up dashboard emulation Before (re-)starting any test-run, the DynamoDB-tables need to be cleared:\ndocker-compose up truncate-tables http://localhost:8050 should now show the empty dashboard, while http://localhost:5001 should show the generic Casymda web canvas animation controls. To enable automatic refreshes use the switch above the chart on the dashboard.\nWhen starting the emulation, orders will be created at the source and flow through the defined process.\nAt the same time, the dashboard should update with a minor delay and visualize the completion times of the relevant process steps of all orders which are currently present in the system. A vertical line in the chart indicates the point in time when the simulation run started and the forecast was created.\nSample flow 1. The first order is created The simulation forecasts process step completion times as defined in the model: 2. The second order arrives and Order-1 production starts The forecast does not show problems: 3. After some time, an ETA update for the Order-2 material delivery is communicated, and a delay of 1/3 is now expected The forecast shows the announced delay (orange) and the expected shift of the production step of Order-2: 4. Order-1 is finished (and therefore excluded from the forecast), but now Order-3 arrives The forecast reveals an upcoming problem! Caused by the capacity constraint of the production step (max. one order concurrently), the delay of Order-2 (orange) will also prevent to start the of production of Order-3 on time, even though the material is expected to be ready by then (red): 5. When Order-2 is almost finished, a 4th order comes in As the forecast shows, the delay of Order-2 will cascade and also affect Order-4: Complete screen-cast:\nThere should have been a video here but your browser does not seem to support it. While this was just a proof of concept and the presented example would have been easy to calculate by hand, there are plenty of improvements and extensions imaginable.\nLooking at the scenario and business use-case, it would be interesting to add more complexity to the process, such as inventory for raw materials, and different replenishment strategies. Similarly, the impacts of stochastic or planned machine maintenance intervals might be evaluated. Another extension would be to incorporate targets into the process, such as order-specific due dates or throughput goals. This might then ask for additional optimization procedures to determine optimal production control policies (similar to the case presented in the thesis mentioned in the beginning of this article).\nInteresting technical extensions include security aspects such as authentication and authorization of different data producing parties, as well as an integration of the IoT-related services of AWS, which might offer dedicated features to gather data with sensors and edge devices for the digital twin. Concerning the analytics of ingested event data, stream processing solutions such as AWS Kinesis Data Analytics might be useful to identify relevant patterns and trigger forecast and optimization runs only in case of critical process deviations.\n","permalink":"https://fladdimir.github.io/post/csa-simulation-based-sc-forecast/","summary":"Proof of Concept using Casymda on AWS, ft. Terraform and LocalStack","title":"Real Time Simulation-based Supply Chain Analytics"},{"content":"The planning and design of logistics systems - both at supply-chain and intralogistics level - is frequently supported by simulation studies, used for comparing design alternatives, assessing their feasibility, as well as estimating KPIs like lead-time or throughput.\nWhen it comes to the realization phase of logistics systems, major challenges relate to the development of controls and operational IT systems. Given the fact that testing, integration, commissioning (and bug-fixing) of these systems tend to consume a significant chunk of the realization phase, it becomes clear that it is beneficial to test a developed system as early as possible - even before physical construction takes place.\nVirtual Commissioning describes the testing of software against the digital counterpart of a real system, making use of simulation models to emulate real-world interaction.\nThis post will show an example of how the integration of simulation-based testing into today\u0026rsquo;s agile software development processes can look like, investigating a case-study on order management \u0026amp; delivery optimization.\nGet the repo from github: https://github.com/fladdimir/csa-vcom\nSlides: https://fladdimir.github.io/presentations/des-based-integration-tests.html\nPaper on researchgate\n1. Scenario \u0026amp; Scope Remember La Pâtisserie, the small French bakery in Hamburg-Altona, which was experiencing a massive shift of demand towards at-home delivery of their sweet pastries?\nHaving evaluated different options of how to scale their business-model with help of an innovative open-source approach to urban delivery network simulation, the growing network now gets harder and harder to manage, calling for an increased software-based support of the bakery\u0026rsquo;s daily logistics operations\u0026hellip;\n2. Processes \u0026amp; Requirements To be able to focus on their core-competencies (to conjure up delicious treats, instead of fighting intractably inconsistent spread-sheet data), our bakery decides to go for a web-based logistics planning application.\nThe core processes to be supported are:\nRegistration of customers, and tracking their orders Managing locations of the depots to plan the best-possible deliveries Keeping track of the trucks, delivering goods according to the planned tours The following BPMN-diagram shows the the processes and a simple token-flow animation:\nThere should have been a video here but your browser does not seem to support it. 3. Test First: Simulation Model + Build-Pipeline To make sure that all required processes are adequatly supported by the developed software, our bakery\u0026rsquo;s software development division opts for a test-driven approach, backed by a build-pipeline which automatically checks all code pushed to the repository.\nBased on the specified business process a Casymda simulation model is generated, ready to emulate the real system, with which the developed software is supposed to work. As processes and scope of the application change, the simulation model is evolved in an agile way.\nGitea and Drone form the basis of the continuous integration infrastructure. As part of a virtual commissioning step, the pipeline spins up the application in a service-container, against which the simulation model runs the test-scenario, emulating interaction and verifying the expected behavior of the software.\nThe pipeline is described by a .drone.yml file. Note that the pipeline could be improved in various ways, e.g. by properly waiting for the app (service) to become available for the simulation-step. A docker-compose.yml allows to start the gitea+drone setup locally (using a single-instance setup, which is not ideal, but sufficient for testing).\n4. Application Design \u0026amp; Implementation Our bakery\u0026rsquo;s app is dealing with management of the data of customers, orders, depots, tours, and trucks. Additionally, it is required to support planning the delivery process by calculating efficient tours and assigning them to available trucks.\nThe app adopts a basic 3-layer structure consisting of a browser-based ui, a backend containing the business logic and optimization algorithms, and a persisting database. The graphic below summarizes the setup, including the simulation model which acts as a client in the automated build pipeline:\nThe backend is implemented using Django+Django-Rest-Framework, and relying on Google-OR-Tools for optimization tasks. Tour planning is modeled as a capacitated vehicle routing problem with multiple depots. For an optimal assignment of pending tours to available trucks, OR-Tools offers a minimum-cost-flow solver which is used on a corresponding bi-partite graph.\nTo create the required distance matrices, we can utilize the Open Source Routing Machine, provided as a ready-to-use Docker image (OSRM-in-a-box). OSRM offers a convenient API which is synchronously consumed upon creation of a new customer or depot. Open-street-map data can be downloaded e.g. from https://download.geofabrik.de. The map of Hamburg has a size of ~35 MB and OSRM-preprocessing (car-profile) takes about 30 seconds (i5 dual-core notebook processor).\nSQLite provides a simple database solution, however, Django makes it easy to switch to a client/server RDBMS like Postgres or MariaDB.\nThe basic frontend is built with Angular, Material, and Leaflet.js (easy to integrate thanks to ngx-leaflet).\n5. Result The screencast below shows the workflow from a users perspective. It comprises registering a new customer, issuing an order, planning tours, assignment to a truck, and tracking deliveries as the tour proceeds:\nThere should have been a video here but your browser does not seem to support it. The shown process matches the one executed by the simulation model in the virtual commissioning pipeline build step, ensuring stable functionality for every version of the software:\nExtensive and automated integration testing with simulation models can help to enable and sustain software quality, particularly in the context of process-centric logistics applications. As we\u0026rsquo;ve seen, today\u0026rsquo;s software development tools and standards allow for an efficient integration of simulation techniques \u0026amp; virtual commissioning approaches into the development process.\n","permalink":"https://fladdimir.github.io/post/csa-vcom/","summary":"Continuous Integration and Virtual Commissioning of Logistics Software","title":"Logistics Process Models for Automated Integration Testing"},{"content":"A really nice and quite unique feature of Anylogic is the possibility to include GIS-maps into simulation models. It allows to place elements on a map and move them along existing routes, based on real spatial information. This is cool because it can be used to simulate entire supply chains, including means to provide a great, tangible visualization for complex problems.\nThis previous project used Anylogic to evaluate different designs and delivery schemes for a more sustainable urban logistics network in the city center of Grenoble in France. The data-driven simulation model allows to quickly calculate KPIs for various transshipment node locations and different types of transport equipment in a multi-tier supply-chain network.\nFollowing the success of the feature, Anylogic even built anyLogistix, combining pre-built \u0026amp; customizable simulation models with a commercial solver for integrated supply chain planning \u0026amp; optimization.\nObviously, those commercial features come at a price, so let\u0026rsquo;s see whether this kind of model can also be realized with different means.\nThe simulation model of this post will be based on a mini-case-study.\nGet the repo from github.\nScenario \u0026amp; Scope Facing the fact that humanity gets more and more used to home-delivery of even everyday necessities, the small French bakery Die Patisserie in Hamburg-Altona wants to deliver sweet pastries to nearby customers.\n3 major purchasers were identified: the lost-and-found office Zentrales Fundbüro, Monkeys Music Club, and the publishing house Carlsen Verlag.\nCoordinates of the different locations:\nNode Name Lat Lon 1. PAT Die Patisserie 53.55668 9.92815 2. ZFB Zentrales Fundbüro (lost-and-found office) 53.55817 9.92829 3. MMC Monkeys Music Club 53.55706 9.93161 4. CAV Carlsen Verlag (publishing house) 53.55703 9.92684 For the sake of simplicity, the order in which nodes are visited is assumed to be fixed. The tour starts \u0026amp; ends at the patisserie:\nSimulation Model The simulation model for the simplistic scenario is created with:\nOSMnx/networkx for retrieving geo-information and calculating shortest-paths SimPy/Casymda for simulating the tour Leaflet.js for browser-based animation 1. OSMnx The awesome OSMnx package provides the possibility to obtain a networkx-graph representation of a street-network from OpenStreetMap with a single line of code. A relevant section for our scenario can be obtained by specifying center and distance for osm-nodes to be included:\nCENTER = (53.55668, 9.92815) DISTANCE = 300 G = ox.graph_from_point(CENTER, distance=DISTANCE, network_type=\u0026#39;drive\u0026#39;) OSMnx lets us pick the nearest osm-node for each of the 4 locations of the tour, and also offers convenient means to plot the network. Blue dots represent osm-nodes, connected by edges. The 4 relevant locations are shown in red:\nTo prepare all information needed by the simulation model, now all shortest paths between the 4 relevant nodes in the network are computed with networkx, and detailed information on all piece-wise linear segments for each route is included. The results are pickled and saved to disk to avoid fetching and recalculation for each run of the model (of course its kind to keep the load for the OSM-server as low as possible).\nThe above described approach could be improved, e.g. by automatically determining the area to be loaded from the given relevant locations. Instead of just picking the closest osm-node for each relevant location, it would also be more precise to first go for the closest edge in the network. And as the network size grows, it might be a better idea to directly query the shortest path between relevant nodes from the OSM-server, instead of fetching the network as a whole (the way Anylogic seems to do it).\n2. Casymda/SimPy Casymda provides block-based modeling of discrete-event-simulation models on top of SimPy.\nOur model can be characterized by a simple process:\nA truck is created at a parameterized Source and then processed at a custom DriveTour-block, which contains the logic for elapsing time according to the length of a route and the movement speed of the truck. It also contains the option to calculate intermediate locations for animation. The nodes to be visited are specified via the text-annotation stops=[\u0026quot;ZFB\u0026quot;, \u0026quot;MMC\u0026quot;, \u0026quot;CAV\u0026quot;].\nThe animation is implemented by exposing information and resources via flask (similar to the tilemap-animation described in the previous post).\n3. Leaflet.js To visualize the location of nodes and entities on a map, Leaflet only requires a few lines. For setting the rotation angle, there is the Rotated.Marker-plugin.\nmarker = L.marker(element.coords, { icon: new L.Icon({iconUrl: element.icon_path}) }).addTo(map); marker.bindPopup(element.text); marker.setRotationAngle(element.direction); Result To run the simulation via http://localhost:5000:\ndocker-compose up geo-web-animation The screencast below shows a complete tour of a truck visiting all nodes in the defined order (Patisserie - Lost-and-found - Monkey\u0026rsquo;s - Carlsen Publishing - Patisserie).\nThere should have been a video here but your browser does not seem to support it. The one-way street Völckersstraße is correctly taken into account when moving from Monkeys Music Club (3., right-most node) to Carlsen Verlag (4., left-most node).\nOf course there are numerous improvements and extensions imaginable, including e.g. the calculation of more realistic driving times based on actual speed limits which are already part of the available OSM-data.\n","permalink":"https://fladdimir.github.io/post/csa-streetmap/","summary":"Building Anylogic\u0026rsquo;s GIS-Feature w/ SimPy, OSMnx and Leaflet.js","title":"Urban Logistics Network Simulation in Python"},{"content":"Reinforcement learning represents an emerging technique from machine learning. It can autonomously derive complex action sequences in dynamic environments and is successfully applied in various fields, e.g. from robotics and gaming. Instead of explicitly defining a specific solution strategy for a problem, we can just provide an environment. A self-learning agent will then autonomously discover successful strategies just by interaction.\nNeedless to say, there is nothing new under the moon and previous studies show the general feasibility of using RL for solving production-logistics problems.\nSo why do we think that there is the need for yet another article about this very topic?\nFirst, there is a lot of active development in RL, as well as in the application of Digital Twins in production/logistics. We believe that there lies even more potential in integrating these concepts. Furthermore, we found the often derogatory-treated \u0026ldquo;low-level implementation work\u0026rdquo; to be an actual obstacle for making progress in this challenging and highly inter-disciplinary area of applied research. This contribution strives to show a working example based on a tool-stack which seamlessly integrates two of the most popular open-source software packages from their respective areas: stable-baselines for RL and SimPy for implementing Digital Twins.\nGet the repo: https://github.com/fladdimir/tugger-routing\nSlides: https://fladdimir.github.io/presentations/des-python-rl.html\nPaper on researchgate\nIntroduction \u0026amp; Basics Reinforcement Learning If you still ask yourself what RL is capable of, we definitely recommend to have a look at what the guys from openai are doing.\nAdmittedly, thats probably a quite sophisticated and highly engineered example, but it breaks down to a simple interaction between an agent and an environment. Technically, this interaction is defined by an interface (or abstract base-class as Python likes to put it), which is part of the gym-package.\nThe graphic below illustrates the exchange of information between agent and environment. First, the agent calls the environment\u0026rsquo;s step method, providing the action to be executed. The environment then processes the action and returns:\nthe new state of the system (observation), the reward which occured during the step (might be zero), a done value potentially indicating the end of an episode (and the need for a subsequent reset) and an info-object (might contain additional information e.g. for logging purposes). The interface also prescribes more, such as the formats of action-space and observation_space, as well as render and reset behavior.\nThe various RL algorithms provided by the stable-baselines-package are ready to work with environments implementing this gym-interface. All that is left to do is creating a compliant environment - and in the next section we will show how this can be achieved in the domain of logistics.\nDigital Twins and Discrete Event Simulation Frankly, Digital Twin is probably the most overused buzzword of all the \u0026ldquo;Lostistics 4.0 / Industry 4.0\u0026rdquo; stuff that is out there. Even though we could not resist to put it into the title, from now on we\u0026rsquo;ll prove that we can do better and use the more specific term \u0026ldquo;Discrete Event Simulation\u0026rdquo; (DES).\nWhy DES? Discrete Event Simulation is one of the widespread tools for analysis and design of logistics systems. Today\u0026rsquo;s applications go beyond the traditional usage for systems planning. They include more operational use-cases such as virtual commissioning or short-term forecasts. Simulation models are getting integrated tightly into other IT-systems. This allows to increase process transparency and to improve our means to analyze, control, and optimize system performance in real-time. Doesn\u0026rsquo;t this sound pretty close to what Digital Twins always promise?\nMost industrial simulation uses are still based on commercial packages.\nHowever, there are a couple of open-source alternatives, which are typically closer to general-purpose language programming. Even though they tend to lack some convenient commercial features, there are upsides such as better scalability and simplified interfacing.\nRelated to Python we became aware of two popular DES packages: Salabim and SimPy. Both are not only free and open-source, but even built on top of the standard library of one of the world\u0026rsquo;s most popular programming languages - let\u0026rsquo;s see what we can get out of that!\nCasymda-Package Based on SimPy, we added bits of complementing functionality to gain some of the modeling convenience of commercial \u0026ldquo;block-based\u0026rdquo; DES-packages.\nCasymda facilitates the usage of bpmn-process-descriptions to generate corresponding simulation-model python-code. .bpmn-files (basically xml) can easily be created with the Camunda-Modeler.\nThis graphical modeling helps to maintain an overview of the high-level model-structure. Generated Casymda-models also include a generic, token-like animation of simulated processes out-of-the-box, ready to be run in a web-browser. For presentation and debugging, animations can be paused and their speed can be changed dynamically. Solely animation-related events are not scheduled if the simulation is run without visualization. This maximizes the execution speed - which becomes especially important related to RL, when a high number of runs is necessary.\nFurther features of Casymda include simulated movements along shortest paths in a 2D-tilemap-space, and gradual typing for development convenience (checkout pyright if you are using vscode).\nFor more info on Casymda have a look at the repo or the (German) website.\nWrapping a DES-Model in a Gym-Environment To be able to train an RL-agent inside a simulation model, we need to make the model implementing the Gym-interface described above.\nThe following diagram illustrates the coupling concept:\nWhen the step function of the Gym-Environment is called (1), the provided action is propagated to the relevant block of the simulation model (1.1). This is realized with help of an ActionHolder, so that a consuming piece of decision logic can dispatch according to the received information.\nSubsequently, the simulation is executed until a next_action_needed-Event is triggered by the simulation model (1.2). This is indicating the end of the current step and the need for another action of the agent.\nOne Gym-step can thus comprise an arbitrary number of discrete SimPy-steps, each of which can in turn take an arbitrary amount of simulated time.\nRewards are managed with help of a RewardHolder object, which is wired into the relevant blocks of the simulation model during environment initialization. At the end of each step, occured rewards are collected (1.3). Depending on the type of the optimization problem to solve, a post-processing of collected rewards can be applied (e.g. taking into account the amount elapsed time, so that an agent can learn time-efficient behavior).\nTo check whether an episode ended (the done part of the returned information), the current state of the model is checked against configured done_criteria (1.4). These can contain e.g. some goals to be reached or a certain amount of time to be simulated.\nTo provide the agent with an observation, a model-specific ModelStateToObservationConverter is used to collect relevant information from the model. The created observation conforms to the defined observation_space (1.5). This step could include e.g. counting the number of entities in different queues or checking inventory levels and creating a NumPy-array out of this information.\nFinally, collected information is returned to the agent (2), which can learn based on the reward and decide for the next action.\nHaving the basics covered, let\u0026rsquo;s see how we get this to work.\nCase-Study Back in August of last year at the MIM2019 in Berlin, we had the chance to attend an interesting talk of two Bavarian guys presenting their research on improving the tour-building for in-plant milk-run systems. These internal deliveries are commonly used for assembly line supply, and the tours are typically following a very rigid plan. Given the fact that the actual demand at the line tends to vary, their research revealed quite a lot of potential to decrease delivery lead times and to increase systems\u0026rsquo; utilization - just by making the tour-planning more dynamic.\nBased on this setting we constructed an abstracted and simplified version of an assembly line with a corresponding material supply system to provide a playground for reinforcement learning algorithms.\nScenario The image below shows a schematic layout plan of the system:\nUnfinished products enter the system on the upper right (I) and are assembled sequentially at 9 different stations, arranged in U-shape (I-IX). Finished products leave the system after the last assembly step (IX).\nStations require a certain amount of resource of either type A or B to be present in the station\u0026rsquo;s inventory before an assembly step can start.\nEach station can only hold one product at a time, and finished products can only be forwarded once the following station is empty (thus multiple upstream stations holding already finished products may be blocked by downstream stations which are still processing a product or waiting for material before being able to start processing).\nMaterial is supplied by a tugger, able to carry a limited discrete amount (\u0026ldquo;boxes\u0026rdquo;). The tugger can load material at a stock (A and/or B, located at the bottom). 1 discrete unit of material (\u0026ldquo;box\u0026rdquo;) can be loaded/unloaded at a time. The goal of the assembly line is achieving the maximal throughput, which also correlates with small lead-times of products.\nAssumptions:\nmaterial can only be loaded at the stocks (A and B), each of which holds an infinite amount of material, so that the tugger never waits for material at a loading site material can only be unloaded at a station actually requiring this type of material (hence a tugger cannot unload a box of A at a station which needs B for assembly) the inventory capacity at the stations (I-IX) is infinite, so that the tugger never waits at an unloading site (otherwise livelocks could occur where a tugger cannot unload material wherever it moves) System parameters Takt-time: processing time per station per product 60s Demand per product of stations type A 1.5 units Demand per product of stations type B 0.5 units Tugger movement speed 10 m/s Tugger capacity 25 units Amount of material (un-)loaded per step 5 units Time needed per (un-)loading step 5s Distances between stocks and stations (higher demands cause more frequent tours):\nRelation Simple Demand-weighted A -\u0026gt; T1 1096.40m 1644.60m B -\u0026gt; T2 926.40m 463.20m A -\u0026gt; T3 736.40m 1104.60m B -\u0026gt; T4 566.40m 283.20m A -\u0026gt; T5 234.10m 351.15m B -\u0026gt; T6 556.40m 278.20m A -\u0026gt; T7 726.40m 1089.60m B -\u0026gt; T8 916.40m 458.20m A -\u0026gt; T9 1086.40m 1629.60m The table below shows a simple throughput estimation by calculating the average cycle time of the tugger and the expected station utilization. The estimation assumes \u0026ldquo;full truck loads\u0026rdquo;, always completely loading at one stock (either A or B), and fully unloading at a station (T1 - T9).\nThroughput estimation Max throughput 24h 60/h x 24h = 1440 Demand / product 9.5 units Demand / time 9.5 / 60s = 0.16/s Average weighted distance 811.37m Average driving time 81.137s (Un-)loading time 25 units 25s Average cycle time (81.137s + 25s) x 2 = 212.274s Delivered units / cycle 25 Delivered units / time 0.12/s Average utilization 0.12/s / 0.16/s = 75% Expected throughput per min 75% x 60/min = 45/min Expected throughput per 24h ~1080/24h As we can see, the delivery performance of the tugger represents the limiting factor (bottleneck) of the system, which means that each improvement made here will be directly reflected by a corresponding increase in the overall throughput.\nFor the sake of simplicity, no stochastic model behaviour (such as e.g. randomly distributed loading or movement times) is assumed, hence the simulation model will be deterministic.\nAs stated: the system as a whole is quite abstracted and simplified - but still capturing at least some of the basic complexity inherent to real-world problems. Will our RL-agent be able to\u0026hellip;\ngrasp the underlying mechanics? distinguish different product types? discover the spots of demand and supply? deal with the limits of the tugger\u0026rsquo;s capacity? reach the maximal possible throughput? We\u0026rsquo;ll find out, but let\u0026rsquo;s first have a look at what the learning environment will look like.\nSimulation Model The simulation model of the system basically consists of 2 processes, both depicted in the graphic below.\nOn the left side, products pass through the 9 assembly steps (ProductStation, rotated U-shape) before leaving the system, occasionally being blocked by downstream stations or waiting for material at a station.\nOn the right side the tugger passes through an infinite cycle of movement and loading/unloading process steps (after initial creation at location A by a TuggerSource):\nthe next movement target is chosen and the movement is completed (no actual movement if the next target equals the current location) (TuggerMovement). Depending on the current location (being either a stock A/B) or a ProductStation, the next tugger process step is chosen: TuggerStock A loading of one unit of A (if tugger-capacity not reached) TuggerStock B loading of one unit of B (if tugger-capacity not reached) TuggerStation unloading of one unit of A or B if possible (material required by station is loaded) Note that even unsuccessful loading or unloading attempts are implemented to take a small, fixed amount of time, so that every possible Gym-step is guaranteed to take at least some simulated time (and a time-constrained episode is guaranteed to reach its end eventually).\nBelow you can see a process animation, as well as an animation of a tilemap. The agent here follows an explicitly defined simple rule of always delivering a complete load of 25 units to the station with the lowest inventory level. To run the animation just clone the repo, run the command, and visit http://localhost:5000.\nProcess animation:\ndocker-compose up web-animation-lia-process Tilemap animation:\ndocker-compose up web-animation-lia Preparing the Gym-Environment The TuggerEnv implements the Gym-Env interface and wraps the simulation model to be used for RL-agent training.\nGeneric functionalities like the mandatory step and reset functions and related helper methods are inherited and abstract/default parent-methods are overridden in a model-specific way as required (Template-Method Pattern):\ninitialize_action_and_reward_holder specifies which model blocks\u0026hellip; need access to gym-actions: TilemapMovement, choosing the next movement target based on the supplied target index number log achieved rewards: ProductSink, simply counting a reward of 1 for each finished product get_reward specifies how the elapsed time is taken into account for reward calculation check_if_model_is_done implements a model-specific check whether a certain amount of time has been simulated. One episode is scheduled to take 24h (86400s). The render method of the Gym-Env is not implemented, since animations at arbitrary moments in time - whenever a Gym-step is finished - do not make much sense for discrete event simulation environments. The animation is controlled separately.\nThe info return value of step is configured to return the number of finished_products which can then be logged.\nObservation- \u0026amp; Action-Space The model-specific extraction of the observation from the current model state is done by an instance of a TuggerEnvModelStateConverter which implements the ModelStateConverter \u0026ldquo;interface\u0026rdquo;.\nSpecifically, the observation consists of the following information which describes the current state of the system (overall 48 values):\nProductStation observations (5 values x 9 stations = 45 values): current inventory-level (normalized 0-1, counted up to a limit of 10 units) busy-state (binary) waiting_for_material-state (binary) empty-state (binary, whether a product is present or not) blocked-by-successor-state (binary) TuggerEntity observations (3 values x 1 tugger = 3 values): loaded amount of A (relative to capacity) loaded amount of B (relative to capacity) current location (index) Note that parts of a station observation can be seen to be redundant (e.g. a station which is neither busy nor waiting nor empty can only be blocked) - behind lies the rationale that an intelligent algorithm will (hopefully) learn an importance of different components of an observation, so that we do not have to worry about more than providing all potentially useful information.\nThe action_space (of type gym.spaces.Discrete) consists of the 11 possible movement targets (9 stations + 2 stocks, encoded by index).\nRewards As stated above, the defined goal of the assembly line is to achieve the best possible throughput of products, which corresponds to producing as many products as possible e.g. during one episode (24h).\nHow do we achieve that? Which kind of incentive is suitable to stimulate such a behavior? The design of appropriate reward functions is known to be a non-trivial matter. In fact, the design of rewards and incentives even for (arguably more intelligent) humans is a major problem in management and education (remember the last time you studied for passing an exam instead of actually learning useful contents).\nFor the environment at hand, we could just think about giving a single reward at the end of each episode, proportionally to the number of achieved products in that fixed amount of time (24h), which would probably properly reflect our aim of maximizing the throughput. However, the resulting reward would be quite sparse and therefore greatly decelerate learning speed (taking the average duration of a random action, each episode would take more than 1000 actions to complete before an agent sees any reward).\nAnother idea would be to reward every successful delivery of material to any station, which would be possible to be completed within 2 steps (movement to the stock \u0026amp; movement to a suitable station consuming the loaded material). This way we would get less sparse rewards, but also an obvious problem of exploitability, caused by the fact that the delivery of material to one station alone would actually never lead to the completion of any product at all.\nAs a compromise, we simply decided to go for a reward of 1 everytime a product completes its final assembly step, which is possible be completed within 12 steps (minimum, not necessarily an optimal strategy). Even exhibiting a random behavior, this would allow an agent to generate a reward of around 50 during one episode, so that there are sufficient \u0026ldquo;randomly succesful\u0026rdquo; samples to learn from.\nOne problem with this reward comes from the fact that the simulated time needed to obtain a reward is not reflected by the reward itself. Since every gym-step can actually eat up a greatly varying amount of simulation time (from 5 seconds to \u0026gt;100), there is a huge implicit impact on the throughput, which the agent is unaware of. To solve this problem we introduced \u0026ldquo;costs of time\u0026rdquo;, which means we simply give a small negative reward every step, proportional to the amount of simulated time that passed. This finally leaves us with the subsequent question of how big these \u0026ldquo;costs\u0026rdquo; should be. If set too high, they would just overrule any of the few actual rewards at the beginning of the training. If put too low, there would not be sufficient stimulus to exhibit time-efficient behavior at all. Again, as a simple compromise, we implemented the costs to grow proportionally with the highest reward seen so far at the end of an episode, which guarantees a certain balance, and rewards increasing time-efficiency.\nThe above described reward that we designed is definitely not \u0026ldquo;perfect\u0026rdquo; and also feels a bit like putting too much effort into \u0026ldquo;reward engineering\u0026rdquo; - nevertheless its a first solution our agents can hopefully work with\u0026hellip;\nRL-Agent Training \u0026amp; Evaluation The environment presented above is characterized by a Discrete action space and a continuous (Box) observations space. The stable-baselines documentation lists available RL algorithms and their compatibility.\nDue to the type of action space, some algorithms are not feasible (i.e. DDPG, SAC, and TD3).\nTo train a stable-baselines RL algorithm, the TuggerEnv is vectorized, using a DummyVecEnv and a standard MlpPolicy. To leverage multiple CPUs for training, it can be desirable to use a SubprocVecEnv (but for simpler logging \u0026amp; analysis we did not go with that one here, instead we did multiple independent training runs in parallel).\nTrain an ACER-agent (by default for 10,000 steps only, which should take \u0026lt;1min):\ndocker-compose up acer-training Plot performance (might require additional setup for connecting the display):\ndocker-compose up acer-plot-training Tilemap-animation of the trained agent (http://localhost:5000):\ndocker-compose up acer-web-animation-tilemap Below we can see an ACER-agent trained for 1m steps:\nAs we can see, the agent manages to fully load the 25 units onto the tugger most of the time, seems to target correct (A/B) stations for material unloading, and the choice of stations with a currently low inventory level seems reasonable too!\nBut how does the overall performance look like?\nPerformance Comparison For comparison we trained four algorithms (ACER, ACKTR, DQN, and PPO2) with standard settings for both 1 and 3 mio. (Gym-)steps. Training took up to 2.5 hours (DQN, 3mio. steps) on a 2.9GHz Intel i9, using a single-process DummyVecEnv as explained above.\nThe following graph shows the number of produced products per episode (24h) over the course of the training run for each algorithm, as well as the performance of the deterministic lowest-inventory heuristics (yellow line; always delivering a complete load of 25 units to the station with the currently lowest inventory), and the average performance of fully random actions (turquoise line, measured over 100 episodes).\nAs we can see, all of the algorithms manage to increase the number of produced products per episode significantly above the level reached by random actions (turquoise line at the bottom), indicating successful learning progress. Furthermore, none of the trained algorithms reaches the performance of the lowest-inventory-heuristics (yellow line at the top). The lowest-inventory-heuristics performance reaches the estimated maximum possible throughput of the system (estimated to appr. 1080/episode). This strategy can therefore be considered to be close to a global optimum. During training, a complete breakdown in performance can occur. Most prominently: ACER_3mio. (blue line, episode 260, no recovery at all). Other algorithms show drops in performance as well but seem to recover better (e.g. ACKTR - green, PPO2 - pink). The best-performing RL algorithm (ACER trained for 1mio. steps, orange line) reached a maximum throughput of 856 products / episode (78% of the near-optimal heuristics performance). The number of episodes varies due to the variable number of Gym-steps per episode (24h of simulated time), depending on the simulated time each Gym-step needs. The small number of episodes of the ACER_3mio. training is explained by the up to 17277 Gym-steps per episode, occurring from episode 260 on. Each step of such an episode takes only 5 seconds (the minimum possible time of all available Gym-steps, \u0026ldquo;achieved\u0026rdquo; by a repeated visit of the same location). This behavior might be caused by the defined negative reward per step, proportional to the amount of simulated time the step needed. Appearently, the agent does not remember how to generate a positive reward and only tries to maximize the short-term reward by minimizing the step-time. Obviously this behavior does not lead to any successful delivery, let alone completion of any product.\nIt is worth to be mentioned that all training runs were done with default algorithm settings, and that the evaluation of different hyperparameters is strongly recommended for performance optimization. Thus, it might not be improbable for an RL agent to close the performance gap towards the theoretically reachable optimum.\nSumming Up Short version: Our best RL agent reached about 78% of the best possible performance inside our production-logistics environment.\nOk, now is this good or bad?\nWell, one could be disappointed by the fact that our agent was not able to reach the performance of a hand-coded heuristics approach.\nBut did we believe when we started that we could get a generic piece of code to cope with the non-trivial relations of our specific and fairly complex environment? Certainly not!\nAnd this was just a first shot - we did not yet start with hyperparameter tuning or the evaluation of alternative rewards.\nWhat do your experiences with reinforcement learning look like?\nWhich logistics problems did you solve with RL?\nDid you spot a bug somewhere in the code or do you want to suggest an improvement?\nOr do you have questions concerning the presented implementation/toolstack?\nJust feel free to drop us a note, thanks for reading!\nWladimir Hofmann - Clemens L. Schwarz - Fredrik Branding\n","permalink":"https://fladdimir.github.io/post/tugger-routing/","summary":"Case-Study on Reinforcement-Learning in Logistics","title":"Digital Twin for Assembly Line Material Provisioning Planning"},{"content":"Discrete Event Simulation is a widespread tool for analysis and design of logistics systems.\nHowever, most industrial applications are realized using commercial simulators, and existing open-source alternatives still seem to lack some of the convenient features commercial packages are offering.\nThe Casymda-package strives to add some of these features, building on established standards such as BPMN and SimPy.\nThis post will introduce main features and show how to run a simple example.\nGet the repo.\nExisting Discrete Event Simulation Packages Current applications of DES go beyond the traditional usage for systems planning. They include more operational use-cases such as virtual commissioning or short-term forecasts \u0026amp; optimization. Consequently, simulation models are getting integrated tightly into other IT-systems. This allows to increase process transparency and to improve our means to analyze, control, and optimize system performance.\nMost industrial simulation uses are still based on commercial packages.\nAnd there are good reasons for this.\nIf you\u0026rsquo;ve already worked with commercial DES packages (such as Anylogic/Arena/ExtendSim/FlexSim/PlantSimulation/Simio/\u0026hellip;YouNameIt), you probably learned to like some typical characteristics:\nblock-based graphical modeling of processes (often) vendor-specific scripting possibilities to define custom behavior and non-standard procedures (this is where things get fun) (3D) visualization / animation capabilities for debug / validation / presentation (fun as well, most of the time) domain-specific libraries with objects providing pre-built industry-specific behaviors (configuration and customization) interfacing options: spreadsheets, databases, socket, COM,\u0026hellip; (what\u0026rsquo;s the last one?) However, there are a couple of open-source alternatives too.\nEven though they tend to lack some of the commercial features described above, there are upsides such as better scalability and simplified interfacing.\nRelated to Python there are (at least) two popular DES packages: Salabim and SimPy. Both are not only free and open-source, but even built on top of the standard library of one of the world\u0026rsquo;s most popular programming languages.\nCasymda: Features \u0026amp; Implementation Based on SimPy3, Casymda adds bits of complementing functionality to gain some of the modeling convenience of commercial \u0026ldquo;block-based\u0026rdquo; DES-packages.\nIt facilitates the usage of bpmn-process-descriptions to generate corresponding simulation-model python-code. .bpmn-files (basically xml) can easily be created with the Camunda-Modeler.\nThis graphical modeling helps to maintain an overview of the high-level model-structure. Generated Casymda-models also include a generic, token-like animation of simulated processes out-of-the-box, ready to be run in a web-browser. For presentation and debugging, animations can be paused and their speed can be changed dynamically. Solely animation-related events are not scheduled if the simulation is run without visualization. This maximizes the execution speed - which becomes especially important when a high number of runs is necessary.\nFurther features of Casymda include simulated movements along shortest paths in a 2D-tilemap-space, and gradual typing for development convenience (checkout pyright if you are using vscode).\nLet\u0026rsquo;s have a quick look at a simple example Casymda-model, illustrating some basic ideas.\nThe animation below shows a simple process consisting of 3 Casymda-blocks: Entities are created in a Source, pass a process-step called TilemapMovement, and leave the system via a Sink.\nTo run the example just enter the following command from within the repository and visit http://localhost:5000:\ndocker-compose up web-animation-tilemap-simple-process Created block-objects can be parametrized with arguments provided as text-annotations (e.g. the inter_arrival_time=100 [seconds] of the entities at the source). The naming of blocks is parsed following the pattern ClassName:instance_name. The default process-animation includes additional information on the processed entities and on the state of each block.\nThe way in which animations are created by Casymda is inspired by the Anylogic implementation. A web-server is running the actual simulation, and providing frame information to be rendered by a browser. The corresponding Casymda functionality is implemented using flask and pixijs.\nBelow you can find an tilemap-animation of the TilemapMovement process step of the model. As specified in the text-annotation, entities move from_node=\u0026quot;A\u0026quot;, to_node=\u0026quot;C\u0026quot;. Tilemaps can be provided in .csv format, with field values indicating possible origin/destination nodes (e.g. A, B, and C), passable nodes (0), and impassable nodes (1). The size of each tile is configured as part of the tilemap configuration. Shortest paths and distances between all origin/destination nodes are computed using networkx.\nTo run the tilemap animation example just enter the following command and visit: http://localhost:5000\ndocker-compose up web-animation-tilemap-simple Additional resources For additional information, feel free to have a look at other projects built with Casymda (German).\nInterested in machine-learning? The next post will show how to use a Casymda-model to train a reinforcement learning algorithm to solve a production logistics problem - stay tuned ;)\n","permalink":"https://fladdimir.github.io/post/casymda/","summary":"A Python-Package for Animated Discrete-Event-Simulation based on BPMN and SimPy","title":"Introducing Casymda"}]