[{"authors":["fladdimir"],"categories":null,"content":"Wladimir is a Software Engineer currently working in Hamburg with Lufthansa Industry Solutions, where he is part of an agile development team.\nBeing born in Dresden and completing high-school in Berlin, he later lived in Magdeburg, Grenoble, and Tampere.\nWhile studying he specialized in logistics simulation technologies. Completed projects span various domains, ranging from urban transport optimization to digital twins of manufacturing systems.\nIn his free time you may find him tinkering around with new technologies, doing calisthenics, or just spending time with his fiancée and friends in Hamburg-Altona.\n","date":1585958400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1585958400,"objectID":"ac3c180d339617d6149c90522af90de9","permalink":"/authors/fladdimir/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/fladdimir/","section":"authors","summary":"Wladimir is a Software Engineer currently working in Hamburg with Lufthansa Industry Solutions, where he is part of an agile development team.\nBeing born in Dresden and completing high-school in Berlin, he later lived in Magdeburg, Grenoble, and Tampere.","tags":null,"title":"Wladimir Hofmann","type":"authors"},{"authors":["Wladimir Hofmann"],"categories":null,"content":"A really nice and quite unique feature of Anylogic is the possibility to include GIS-maps into simulation models. It allows to place elements on a map and move them along existing routes, based on real spatial information. This is cool because it can be used to simulate entire supply chains, including means to provide a great, tangible visualization for complex problems.\nIn a previous project I used Anylogic to evaluate different designs and delivery schemes for a more sustainable urban logistics network in the city center of Grenoble in France. The data-driven simulation model allowed to quickly calculate KPIs for various transshipment node locations and different types of transport equipment in a multi-tier supply-chain network.\nBased on the success and positive feedback for their feature, Anylogic even built anyLogistix, a \u0026ldquo;spin-off\u0026rdquo; product combining pre-built \u0026amp; customizable simulation models with a commercial solver for integrated supply chain planning \u0026amp; optimization.\nObviously, those commercial features come at a price, so let\u0026rsquo;s see whether we can also realize this kind of model using open-source technologies.\nThe simulation model presented in this post will be based on a mini-case-study.\n   Get the repo.\n  Scenario \u0026amp; Scope Facing the fact that humanity gets more and more used to home-delivery of even everyday necessities, the small French bakery Die Patisserie in Hamburg-Altona wants to deliver sweet pastries to nearby customers.\nThey identified 3 major purchasers: the lost-and-found office Zentrales Fundbüro, Monkeys Music Club, and the publishing house Carlsen Verlag.\nCoordinates of the different locations:\n    Node Name Lat Lon     1. PAT Die Patisserie 53.55668 9.92815   2. ZFB Zentrales Fundbüro (lost-and-found office) 53.55817 9.92829   3. MMC Monkeys Music Club 53.55706 9.93161   4. CAV Carlsen Verlag (publishing house) 53.55703 9.92684    For the sake of simplicity, the order in which nodes are visited is assumed to be fixed. The tour starts \u0026amp; ends at the patisserie:\n Simulation Model The simulation model for the simplistic scenario is created with:\n OSMnx/networkx for retrieving geo-information and calculating shortest-paths SimPy/Casymda for simulating the tour Leaflet.js for browser-based animation  1. OSMnx The awesome OSMnx package provides the possibility to obtain a networkx-graph representation of a street-network from OpenStreetMap with a single line of code. A relevant section for our scenario can be obtained by specifying center and distance for osm-nodes to be included:\nCENTER = (53.55668, 9.92815) DISTANCE = 300 G = ox.graph_from_point(CENTER, distance=DISTANCE, network_type='drive')  OSMnx let\u0026rsquo;s us pick the nearest osm-node for each of the 4 locations of the tour, and also offers convenient means to plot the network. Blue dots represent osm-nodes, connected by edges. The 4 relevant locations are shown in red:\nTo prepare all information needed by the simulation model, we can now just compute all shortest paths between the 4 relevant nodes in the network with networkx, and include detailed information on all piece-wise linear segments for each route.\nThe results are pickled and saved to disk to avoid fetching and recalculation for each run of the model (of course its kind to keep the load for the OSM-server as low as possible).\nThe above described approach could be improved, e.g. by automatically determining the area to be loaded from the given relevant locations. Instead of just picking the closest osm-node for each relevant location, it would also be more precise to first go for the closest edge in the network. And as the network size grows, it might be a better idea to directly query the shortest path between relevant nodes from the OSM-server, instead of fetching the network as a whole (the way Anylogic seems to do it).\n2. Casymda/SimPy  Casymda provides block-based modeling of discrete-event-simulation models on top of SimPy.\nOur model can be characterized by a simple process:\nA truck is created at a parameterized Source and then processed at a custom DriveTour-block, which contains the logic for elapsing time according to the length of a route and the movement speed of the truck. It also contains the option to calculate intermediate locations for animation. The nodes to be visited are specified via the text-annotation stops=[\u0026quot;ZFB\u0026quot;, \u0026quot;MMC\u0026quot;, \u0026quot;CAV\u0026quot;].\nThe animation is implemented by exposing information and resources via flask (similar to the tilemap-animation described in the previous post).\n3. Leaflet.js To visualize the location of nodes and entities on a map, Leaflet only requires a few lines. For setting the rotation angle, there is the Rotated.Marker-plugin.\nmarker = L.marker(element.coords, { icon: new L.Icon({iconUrl: element.icon_path}) }).addTo(map); marker.bindPopup(element.text); marker.setRotationAngle(element.direction);   Result To run the simulation via http://localhost:5000:\ndocker-compose up geo-web-animation  The screencast below shows a complete tour of a truck visiting all nodes in the defined order (Patisserie - Lost-and-found - Monkey\u0026rsquo;s - Carlsen Publishing - Patisserie).\n  As you can notice, the one-way street Völckersstraße is correctly taken into account when moving from Monkeys Music Club (3., right-most node) to Carlsen Verlag (4., left-most node).\n Of course there are numerous improvements and extensions imaginable, including e.g. the calculation of more realistic driving times based on actual speed limits which are already part of the available OSM-data.\n","date":1585958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585958400,"objectID":"af0bc37ce2a741595c24c45ed53e0aa9","permalink":"/post/csa-streetmap/","publishdate":"2020-04-04T00:00:00Z","relpermalink":"/post/csa-streetmap/","section":"post","summary":"Implementing Anylogic's GIS-Feature w/ SimPy, OSMnx and Leaflet.js","tags":null,"title":"Urban Logistics Network Simulation in Python","type":"post"},{"authors":["Wladimir Hofmann","Clemens L. Schwarz","Fredrik Branding"],"categories":null,"content":"Artificial Intelligence, Digital Twins, Industry 4.0\u0026hellip; you might have heard these buzzwords before.\nBut wouldn\u0026rsquo;t it be cool to apply them, solving an actual problem?\nTo learn about practical application rather than abstract concepts?\nTo know which challenges to master, and which pitfalls to avoid?\nGreat, so this article is for you!\nIt will..\n describe how to use reinforcement learning to solve a production-logistics problem explain basic concepts of reinforcement learning and industrial digital twins present an in-depth case-study on assembly line supply strategy planning  And it will include code, ready for execution on your machine. Docker-based, no further installation needed.\n Get the repo: https://github.com/fladdimir/tugger-routing\n Disclaimer: This article is the result of leisure-time work. It represents our individual perspectives on the discussed topics. It\u0026rsquo;s a report capturing our personal experiences, rather than a purely scientific study. The content, concepts, and their implementation can probably be improved in virtually any imaginable way.\nWe still hope that you enjoy reading.\nOutline   Introduction \u0026amp; Basics   Reinforcement Learning  Digital Twins and Discrete Event Simulation  Wrapping a DES-Model in a Gym-Environment    Case-Study   Scenario  Simulation Model  Preparing the Gym-Environment  RL-Agent Training \u0026amp; Evaluation  Performance Comparison    Summing Up  Introduction \u0026amp; Basics Reinforcement learning represents an emerging technique from machine learning. It can autonomously derive complex action sequences in dynamic environments and is successfully applied in various fields, e.g. from robotics and gaming. Instead of explicitly defining a specific solution strategy for a problem, we can just provide an environment. A self-learning agent will then autonomously discover successful strategies just by interaction.\nNeedless to say, there is nothing new under the moon and previous studies show the general feasibility of using RL for solving production-logistics problems.\nSo why do we think that there is the need for yet another article about this very topic?\nFirst, there is a lot of active development in RL, as well as in the application of Digital Twins in production/logistics. We believe that there lies even more potential in integrating these concepts. Furthermore, we found the often derogatory-treated \u0026ldquo;low-level implementation work\u0026rdquo; to be an actual obstacle for making progress in this challenging and highly inter-disciplinary area of applied research. This contribution strives to show a working example based on a tool-stack which seamlessly integrates two of the most popular open-source software packages from their respective areas: stable-baselines for RL and SimPy for implementing Digital Twins.\nReinforcement Learning If you still ask yourself what RL is capable of, we definitely recommend to have a look at what the guys from openai are doing.\nAdmittedly, thats probably a quite sophisticated and highly engineered example, but it breaks down to a simple interaction between an agent and an environment. Technically, this interaction is defined by an interface (or abstract base-class as Python likes to put it), which is part of the gym-package.\nThe graphic below illustrates the exchange of information between agent and environment. First, the agent calls the environment\u0026rsquo;s step method, providing the action to be executed. The environment then processes the action and returns:\n the new state of the system (observation), the reward which occured during the step (might be zero), a done value potentially indicating the end of an episode (and the need for a subsequent reset) and an info-object (might contain additional information e.g. for logging purposes).  The interface also prescribes more, such as the formats of action-space and observation_space, as well as render and reset behavior.\nThe various RL algorithms provided by the stable-baselines-package are ready to work with environments implementing this gym-interface. All that is left to do is creating a compliant environment - and in the next section we will show how this can be achieved in the domain of logistics.\nDigital Twins and Discrete Event Simulation Frankly, Digital Twin is probably the most overused buzzword of all the \u0026ldquo;Lostistics 4.0 / Industry 4.0\u0026rdquo; stuff that is out there. Even though we could not resist to put it into the title, from now on we\u0026rsquo;ll prove that we can do better and use the more specific term \u0026ldquo;Discrete Event Simulation\u0026rdquo; (DES).\nWhy DES? Discrete Event Simulation is one of the widespread tools for analysis and design of logistics systems. Today\u0026rsquo;s applications go beyond the traditional usage for systems planning. They include more operational use-cases such as virtual commissioning or short-term forecasts. Simulation models are getting integrated tightly into other IT-systems. This allows to increase process transparency and to improve our means to analyze, control, and optimize system performance in real-time. Doesn\u0026rsquo;t this sound pretty close to what Digital Twins always promise?\nMost industrial simulation uses are still based on commercial packages.\nHowever, there are a couple of open-source alternatives, which are typically closer to general-purpose language programming. Even though they tend to lack some convenient commercial features, there are upsides such as better scalability and simplified interfacing.\nRelated to Python we became aware of two popular DES packages: Salabim and SimPy. Both are not only free and open-source, but even built on top of the standard library of one of the world\u0026rsquo;s most popular programming languages - let\u0026rsquo;s see what we can get out of that!\nCasymda-Package Based on SimPy, we added bits of complementing functionality to gain some of the modeling convenience of commercial \u0026ldquo;block-based\u0026rdquo; DES-packages.\nCasymda facilitates the usage of bpmn-process-descriptions to generate corresponding simulation-model python-code. .bpmn-files (basically xml) can easily be created with the Camunda-Modeler.\nThis graphical modeling helps to maintain an overview of the high-level model-structure. Generated Casymda-models also include a generic, token-like animation of simulated processes out-of-the-box, ready to be run in a web-browser. For presentation and debugging, animations can be paused and their speed can be changed dynamically. Solely animation-related events are not scheduled if the simulation is run without visualization. This maximizes the execution speed - which becomes especially important related to RL, when a high number of runs is necessary.\nFurther features of Casymda include simulated movements along shortest paths in a 2D-tilemap-space, and gradual typing for development convenience (checkout pyright if you are using vscode).\nFor more info on Casymda have a look at the repo or the (German) website.\nWrapping a DES-Model in a Gym-Environment To be able to train an RL-agent inside a simulation model, we need to make the model implementing the Gym-interface described above.\nThe following diagram illustrates the coupling concept:\nWhen the step function of the Gym-Environment is called (1), the provided action is propagated to the relevant block of the simulation model (1.1). This is realized with help of an ActionHolder, so that a consuming piece of decision logic can dispatch according to the received information.\nSubsequently, the simulation is executed until a next_action_needed-Event is triggered by the simulation model (1.2). This is indicating the end of the current step and the need for another action of the agent.\n One Gym-step can thus comprise an arbitrary number of discrete SimPy-steps, each of which can in turn take an arbitrary amount of simulated time.\n Rewards are managed with help of a RewardHolder object, which is wired into the relevant blocks of the simulation model during environment initialization. At the end of each step, occured rewards are collected (1.3). Depending on the type of the optimization problem to solve, a post-processing of collected rewards can be applied (e.g. taking into account the amount elapsed time, so that an agent can learn time-efficient behavior).\nTo check whether an episode ended (the done part of the returned information), the current state of the model is checked against configured done_criteria (1.4). These can contain e.g. some goals to be reached or a certain amount of time to be simulated.\nTo provide the agent with an observation, a model-specific ModelStateToObservationConverter is used to collect relevant information from the model. The created observation conforms to the defined observation_space (1.5). This step could include e.g. counting the number of entities in different queues or checking inventory levels and creating a NumPy-array out of this information.\nFinally, collected information is returned to the agent (2), which can learn based on the reward and decide for the next action.\nHaving the basics covered, let\u0026rsquo;s see how we get this to work.\nCase-Study Back in August of last year at the MIM2019 in Berlin, we had the chance to attend an interesting talk of two Bavarian guys presenting their research on improving the tour-building for in-plant milk-run systems. These internal deliveries are commonly used for assembly line supply, and the tours are typically following a very rigid plan. Given the fact that the actual demand at the line tends to vary, their research revealed quite a lot of potential to decrease delivery lead times and to increase systems\u0026rsquo; utilization - just by making the tour-planning more dynamic.\nBased on this setting we constructed an abstracted and simplified version of an assembly line with a corresponding material supply system to provide a playground for reinforcement learning algorithms.\nScenario The image below shows a schematic layout plan of the system:\nUnfinished products enter the system on the upper right (I) and are assembled sequentially at 9 different stations, arranged in U-shape (I-IX). Finished products leave the system after the last assembly step (IX).\nStations require a certain amount of resource of either type A or B to be present in the station\u0026rsquo;s inventory before an assembly step can start.\nEach station can only hold one product at a time, and finished products can only be forwarded once the following station is empty (thus multiple upstream stations holding already finished products may be blocked by downstream stations which are still processing a product or waiting for material before being able to start processing).\nMaterial is supplied by a tugger, able to carry a limited discrete amount (\u0026ldquo;boxes\u0026rdquo;). The tugger can load material at a stock (A and/or B, located at the bottom). 1 discrete unit of material (\u0026ldquo;box\u0026rdquo;) can be loaded/unloaded at a time. The goal of the assembly line is achieving the maximal throughput, which also correlates with small lead-times of products.\nAssumptions:\n material can only be loaded at the stocks (A and B), each of which holds an infinite amount of material, so that the tugger never waits for material at a loading site material can only be unloaded at a station actually requiring this type of material (hence a tugger cannot unload a box of A at a station which needs B for assembly) the inventory capacity at the stations (I-IX) is infinite, so that the tugger never waits at an unloading site (otherwise livelocks could occur where a tugger cannot unload material wherever it moves)     System parameters      Takt-time: processing time per station per product 60s   Demand per product of stations type A 1.5 units   Demand per product of stations type B 0.5 units   Tugger movement speed 10 m/s   Tugger capacity 25 units   Amount of material (un-)loaded per step 5 units   Time needed per (un-)loading step 5s    Distances between stocks and stations (higher demands cause more frequent tours):\n   Relation Simple Demand-weighted     A -\u0026gt; T1 1096.40m 1644.60m   B -\u0026gt; T2 926.40m 463.20m   A -\u0026gt; T3 736.40m 1104.60m   B -\u0026gt; T4 566.40m 283.20m   A -\u0026gt; T5 234.10m 351.15m   B -\u0026gt; T6 556.40m 278.20m   A -\u0026gt; T7 726.40m 1089.60m   B -\u0026gt; T8 916.40m 458.20m   A -\u0026gt; T9 1086.40m 1629.60m    The table below shows a simple throughput estimation by calculating the average cycle time of the tugger and the expected station utilization. The estimation assumes \u0026ldquo;full truck loads\u0026rdquo;, always completely loading at one stock (either A or B), and fully unloading at a station (T1 - T9).\n   Throughput estimation      Max throughput 24h 60/h x 24h = 1440   Demand / product 9.5 units   Demand / time 9.5 / 60s = 0.16/s   Average weighted distance 811.37m   Average driving time 81.137s   (Un-)loading time 25 units 25s   Average cycle time (81.137s + 25s) x 2 = 212.274s   Delivered units / cycle 25   Delivered units / time 0.12/s   Average utilization 0.12/s / 0.16/s = 75%   Expected throughput per min 75% x 60/min = 45/min   Expected throughput per 24h ~1080/24h    As we can see, the delivery performance of the tugger represents the limiting factor (bottleneck) of the system, which means that each improvement made here will be directly reflected by a corresponding increase in the overall throughput.\nFor the sake of simplicity, no stochastic model behaviour (such as e.g. randomly distributed loading or movement times) is assumed, hence the simulation model will be deterministic.\nAs stated: the system as a whole is quite abstracted and simplified - but still capturing at least some of the basic complexity inherent to real-world problems. Will our RL-agent be able to\u0026hellip;\n grasp the underlying mechanics? distinguish different product types? discover the spots of demand and supply? deal with the limits of the tugger\u0026rsquo;s capacity? reach the maximal possible throughput?  We\u0026rsquo;ll find out, but let\u0026rsquo;s first have a look at what the learning environment will look like.\nSimulation Model The simulation model of the system basically consists of 2 processes, both depicted in the graphic below.\nOn the left side, products pass through the 9 assembly steps (ProductStation, rotated U-shape) before leaving the system, occasionally being blocked by downstream stations or waiting for material at a station.\nOn the right side the tugger passes through an infinite cycle of movement and loading/unloading process steps (after initial creation at location A by a TuggerSource):\n the next movement target is chosen and the movement is completed (no actual movement if the next target equals the current location) (TuggerMovement). Depending on the current location (being either a stock A/B) or a ProductStation, the next tugger process step is chosen:  TuggerStock A loading of one unit of A (if tugger-capacity not reached) TuggerStock B loading of one unit of B (if tugger-capacity not reached) TuggerStation unloading of one unit of A or B if possible (material required by station is loaded)    Note that even unsuccessful loading or unloading attempts are implemented to take a small, fixed amount of time, so that every possible Gym-step is guaranteed to take at least some simulated time (and a time-constrained episode is guaranteed to reach its end eventually).\nBelow you can see a process animation, as well as an animation of a tilemap. The agent here follows an explicitly defined simple rule of always delivering a complete load of 25 units to the station with the lowest inventory level. To run the animation just clone the repo, run the command, and visit http://localhost:5000.\nProcess animation:\ndocker-compose up web-animation-lia-process  Tilemap animation:\ndocker-compose up web-animation-lia  Preparing the Gym-Environment The TuggerEnv implements the Gym-Env interface and wraps the simulation model to be used for RL-agent training.\nGeneric functionalities like the mandatory step and reset functions and related helper methods are inherited and abstract/default parent-methods are overridden in a model-specific way as required (Template-Method Pattern):\n initialize_action_and_reward_holder specifies which model blocks\u0026hellip;  need access to gym-actions: TilemapMovement, choosing the next movement target based on the supplied target index number log achieved rewards: ProductSink, simply counting a reward of 1 for each finished product   get_reward specifies how the elapsed time is taken into account for reward calculation check_if_model_is_done implements a model-specific check whether a certain amount of time has been simulated. One episode is scheduled to take 24h (86400s).  The render method of the Gym-Env is not implemented, since animations at arbitrary moments in time - whenever a Gym-step is finished - do not make much sense for discrete event simulation environments. The animation is controlled separately.\nThe info return value of step is configured to return the number of finished_products which can then be logged.\nObservation- \u0026amp; Action-Space The model-specific extraction of the observation from the current model state is done by an instance of a TuggerEnvModelStateConverter which implements the ModelStateConverter \u0026ldquo;interface\u0026rdquo;.\nSpecifically, the observation consists of the following information which describes the current state of the system (overall 48 values):\n ProductStation observations (5 values x 9 stations = 45 values):  current inventory-level (normalized 0-1, counted up to a limit of 10 units) busy-state (binary) waiting_for_material-state (binary) empty-state (binary, whether a product is present or not) blocked-by-successor-state (binary)   TuggerEntity observations (3 values x 1 tugger = 3 values):  loaded amount of A (relative to capacity) loaded amount of B (relative to capacity) current location (index)    Note that parts of a station observation can be seen to be redundant (e.g. a station which is neither busy nor waiting nor empty can only be blocked) - behind lies the rationale that an intelligent algorithm will (hopefully) learn an importance of different components of an observation, so that we do not have to worry about more than providing all potentially useful information.\nThe action_space (of type gym.spaces.Discrete) consists of the 11 possible movement targets (9 stations + 2 stocks, encoded by index).\nRewards As stated above, the defined goal of the assembly line is to achieve the best possible throughput of products, which corresponds to producing as many products as possible e.g. during one episode (24h).\nHow do we achieve that? Which kind of incentive is suitable to stimulate such a behavior? The design of appropriate reward functions is known to be a non-trivial matter. In fact, the design of rewards and incentives even for (arguably more intelligent) humans is a major problem in management and education (remember the last time you studied for passing an exam instead of actually learning useful contents).\nFor the environment at hand, we could just think about giving a single reward at the end of each episode, proportionally to the number of achieved products in that fixed amount of time (24h), which would probably properly reflect our aim of maximizing the throughput. However, the resulting reward would be quite sparse and therefore greatly decelerate learning speed (taking the average duration of a random action, each episode would take more than 1000 actions to complete before an agent sees any reward).\nAnother idea would be to reward every successful delivery of material to any station, which would be possible to be completed within 2 steps (movement to the stock \u0026amp; movement to a suitable station consuming the loaded material). This way we would get less sparse rewards, but also an obvious problem of exploitability, caused by the fact that the delivery of material to one station alone would actually never lead to the completion of any product at all.\nAs a compromise, we simply decided to go for a reward of 1 everytime a product completes its final assembly step, which is possible be completed within 12 steps (minimum, not necessarily an optimal strategy). Even exhibiting a random behavior, this would allow an agent to generate a reward of around 50 during one episode, so that there are sufficient \u0026ldquo;randomly succesful\u0026rdquo; samples to learn from.\nOne problem with this reward comes from the fact that the simulated time needed to obtain a reward is not reflected by the reward itself. Since every gym-step can actually eat up a greatly varying amount of simulation time (from 5 seconds to \u0026gt;100), there is a huge implicit impact on the throughput, which the agent is unaware of. To solve this problem we introduced \u0026ldquo;costs of time\u0026rdquo;, which means we simply give a small negative reward every step, proportional to the amount of simulated time that passed. This finally leaves us with the subsequent question of how big these \u0026ldquo;costs\u0026rdquo; should be. If set too high, they would just overrule any of the few actual rewards at the beginning of the training. If put too low, there would not be sufficient stimulus to exhibit time-efficient behavior at all. Again, as a simple compromise, we implemented the costs to grow proportionally with the highest reward seen so far at the end of an episode, which guarantees a certain balance, and rewards increasing time-efficiency.\nThe above described reward that we designed is definitely not \u0026ldquo;perfect\u0026rdquo; and also feels a bit like putting too much effort into \u0026ldquo;reward engineering\u0026rdquo; - nevertheless its a first solution our agents can hopefully work with\u0026hellip;\nRL-Agent Training \u0026amp; Evaluation The environment presented above is characterized by a Discrete action space and a continuous (Box) observations space. The stable-baselines documentation lists available RL algorithms and their compatibility.\nDue to the type of action space, some algorithms are not feasible (i.e. DDPG, SAC, and TD3).\nTo train a stable-baselines RL algorithm, the TuggerEnv is vectorized, using a DummyVecEnv and a standard MlpPolicy. To leverage multiple CPUs for training, it can be desirable to use a SubprocVecEnv (but for simpler logging \u0026amp; analysis we did not go with that one here, instead we did multiple independent training runs in parallel).\nTrain an ACER-agent (by default for 10,000 steps only, which should take \u0026lt;1min):\ndocker-compose up acer-training  Plot performance (might require additional setup for connecting the display):\ndocker-compose up acer-plot-training  Tilemap-animation of the trained agent (http://localhost:5000):\ndocker-compose up acer-web-animation-tilemap  Below we can see an ACER-agent trained for 1m steps:\n As we can see, the agent manages to fully load the 25 units onto the tugger most of the time, seems to target correct (A/B) stations for material unloading, and the choice of stations with a currently low inventory level seems reasonable too!\n But how does the overall performance look like?\nPerformance Comparison For comparison we trained four algorithms (ACER, ACKTR, DQN, and PPO2) with standard settings for both 1 and 3 mio. (Gym-)steps. Training took up to 2.5 hours (DQN, 3mio. steps) on a 2.9GHz Intel i9, using a single-process DummyVecEnv as explained above.\nThe following graph shows the number of produced products per episode (24h) over the course of the training run for each algorithm, as well as the performance of the deterministic lowest-inventory heuristics (yellow line; always delivering a complete load of 25 units to the station with the currently lowest inventory), and the average performance of fully random actions (turquoise line, measured over 100 episodes).\n As we can see, all of the algorithms manage to increase the number of produced products per episode significantly above the level reached by random actions (turquoise line at the bottom), indicating successful learning progress. Furthermore, none of the trained algorithms reaches the performance of the lowest-inventory-heuristics (yellow line at the top). The lowest-inventory-heuristics performance reaches the estimated maximum possible throughput of the system (estimated to appr. 1080/episode). This strategy can therefore be considered to be close to a global optimum. During training, a complete breakdown in performance can occur. Most prominently: ACER_3mio. (blue line, episode 260, no recovery at all). Other algorithms show drops in performance as well but seem to recover better (e.g. ACKTR - green, PPO2 - pink). The best-performing RL algorithm (ACER trained for 1mio. steps, orange line) reached a maximum throughput of 856 products / episode (78% of the near-optimal heuristics performance).  The number of episodes varies due to the variable number of Gym-steps per episode (24h of simulated time), depending on the simulated time each Gym-step needs. The small number of episodes of the ACER_3mio. training is explained by the up to 17277 Gym-steps per episode, occurring from episode 260 on. Each step of such an episode takes only 5 seconds (the minimum possible time of all available Gym-steps, \u0026ldquo;achieved\u0026rdquo; by a repeated visit of the same location). This behavior might be caused by the defined negative reward per step, proportional to the amount of simulated time the step needed. Appearently, the agent does not remember how to generate a positive reward and only tries to maximize the short-term reward by minimizing the step-time. Obviously this behavior does not lead to any successful delivery, let alone completion of any product.\nIt is worth to be mentioned that all training runs were done with default algorithm settings, and that the evaluation of different hyperparameters is strongly recommended for performance optimization. Thus, it might not be improbable for an RL agent to close the performance gap towards the theoretically reachable optimum.\nSumming Up Short version: Our best RL agent reached about 78% of the best possible performance inside our production-logistics environment.\nOk, now is this good or bad?\nWell, one could be disappointed by the fact that our agent was not able to reach the performance of a hand-coded heuristics approach.\nBut did we believe when we started that we could get a generic piece of code to cope with the non-trivial relations of our specific and fairly complex environment? Certainly not!\nAnd this was just a first shot - we did not yet start with hyperparameter tuning or the evaluation of alternative rewards.\nWhat do your experiences with reinforcement learning look like?\nWhich logistics problems did you solve with RL?\nDid you spot a bug somewhere in the code or do you want to suggest an improvement?\nOr do you have questions concerning the presented implementation/toolstack?\nJust feel free to drop us a note, thanks for reading!\n Wladimir Hofmann - Clemens L. Schwarz - Fredrik Branding\n","date":1584748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584748800,"objectID":"84970bbd3d98a4f423090d18e66425bb","permalink":"/post/tugger-routing/","publishdate":"2020-03-21T00:00:00Z","relpermalink":"/post/tugger-routing/","section":"post","summary":"A Hands-on Introduction to Reinforcement-Learning in Logistics","tags":null,"title":"Using a Digital Twin for Assembly Line Supply Strategy Planning","type":"post"},{"authors":["Wladimir Hofmann"],"categories":null,"content":"Discrete Event Simulation is a widespread tool for analysis and design of logistics systems.\nHowever, most industrial applications are realized using commercial simulators, and existing open-source alternatives still seem to lack some of the convenient features commercial packages are offering.\nThe Casymda-package strives to add some of these features, building on established standards such as BPMN and SimPy.\nThis post will introduce main features and show how to run a simple example.\n Get the repo.\n Existing Discrete Event Simulation Packages Current applications of DES go beyond the traditional usage for systems planning. They include more operational use-cases such as virtual commissioning or short-term forecasts \u0026amp; optimization. Consequently, simulation models are getting integrated tightly into other IT-systems. This allows to increase process transparency and to improve our means to analyze, control, and optimize system performance.\nMost industrial simulation uses are still based on commercial packages.\nAnd there are good reasons for this.\nIf you\u0026rsquo;ve already worked with commercial DES packages (such as Anylogic/Arena/ExtendSim/FlexSim/PlantSimulation/Simio/\u0026hellip;YouNameIt), you probably learned to like some typical characteristics:\n block-based graphical modeling of processes (often) vendor-specific scripting possibilities to define custom behavior and non-standard procedures (this is where things get fun) (3D) visualization / animation capabilities for debug / validation / presentation (fun as well, most of the time) domain-specific libraries with objects providing pre-built industry-specific behaviors (configuration and customization) interfacing options: spreadsheets, databases, socket, COM,\u0026hellip; (what\u0026rsquo;s the last one?)  However, there are a couple of open-source alternatives too.\nEven though they tend to lack some of the commercial features described above, there are upsides such as better scalability and simplified interfacing.\nRelated to Python there are (at least) two popular DES packages: Salabim and SimPy. Both are not only free and open-source, but even built on top of the standard library of one of the world\u0026rsquo;s most popular programming languages.\nCasymda: Features \u0026amp; Implementation Based on SimPy3, Casymda adds bits of complementing functionality to gain some of the modeling convenience of commercial \u0026ldquo;block-based\u0026rdquo; DES-packages.\nIt facilitates the usage of bpmn-process-descriptions to generate corresponding simulation-model python-code. .bpmn-files (basically xml) can easily be created with the Camunda-Modeler.\nThis graphical modeling helps to maintain an overview of the high-level model-structure. Generated Casymda-models also include a generic, token-like animation of simulated processes out-of-the-box, ready to be run in a web-browser. For presentation and debugging, animations can be paused and their speed can be changed dynamically. Solely animation-related events are not scheduled if the simulation is run without visualization. This maximizes the execution speed - which becomes especially important when a high number of runs is necessary.\nFurther features of Casymda include simulated movements along shortest paths in a 2D-tilemap-space, and gradual typing for development convenience (checkout pyright if you are using vscode).\nLet\u0026rsquo;s have a quick look at a simple example Casymda-model, illustrating some basic ideas.\nThe animation below shows a simple process consisting of 3 Casymda-blocks: Entities are created in a Source, pass a process-step called TilemapMovement, and leave the system via a Sink.\nTo run the example just enter the following command from within the repository and visit http://localhost:5000:\ndocker-compose up web-animation-tilemap-simple-process  Created block-objects can be parametrized with arguments provided as text-annotations (e.g. the inter_arrival_time=100 [seconds] of the entities at the source). The naming of blocks is parsed following the pattern ClassName:instance_name. The default process-animation includes additional information on the processed entities and on the state of each block.\nThe way in which animations are created by Casymda is inspired by the Anylogic implementation. A web-server is running the actual simulation, and providing frame information to be rendered by a browser. The corresponding Casymda functionality is implemented using flask and pixijs.\nBelow you can find an tilemap-animation of the TilemapMovement process step of the model. As specified in the text-annotation, entities move from_node=\u0026quot;A\u0026quot;, to_node=\u0026quot;C\u0026quot;. Tilemaps can be provided in .csv format, with field values indicating possible origin/destination nodes (e.g. A, B, and C), passable nodes (0), and impassable nodes (1). The size of each tile is configured as part of the tilemap configuration. Shortest paths and distances between all origin/destination nodes are computed using networkx.\nTo run the tilemap animation example just enter the following command and visit: http://localhost:5000\ndocker-compose up web-animation-tilemap-simple  Additional resources For additional information, feel free to have a look at other projects built with Casymda (German).\nInterested in machine-learning? The next post will show how to use a Casymda-model to train a reinforcement learning algorithm to solve a production logistics problem - stay tuned ;)\n","date":1584662400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584662400,"objectID":"ac0e96e48e4078699830079dfbbe3f0f","permalink":"/post/casymda/","publishdate":"2020-03-20T00:00:00Z","relpermalink":"/post/casymda/","section":"post","summary":"A Python-Package for Animated Discrete-Event-Simulation based on BPMN and SimPy","tags":null,"title":"Introducing Casymda","type":"post"},{"authors":["Wladimir Hofmann","Fredrik Branding"],"categories":null,"content":"Presented at: 9th IFAC Conference on Manufacturing Modelling, Management and Control 2019, Berlin, Germany\nFull text on researchgate.\n","date":1566950400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566950400,"objectID":"2e24a158bf498d4b15206f774600b942","permalink":"/publication/mim2019/","publishdate":"2019-08-28T00:00:00Z","relpermalink":"/publication/mim2019/","section":"publication","summary":"Seaborne trade volumes are growing and the resulting traffic load on the road infrastructure in port areas calls for new solutions to improve handling and coordination of vehicles and shipments. In this contribution, a digital twin for truck dispatching operator assistance is presented, which enables the determination of optimal dispatching policies using simulation-based performance forecasts. Proprietary simulation software with limited interfaces, the application deployment in demanding industrial environments, and the integration of real-time sensor information represent three major challenges when realizing digital twin applications for logistics systems. This implementation, therefore, uses an extensible open-source simulation package and it is coupled with an IoT-platform for easy integration of real-time information. The digital twin is deployed as a cloud-based service, allowing for simple deployment and scalability.","tags":null,"title":"Implementation of an IoT- and Cloud-based Digital Twin for Real-Time Decision Support in Port Operations","type":"publication"},{"authors":["Wladimir Hofmann","Jan Hendrik Ulrich","Sebastian Lang","Tobias Reggelin","Juri Tolujew"],"categories":null,"content":"Presented at: 16th IFAC Symposium on Information Control Problems in Manufacturing, June 11-13 2018, Bergamo, Italy\nFull text on researchgate.\n","date":1528675200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528675200,"objectID":"6cc3ef54994f68b8354c46b8575df1c3","permalink":"/publication/incom2018/","publishdate":"2018-06-11T00:00:00Z","relpermalink":"/publication/incom2018/","section":"publication","summary":"Changeable material flow systems as well as the use of material flow simulation represent approaches to cope with the challenges of future logistics operations in times of growing complexity, frequently varying demands and shorter product lifecycles. Modular plug-and-play continuous conveying systems are a special type of changeable material flow systems suitable to meet the imposed requirements. This paper presents how simulation methods can be applied to support the development of modules for this kind of system. As an example, a simulation model for the virtual commissioning of modules for an “Industry 4.0 - Learning Factory” is described, which is currently developed at the Institute for Logistics and Material Handling Systems of the Otto-von-Guericke-University together with the partner institutions Fraunhofer IFF Magdeburg and TTI Riga, Latvia.","tags":null,"title":"Simulation and Virtual Commissioning of Modules for a Plug-and-Play Conveying System","type":"publication"},{"authors":["Wladimir Hofmann","Tom Assmann","Parisa Dolati Neghabadi","Van Dat Cung","Juri Tolujew"],"categories":null,"content":"Presented at: International Workshop on Simulation for Energy, Sustainable Development \u0026amp; Environment, September 18-20 2017, Barcelona, Spain\nFull text on researchgate.\n","date":1505692800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505692800,"objectID":"367266f93f5a50d6e616608075d10e10","permalink":"/publication/i3m2017/","publishdate":"2017-09-18T00:00:00Z","relpermalink":"/publication/i3m2017/","section":"publication","summary":"The use of cargo bikes for goods deliveries represents a promising concept of urban logistics. In this contribution, a new simulation-based assessment tool integrating this emission free vehicles in urban distribution systems is presented. First, typical schemes are identified and an analysis of the underlying planning problems is conducted. Second, the developed GIS-based discrete-event simulation model and the coupled tour-planning algorithm are described, implementing the pattern of control optimization. To the best of our knowledge, such a tool is not yet existing. Finally, the tool is applied, evaluating the potential use of cargo bikes for B2B-deliveries in the medium size city of Grenoble in France.","tags":null,"title":"A Simulation Tool to Assess the Integration of Cargo Bikes into an Urban Distribution System","type":"publication"}]