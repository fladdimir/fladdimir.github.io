<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Digital Twin for Assembly Line Material Provisioning Planning | wh</title><meta name=keywords content="Python,SimPy,Reinforcement Learning,OpenAI Gym,Slides"><meta name=description content="Case-Study on Reinforcement-Learning in Logistics"><meta name=author content><link rel=canonical href=https://fladdimir.github.io/post/tugger-routing/><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://fladdimir.github.io/icon.png><link rel=icon type=image/png sizes=16x16 href=https://fladdimir.github.io/icon.png><link rel=icon type=image/png sizes=32x32 href=https://fladdimir.github.io/icon.png><link rel=apple-touch-icon href=https://fladdimir.github.io/apple-touch-icon.png><link rel=mask-icon href=https://fladdimir.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script data-goatcounter=https://fmghio.goatcounter.com/count async src=//gc.zgo.at/count.js></script><meta property="og:title" content="Digital Twin for Assembly Line Material Provisioning Planning"><meta property="og:description" content="Case-Study on Reinforcement-Learning in Logistics"><meta property="og:type" content="article"><meta property="og:url" content="https://fladdimir.github.io/post/tugger-routing/"><meta property="og:image" content="https://fladdimir.github.io/post/tugger-routing/featured.png"><meta property="article:section" content="post"><meta property="article:published_time" content="2020-03-21T00:00:00+00:00"><meta property="article:modified_time" content="2020-03-21T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://fladdimir.github.io/post/tugger-routing/featured.png"><meta name=twitter:title content="Digital Twin for Assembly Line Material Provisioning Planning"><meta name=twitter:description content="Case-Study on Reinforcement-Learning in Logistics"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://fladdimir.github.io/post/"},{"@type":"ListItem","position":2,"name":"Digital Twin for Assembly Line Material Provisioning Planning","item":"https://fladdimir.github.io/post/tugger-routing/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Digital Twin for Assembly Line Material Provisioning Planning","name":"Digital Twin for Assembly Line Material Provisioning Planning","description":"Case-Study on Reinforcement-Learning in Logistics","keywords":["Python","SimPy","Reinforcement Learning","OpenAI Gym","Slides"],"articleBody":"Reinforcement learning represents an emerging technique from machine learning. It can autonomously derive complex action sequences in dynamic environments and is successfully applied in various fields, e.g. from robotics and gaming. Instead of explicitly defining a specific solution strategy for a problem, we can just provide an environment. A self-learning agent will then autonomously discover successful strategies just by interaction.\nNeedless to say, there is nothing new under the moon and previous studies show the general feasibility of using RL for solving production-logistics problems.\nSo why do we think that there is the need for yet another article about this very topic?\nFirst, there is a lot of active development in RL, as well as in the application of Digital Twins in production/logistics. We believe that there lies even more potential in integrating these concepts. Furthermore, we found the often derogatory-treated “low-level implementation work” to be an actual obstacle for making progress in this challenging and highly inter-disciplinary area of applied research. This contribution strives to show a working example based on a tool-stack which seamlessly integrates two of the most popular open-source software packages from their respective areas: stable-baselines for RL and SimPy for implementing Digital Twins.\nGet the repo: https://github.com/fladdimir/tugger-routing\nSlides: https://fladdimir.github.io/presentations/des-python-rl.html\nPaper on researchgate\nIntroduction \u0026 Basics Reinforcement Learning If you still ask yourself what RL is capable of, we definitely recommend to have a look at what the guys from openai are doing.\nAdmittedly, thats probably a quite sophisticated and highly engineered example, but it breaks down to a simple interaction between an agent and an environment. Technically, this interaction is defined by an interface (or abstract base-class as Python likes to put it), which is part of the gym-package.\nThe graphic below illustrates the exchange of information between agent and environment. First, the agent calls the environment’s step method, providing the action to be executed. The environment then processes the action and returns:\nthe new state of the system (observation), the reward which occured during the step (might be zero), a done value potentially indicating the end of an episode (and the need for a subsequent reset) and an info-object (might contain additional information e.g. for logging purposes). The interface also prescribes more, such as the formats of action-space and observation_space, as well as render and reset behavior.\nThe various RL algorithms provided by the stable-baselines-package are ready to work with environments implementing this gym-interface. All that is left to do is creating a compliant environment - and in the next section we will show how this can be achieved in the domain of logistics.\nDigital Twins and Discrete Event Simulation Frankly, Digital Twin is probably the most overused buzzword of all the “Lostistics 4.0 / Industry 4.0” stuff that is out there. Even though we could not resist to put it into the title, from now on we’ll prove that we can do better and use the more specific term “Discrete Event Simulation” (DES).\nWhy DES? Discrete Event Simulation is one of the widespread tools for analysis and design of logistics systems. Today’s applications go beyond the traditional usage for systems planning. They include more operational use-cases such as virtual commissioning or short-term forecasts. Simulation models are getting integrated tightly into other IT-systems. This allows to increase process transparency and to improve our means to analyze, control, and optimize system performance in real-time. Doesn’t this sound pretty close to what Digital Twins always promise?\nMost industrial simulation uses are still based on commercial packages.\nHowever, there are a couple of open-source alternatives, which are typically closer to general-purpose language programming. Even though they tend to lack some convenient commercial features, there are upsides such as better scalability and simplified interfacing.\nRelated to Python we became aware of two popular DES packages: Salabim and SimPy. Both are not only free and open-source, but even built on top of the standard library of one of the world’s most popular programming languages - let’s see what we can get out of that!\nCasymda-Package Based on SimPy, we added bits of complementing functionality to gain some of the modeling convenience of commercial “block-based” DES-packages.\nCasymda facilitates the usage of bpmn-process-descriptions to generate corresponding simulation-model python-code. .bpmn-files (basically xml) can easily be created with the Camunda-Modeler.\nThis graphical modeling helps to maintain an overview of the high-level model-structure. Generated Casymda-models also include a generic, token-like animation of simulated processes out-of-the-box, ready to be run in a web-browser. For presentation and debugging, animations can be paused and their speed can be changed dynamically. Solely animation-related events are not scheduled if the simulation is run without visualization. This maximizes the execution speed - which becomes especially important related to RL, when a high number of runs is necessary.\nFurther features of Casymda include simulated movements along shortest paths in a 2D-tilemap-space, and gradual typing for development convenience (checkout pyright if you are using vscode).\nFor more info on Casymda have a look at the repo or the (German) website.\nWrapping a DES-Model in a Gym-Environment To be able to train an RL-agent inside a simulation model, we need to make the model implementing the Gym-interface described above.\nThe following diagram illustrates the coupling concept:\nWhen the step function of the Gym-Environment is called (1), the provided action is propagated to the relevant block of the simulation model (1.1). This is realized with help of an ActionHolder, so that a consuming piece of decision logic can dispatch according to the received information.\nSubsequently, the simulation is executed until a next_action_needed-Event is triggered by the simulation model (1.2). This is indicating the end of the current step and the need for another action of the agent.\nOne Gym-step can thus comprise an arbitrary number of discrete SimPy-steps, each of which can in turn take an arbitrary amount of simulated time.\nRewards are managed with help of a RewardHolder object, which is wired into the relevant blocks of the simulation model during environment initialization. At the end of each step, occured rewards are collected (1.3). Depending on the type of the optimization problem to solve, a post-processing of collected rewards can be applied (e.g. taking into account the amount elapsed time, so that an agent can learn time-efficient behavior).\nTo check whether an episode ended (the done part of the returned information), the current state of the model is checked against configured done_criteria (1.4). These can contain e.g. some goals to be reached or a certain amount of time to be simulated.\nTo provide the agent with an observation, a model-specific ModelStateToObservationConverter is used to collect relevant information from the model. The created observation conforms to the defined observation_space (1.5). This step could include e.g. counting the number of entities in different queues or checking inventory levels and creating a NumPy-array out of this information.\nFinally, collected information is returned to the agent (2), which can learn based on the reward and decide for the next action.\nHaving the basics covered, let’s see how we get this to work.\nCase-Study Back in August of last year at the MIM2019 in Berlin, we had the chance to attend an interesting talk of two Bavarian guys presenting their research on improving the tour-building for in-plant milk-run systems. These internal deliveries are commonly used for assembly line supply, and the tours are typically following a very rigid plan. Given the fact that the actual demand at the line tends to vary, their research revealed quite a lot of potential to decrease delivery lead times and to increase systems’ utilization - just by making the tour-planning more dynamic.\nBased on this setting we constructed an abstracted and simplified version of an assembly line with a corresponding material supply system to provide a playground for reinforcement learning algorithms.\nScenario The image below shows a schematic layout plan of the system:\nUnfinished products enter the system on the upper right (I) and are assembled sequentially at 9 different stations, arranged in U-shape (I-IX). Finished products leave the system after the last assembly step (IX).\nStations require a certain amount of resource of either type A or B to be present in the station’s inventory before an assembly step can start.\nEach station can only hold one product at a time, and finished products can only be forwarded once the following station is empty (thus multiple upstream stations holding already finished products may be blocked by downstream stations which are still processing a product or waiting for material before being able to start processing).\nMaterial is supplied by a tugger, able to carry a limited discrete amount (“boxes”). The tugger can load material at a stock (A and/or B, located at the bottom). 1 discrete unit of material (“box”) can be loaded/unloaded at a time. The goal of the assembly line is achieving the maximal throughput, which also correlates with small lead-times of products.\nAssumptions:\nmaterial can only be loaded at the stocks (A and B), each of which holds an infinite amount of material, so that the tugger never waits for material at a loading site material can only be unloaded at a station actually requiring this type of material (hence a tugger cannot unload a box of A at a station which needs B for assembly) the inventory capacity at the stations (I-IX) is infinite, so that the tugger never waits at an unloading site (otherwise livelocks could occur where a tugger cannot unload material wherever it moves) System parameters Takt-time: processing time per station per product 60s Demand per product of stations type A 1.5 units Demand per product of stations type B 0.5 units Tugger movement speed 10 m/s Tugger capacity 25 units Amount of material (un-)loaded per step 5 units Time needed per (un-)loading step 5s Distances between stocks and stations (higher demands cause more frequent tours):\nRelation Simple Demand-weighted A -\u003e T1 1096.40m 1644.60m B -\u003e T2 926.40m 463.20m A -\u003e T3 736.40m 1104.60m B -\u003e T4 566.40m 283.20m A -\u003e T5 234.10m 351.15m B -\u003e T6 556.40m 278.20m A -\u003e T7 726.40m 1089.60m B -\u003e T8 916.40m 458.20m A -\u003e T9 1086.40m 1629.60m The table below shows a simple throughput estimation by calculating the average cycle time of the tugger and the expected station utilization. The estimation assumes “full truck loads”, always completely loading at one stock (either A or B), and fully unloading at a station (T1 - T9).\nThroughput estimation Max throughput 24h 60/h x 24h = 1440 Demand / product 9.5 units Demand / time 9.5 / 60s = 0.16/s Average weighted distance 811.37m Average driving time 81.137s (Un-)loading time 25 units 25s Average cycle time (81.137s + 25s) x 2 = 212.274s Delivered units / cycle 25 Delivered units / time 0.12/s Average utilization 0.12/s / 0.16/s = 75% Expected throughput per min 75% x 60/min = 45/min Expected throughput per 24h ~1080/24h As we can see, the delivery performance of the tugger represents the limiting factor (bottleneck) of the system, which means that each improvement made here will be directly reflected by a corresponding increase in the overall throughput.\nFor the sake of simplicity, no stochastic model behaviour (such as e.g. randomly distributed loading or movement times) is assumed, hence the simulation model will be deterministic.\nAs stated: the system as a whole is quite abstracted and simplified - but still capturing at least some of the basic complexity inherent to real-world problems. Will our RL-agent be able to…\ngrasp the underlying mechanics? distinguish different product types? discover the spots of demand and supply? deal with the limits of the tugger’s capacity? reach the maximal possible throughput? We’ll find out, but let’s first have a look at what the learning environment will look like.\nSimulation Model The simulation model of the system basically consists of 2 processes, both depicted in the graphic below.\nOn the left side, products pass through the 9 assembly steps (ProductStation, rotated U-shape) before leaving the system, occasionally being blocked by downstream stations or waiting for material at a station.\nOn the right side the tugger passes through an infinite cycle of movement and loading/unloading process steps (after initial creation at location A by a TuggerSource):\nthe next movement target is chosen and the movement is completed (no actual movement if the next target equals the current location) (TuggerMovement). Depending on the current location (being either a stock A/B) or a ProductStation, the next tugger process step is chosen: TuggerStock A loading of one unit of A (if tugger-capacity not reached) TuggerStock B loading of one unit of B (if tugger-capacity not reached) TuggerStation unloading of one unit of A or B if possible (material required by station is loaded) Note that even unsuccessful loading or unloading attempts are implemented to take a small, fixed amount of time, so that every possible Gym-step is guaranteed to take at least some simulated time (and a time-constrained episode is guaranteed to reach its end eventually).\nBelow you can see a process animation, as well as an animation of a tilemap. The agent here follows an explicitly defined simple rule of always delivering a complete load of 25 units to the station with the lowest inventory level. To run the animation just clone the repo, run the command, and visit http://localhost:5000.\nProcess animation:\ndocker-compose up web-animation-lia-process Tilemap animation:\ndocker-compose up web-animation-lia Preparing the Gym-Environment The TuggerEnv implements the Gym-Env interface and wraps the simulation model to be used for RL-agent training.\nGeneric functionalities like the mandatory step and reset functions and related helper methods are inherited and abstract/default parent-methods are overridden in a model-specific way as required (Template-Method Pattern):\ninitialize_action_and_reward_holder specifies which model blocks… need access to gym-actions: TilemapMovement, choosing the next movement target based on the supplied target index number log achieved rewards: ProductSink, simply counting a reward of 1 for each finished product get_reward specifies how the elapsed time is taken into account for reward calculation check_if_model_is_done implements a model-specific check whether a certain amount of time has been simulated. One episode is scheduled to take 24h (86400s). The render method of the Gym-Env is not implemented, since animations at arbitrary moments in time - whenever a Gym-step is finished - do not make much sense for discrete event simulation environments. The animation is controlled separately.\nThe info return value of step is configured to return the number of finished_products which can then be logged.\nObservation- \u0026 Action-Space The model-specific extraction of the observation from the current model state is done by an instance of a TuggerEnvModelStateConverter which implements the ModelStateConverter “interface”.\nSpecifically, the observation consists of the following information which describes the current state of the system (overall 48 values):\nProductStation observations (5 values x 9 stations = 45 values): current inventory-level (normalized 0-1, counted up to a limit of 10 units) busy-state (binary) waiting_for_material-state (binary) empty-state (binary, whether a product is present or not) blocked-by-successor-state (binary) TuggerEntity observations (3 values x 1 tugger = 3 values): loaded amount of A (relative to capacity) loaded amount of B (relative to capacity) current location (index) Note that parts of a station observation can be seen to be redundant (e.g. a station which is neither busy nor waiting nor empty can only be blocked) - behind lies the rationale that an intelligent algorithm will (hopefully) learn an importance of different components of an observation, so that we do not have to worry about more than providing all potentially useful information.\nThe action_space (of type gym.spaces.Discrete) consists of the 11 possible movement targets (9 stations + 2 stocks, encoded by index).\nRewards As stated above, the defined goal of the assembly line is to achieve the best possible throughput of products, which corresponds to producing as many products as possible e.g. during one episode (24h).\nHow do we achieve that? Which kind of incentive is suitable to stimulate such a behavior? The design of appropriate reward functions is known to be a non-trivial matter. In fact, the design of rewards and incentives even for (arguably more intelligent) humans is a major problem in management and education (remember the last time you studied for passing an exam instead of actually learning useful contents).\nFor the environment at hand, we could just think about giving a single reward at the end of each episode, proportionally to the number of achieved products in that fixed amount of time (24h), which would probably properly reflect our aim of maximizing the throughput. However, the resulting reward would be quite sparse and therefore greatly decelerate learning speed (taking the average duration of a random action, each episode would take more than 1000 actions to complete before an agent sees any reward).\nAnother idea would be to reward every successful delivery of material to any station, which would be possible to be completed within 2 steps (movement to the stock \u0026 movement to a suitable station consuming the loaded material). This way we would get less sparse rewards, but also an obvious problem of exploitability, caused by the fact that the delivery of material to one station alone would actually never lead to the completion of any product at all.\nAs a compromise, we simply decided to go for a reward of 1 everytime a product completes its final assembly step, which is possible be completed within 12 steps (minimum, not necessarily an optimal strategy). Even exhibiting a random behavior, this would allow an agent to generate a reward of around 50 during one episode, so that there are sufficient “randomly succesful” samples to learn from.\nOne problem with this reward comes from the fact that the simulated time needed to obtain a reward is not reflected by the reward itself. Since every gym-step can actually eat up a greatly varying amount of simulation time (from 5 seconds to \u003e100), there is a huge implicit impact on the throughput, which the agent is unaware of. To solve this problem we introduced “costs of time”, which means we simply give a small negative reward every step, proportional to the amount of simulated time that passed. This finally leaves us with the subsequent question of how big these “costs” should be. If set too high, they would just overrule any of the few actual rewards at the beginning of the training. If put too low, there would not be sufficient stimulus to exhibit time-efficient behavior at all. Again, as a simple compromise, we implemented the costs to grow proportionally with the highest reward seen so far at the end of an episode, which guarantees a certain balance, and rewards increasing time-efficiency.\nThe above described reward that we designed is definitely not “perfect” and also feels a bit like putting too much effort into “reward engineering” - nevertheless its a first solution our agents can hopefully work with…\nRL-Agent Training \u0026 Evaluation The environment presented above is characterized by a Discrete action space and a continuous (Box) observations space. The stable-baselines documentation lists available RL algorithms and their compatibility.\nDue to the type of action space, some algorithms are not feasible (i.e. DDPG, SAC, and TD3).\nTo train a stable-baselines RL algorithm, the TuggerEnv is vectorized, using a DummyVecEnv and a standard MlpPolicy. To leverage multiple CPUs for training, it can be desirable to use a SubprocVecEnv (but for simpler logging \u0026 analysis we did not go with that one here, instead we did multiple independent training runs in parallel).\nTrain an ACER-agent (by default for 10,000 steps only, which should take \u003c1min):\ndocker-compose up acer-training Plot performance (might require additional setup for connecting the display):\ndocker-compose up acer-plot-training Tilemap-animation of the trained agent (http://localhost:5000):\ndocker-compose up acer-web-animation-tilemap Below we can see an ACER-agent trained for 1m steps:\nAs we can see, the agent manages to fully load the 25 units onto the tugger most of the time, seems to target correct (A/B) stations for material unloading, and the choice of stations with a currently low inventory level seems reasonable too!\nBut how does the overall performance look like?\nPerformance Comparison For comparison we trained four algorithms (ACER, ACKTR, DQN, and PPO2) with standard settings for both 1 and 3 mio. (Gym-)steps. Training took up to 2.5 hours (DQN, 3mio. steps) on a 2.9GHz Intel i9, using a single-process DummyVecEnv as explained above.\nThe following graph shows the number of produced products per episode (24h) over the course of the training run for each algorithm, as well as the performance of the deterministic lowest-inventory heuristics (yellow line; always delivering a complete load of 25 units to the station with the currently lowest inventory), and the average performance of fully random actions (turquoise line, measured over 100 episodes).\nAs we can see, all of the algorithms manage to increase the number of produced products per episode significantly above the level reached by random actions (turquoise line at the bottom), indicating successful learning progress. Furthermore, none of the trained algorithms reaches the performance of the lowest-inventory-heuristics (yellow line at the top). The lowest-inventory-heuristics performance reaches the estimated maximum possible throughput of the system (estimated to appr. 1080/episode). This strategy can therefore be considered to be close to a global optimum. During training, a complete breakdown in performance can occur. Most prominently: ACER_3mio. (blue line, episode 260, no recovery at all). Other algorithms show drops in performance as well but seem to recover better (e.g. ACKTR - green, PPO2 - pink). The best-performing RL algorithm (ACER trained for 1mio. steps, orange line) reached a maximum throughput of 856 products / episode (78% of the near-optimal heuristics performance). The number of episodes varies due to the variable number of Gym-steps per episode (24h of simulated time), depending on the simulated time each Gym-step needs. The small number of episodes of the ACER_3mio. training is explained by the up to 17277 Gym-steps per episode, occurring from episode 260 on. Each step of such an episode takes only 5 seconds (the minimum possible time of all available Gym-steps, “achieved” by a repeated visit of the same location). This behavior might be caused by the defined negative reward per step, proportional to the amount of simulated time the step needed. Appearently, the agent does not remember how to generate a positive reward and only tries to maximize the short-term reward by minimizing the step-time. Obviously this behavior does not lead to any successful delivery, let alone completion of any product.\nIt is worth to be mentioned that all training runs were done with default algorithm settings, and that the evaluation of different hyperparameters is strongly recommended for performance optimization. Thus, it might not be improbable for an RL agent to close the performance gap towards the theoretically reachable optimum.\nSumming Up Short version: Our best RL agent reached about 78% of the best possible performance inside our production-logistics environment.\nOk, now is this good or bad?\nWell, one could be disappointed by the fact that our agent was not able to reach the performance of a hand-coded heuristics approach.\nBut did we believe when we started that we could get a generic piece of code to cope with the non-trivial relations of our specific and fairly complex environment? Certainly not!\nAnd this was just a first shot - we did not yet start with hyperparameter tuning or the evaluation of alternative rewards.\nWhat do your experiences with reinforcement learning look like?\nWhich logistics problems did you solve with RL?\nDid you spot a bug somewhere in the code or do you want to suggest an improvement?\nOr do you have questions concerning the presented implementation/toolstack?\nJust feel free to drop us a note, thanks for reading!\nWladimir Hofmann - Clemens L. Schwarz - Fredrik Branding\n","wordCount":"3930","inLanguage":"en","datePublished":"2020-03-21T00:00:00Z","dateModified":"2020-03-21T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://fladdimir.github.io/post/tugger-routing/"},"publisher":{"@type":"Organization","name":"wh","logo":{"@type":"ImageObject","url":"https://fladdimir.github.io/icon.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://fladdimir.github.io/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://fladdimir.github.io/tags/ title=/tags><span>/tags</span></a></li><li><a href=https://fladdimir.github.io/search title="/search (Alt + /)" accesskey=/><span>/search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://fladdimir.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://fladdimir.github.io/post/>Posts</a></div><h1 class=post-title>Digital Twin for Assembly Line Material Provisioning Planning</h1><div class=post-meta><span title='2020-03-21 00:00:00 +0000 UTC'>March 21, 2020</span>&nbsp;·&nbsp;19 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#introduction--basics>Introduction & Basics</a><ul><li><a href=#reinforcement-learning>Reinforcement Learning</a></li><li><a href=#digital-twins-and-discrete-event-simulation>Digital Twins and Discrete Event Simulation</a></li><li><a href=#wrapping-a-des-model-in-a-gym-environment>Wrapping a DES-Model in a Gym-Environment</a></li></ul></li><li><a href=#case-study>Case-Study</a><ul><li><a href=#scenario>Scenario</a></li><li><a href=#simulation-model>Simulation Model</a></li><li><a href=#preparing-the-gym-environment>Preparing the Gym-Environment</a></li><li><a href=#rl-agent-training--evaluation>RL-Agent Training & Evaluation</a></li><li><a href=#performance-comparison>Performance Comparison</a></li></ul></li><li><a href=#summing-up>Summing Up</a></li></ul></nav></div></details></div><div class=post-content><p>Reinforcement learning represents an emerging technique from machine learning.
It can autonomously derive complex action sequences in dynamic environments and is successfully applied in various fields, e.g. from <a href=https://openai.com/blog/>robotics and gaming</a>.
Instead of explicitly defining a specific solution strategy for a problem, we can just provide an <em>environment</em>. A self-learning <em>agent</em> will then autonomously discover successful strategies just by interaction.</p><p>Needless to say, there is nothing new under the moon and <a href=https://www.researchgate.net/publication/326039529_Optimization_of_global_production_scheduling_with_deep_reinforcement_learning>previous studies</a> show the general feasibility of using RL for solving production-logistics problems.</p><p>So why do we think that there is the need for yet another article about this very topic?</p><p>First, there is a lot of active development in <em>RL</em>, as well as in the application of <em>Digital Twins</em> in production/logistics.
We believe that there lies even more potential in integrating these concepts.
Furthermore, we found the often derogatory-treated <em>&ldquo;low-level implementation work&rdquo;</em> to be an actual obstacle for making progress in this challenging and highly inter-disciplinary area of applied research.
This contribution strives to show a working example based on a tool-stack which seamlessly integrates two of the most popular open-source software packages from their respective areas:
<a href=https://stable-baselines.readthedocs.io/en/master/>stable-baselines</a> for <em>RL</em> and <a href=https://simpy.readthedocs.io/en/latest/>SimPy</a> for implementing <em>Digital Twins</em>.</p><blockquote><p>Get the repo: <a href=https://github.com/fladdimir/tugger-routing>https://github.com/fladdimir/tugger-routing</a></p><p>Slides: <a href=https://fladdimir.github.io/presentations/des-python-rl.html>https://fladdimir.github.io/presentations/des-python-rl.html</a></p><p>Paper <a href=https://www.researchgate.net/publication/355717138_Towards_the_Productive_Application_of_Reinforcement_Learning_in_Logistics_A_Case_Study_on_Assembly_Line_Material_Provision_Planning>on researchgate</a></p></blockquote><h2 id=introduction--basics>Introduction & Basics<a hidden class=anchor aria-hidden=true href=#introduction--basics>#</a></h2><h3 id=reinforcement-learning>Reinforcement Learning<a hidden class=anchor aria-hidden=true href=#reinforcement-learning>#</a></h3><p>If you still ask yourself what RL is capable of, we definitely recommend to have a look at <a href=https://openai.com/blog/emergent-tool-use/>what the guys from openai are doing</a>.</p><p>Admittedly, thats probably a quite sophisticated and highly engineered example, but it breaks down to a simple interaction between an <em>agent</em> and an <em>environment</em>.
Technically, this interaction is defined by an interface (or abstract base-class as Python likes to put it), which is part of the <a href=https://gym.openai.com/docs/>gym-package</a>.</p><p>The graphic below illustrates the exchange of information between agent and environment.
First, the agent calls the environment&rsquo;s <em>step</em> method, providing the action to be executed.
The environment then processes the action and returns:</p><ul><li>the new state of the system (<em>observation</em>),</li><li>the <em>reward</em> which occured during the step (might be zero),</li><li>a <em>done</em> value potentially indicating the end of an episode (and the need for a subsequent <em>reset</em>)</li><li>and an <em>info</em>-object (might contain additional information e.g. for logging purposes).</li></ul><p><img loading=lazy src=diagrams/gym_rl_environment_interface.jpg alt=Gym-RL-Environment-Interface></p><p>The interface also prescribes more, such as the formats of <em>action-space</em> and <em>observation_space</em>, as well as <em>render</em> and <em>reset</em> behavior.<br>The various RL algorithms provided by the <a href=https://pypi.org/project/stable-baselines/>stable-baselines</a>-package are ready to work with environments implementing this gym-interface.
All that is left to do is creating a compliant environment - and in the next section we will show how this can be achieved in the domain of logistics.</p><h3 id=digital-twins-and-discrete-event-simulation>Digital Twins and Discrete Event Simulation<a hidden class=anchor aria-hidden=true href=#digital-twins-and-discrete-event-simulation>#</a></h3><p>Frankly, <em>Digital Twin</em> is probably the most overused buzzword of all the &ldquo;Lostistics 4.0 / Industry 4.0&rdquo; stuff that is out there.
Even though we could not resist to put it into the title, from now on we&rsquo;ll prove that we can do better and use the more specific term &ldquo;Discrete Event Simulation&rdquo; (DES).</p><p>Why DES? Discrete Event Simulation is one of the widespread tools for analysis and design of logistics systems.
Today&rsquo;s applications go beyond the traditional usage for systems planning.
They include more operational use-cases such as virtual commissioning or short-term forecasts.
Simulation models are getting integrated tightly into other IT-systems.
This allows to increase process transparency and to improve our means to analyze, control, and optimize system performance in real-time.
Doesn&rsquo;t this sound pretty close to what <em>Digital Twins</em> always promise?</p><p>Most industrial simulation uses are still based on commercial packages.<br>However, there are a couple of open-source alternatives, which are typically closer to general-purpose language programming. Even though they tend to lack some convenient commercial features, there are upsides such as better scalability and simplified interfacing.</p><p>Related to Python we became aware of two popular DES packages: <a href=https://www.salabim.org/>Salabim</a> and <a href=https://simpy.readthedocs.io/en/latest/>SimPy</a>.
Both are not only free and open-source, but even built on top of the standard library of one of the world&rsquo;s most popular programming languages - let&rsquo;s see what we can get out of that!</p><h4 id=casymda-package>Casymda-Package<a hidden class=anchor aria-hidden=true href=#casymda-package>#</a></h4><p>Based on <em>SimPy</em>, we added bits of complementing functionality to gain some of the modeling convenience of commercial &ldquo;block-based&rdquo; DES-packages.<br><a href=https://pypi.org/project/casymda/><em>Casymda</em></a> facilitates the usage of bpmn-process-descriptions to generate corresponding simulation-model python-code.
.bpmn-files (basically xml) can easily be created with the <a href=http://www.bpmn.io>Camunda-Modeler</a>.<br>This graphical modeling helps to maintain an overview of the high-level model-structure.
Generated Casymda-models also include a generic, token-like animation of simulated processes out-of-the-box, ready to be run in a web-browser.
For presentation and debugging, animations can be paused and their speed can be changed dynamically.
Solely animation-related events are not scheduled if the simulation is run without visualization.
This maximizes the execution speed - which becomes especially important related to RL, when a high number of runs is necessary.<br>Further features of Casymda include simulated movements along shortest paths in a 2D-tilemap-space, and gradual typing for development convenience (checkout <a href=https://github.com/microsoft/pyright>pyright</a> if you are using <a href=https://code.visualstudio.com/docs/python/editing>vscode</a>).</p><p>For more info on <em>Casymda</em> have a look at <a href=https://github.com/fladdimir/casymda>the repo</a> or <a href=https://casymda.github.io/page/Webpage/Startpage.html>the (German) website</a>.</p><h3 id=wrapping-a-des-model-in-a-gym-environment>Wrapping a DES-Model in a Gym-Environment<a hidden class=anchor aria-hidden=true href=#wrapping-a-des-model-in-a-gym-environment>#</a></h3><p>To be able to train an RL-agent inside a simulation model, we need to make the model implementing the Gym-interface described above.</p><p>The following diagram illustrates the coupling concept:</p><p><img loading=lazy src=diagrams/sim_env_wrapper.jpg alt="Gym-environment wrapper for a simulation model"></p><p>When the <code>step</code> function of the Gym-Environment is called (<code>1</code>), the provided action is propagated to the relevant block of the simulation model (<code>1.1</code>).
This is realized with help of an <code>ActionHolder</code>, so that a consuming piece of decision logic can dispatch according to the received information.</p><p>Subsequently, the simulation is executed until a <code>next_action_needed</code>-Event is triggered by the simulation model (<code>1.2</code>).
This is indicating the end of the current step and the need for another action of the agent.</p><blockquote><p>One <em>Gym-step</em> can thus comprise an arbitrary number of discrete <em>SimPy-steps</em>, each of which can in turn take an arbitrary amount of simulated time.</p></blockquote><p>Rewards are managed with help of a <code>RewardHolder</code> object, which is wired into the relevant blocks of the simulation model during environment initialization.
At the end of each step, occured rewards are collected (<code>1.3</code>). Depending on the type of the optimization problem to solve, a post-processing of collected rewards can be applied (e.g. taking into account the amount elapsed time, so that an agent can learn time-efficient behavior).</p><p>To check whether an episode ended (the <em>done</em> part of the returned information), the current state of the model is checked against configured <code>done_criteria</code> (<code>1.4</code>).
These can contain e.g. some goals to be reached or a certain amount of time to be simulated.</p><p>To provide the agent with an observation, a model-specific <code>ModelStateToObservationConverter</code> is used to collect relevant information from the model.
The created observation conforms to the defined <code>observation_space</code> (<code>1.5</code>).
This step could include e.g. counting the number of entities in different queues or checking inventory levels and creating a NumPy-array out of this information.</p><p>Finally, collected information is returned to the agent (<code>2</code>), which can learn based on the reward and decide for the next action.</p><p>Having the basics covered, let&rsquo;s see how we get this to work.</p><h2 id=case-study>Case-Study<a hidden class=anchor aria-hidden=true href=#case-study>#</a></h2><p>Back in August of last year at the MIM2019 in Berlin, we had the chance to attend an interesting talk of two Bavarian guys presenting their research on <a href=https://www.sciencedirect.com/science/article/pii/S2405896319315010>improving the tour-building for in-plant milk-run systems</a>.
These internal deliveries are commonly used for assembly line supply, and the tours are typically following a very rigid plan.
Given the fact that the actual demand at the line tends to vary, their research revealed quite a lot of potential to decrease delivery lead times and to increase systems&rsquo; utilization - just by making the tour-planning more dynamic.</p><p>Based on this setting we constructed an abstracted and simplified version of an assembly line with a corresponding material supply system to provide a playground for reinforcement learning algorithms.</p><h3 id=scenario>Scenario<a hidden class=anchor aria-hidden=true href=#scenario>#</a></h3><p>The image below shows a schematic layout plan of the system:</p><p><img loading=lazy src=images/layout_50perc.png alt=case-study-layout></p><p>Unfinished products enter the system on the upper right (<code>I</code>) and are assembled sequentially at 9 different stations, arranged in U-shape (<code>I-IX</code>).
Finished products leave the system after the last assembly step (<code>IX</code>).<br>Stations require a certain amount of resource of either type <code>A</code> or <code>B</code> to be present in the station&rsquo;s inventory before an assembly step can start.<br>Each station can only hold one product at a time, and finished products can only be forwarded once the following station is empty (thus multiple upstream stations holding already finished products may be blocked by downstream stations which are still processing a product or waiting for material before being able to start processing).<br>Material is supplied by a tugger, able to carry a limited discrete amount (&ldquo;boxes&rdquo;). The tugger can load material at a stock (<code>A</code> and/or <code>B</code>, located at the bottom).
1 discrete unit of material (&ldquo;box&rdquo;) can be loaded/unloaded at a time.
The goal of the assembly line is achieving the maximal throughput, which also correlates with small lead-times of products.</p><p>Assumptions:</p><ul><li>material can only be loaded at the stocks (<code>A</code> and <code>B</code>), each of which holds an infinite amount of material, so that the tugger never waits for material at a loading site</li><li>material can only be unloaded at a station actually requiring this type of material (hence a tugger cannot unload a box of <code>A</code> at a station which needs <code>B</code> for assembly)</li><li>the inventory capacity at the stations (<code>I-IX</code>) is infinite, so that the tugger never waits at an unloading site (otherwise livelocks could occur where a tugger cannot unload material wherever it moves)</li></ul><table><thead><tr><th style=text-align:left>System parameters</th><th style=text-align:center></th></tr></thead><tbody><tr><td style=text-align:left>Takt-time: processing time per station per product</td><td style=text-align:center>60s</td></tr><tr><td style=text-align:left>Demand per product of stations type <code>A</code></td><td style=text-align:center>1.5 units</td></tr><tr><td style=text-align:left>Demand per product of stations type <code>B</code></td><td style=text-align:center>0.5 units</td></tr><tr><td style=text-align:left>Tugger movement speed</td><td style=text-align:center>10 m/s</td></tr><tr><td style=text-align:left>Tugger capacity</td><td style=text-align:center>25 units</td></tr><tr><td style=text-align:left>Amount of material (un-)loaded per step</td><td style=text-align:center>5 units</td></tr><tr><td style=text-align:left>Time needed per (un-)loading step</td><td style=text-align:center>5s</td></tr></tbody></table><p>Distances between stocks and stations (higher demands cause more frequent tours):</p><table><thead><tr><th style=text-align:center>Relation</th><th style=text-align:center>Simple</th><th style=text-align:center>Demand-weighted</th></tr></thead><tbody><tr><td style=text-align:center>A -> T1</td><td style=text-align:center>1096.40m</td><td style=text-align:center>1644.60m</td></tr><tr><td style=text-align:center>B -> T2</td><td style=text-align:center>926.40m</td><td style=text-align:center>463.20m</td></tr><tr><td style=text-align:center>A -> T3</td><td style=text-align:center>736.40m</td><td style=text-align:center>1104.60m</td></tr><tr><td style=text-align:center>B -> T4</td><td style=text-align:center>566.40m</td><td style=text-align:center>283.20m</td></tr><tr><td style=text-align:center>A -> T5</td><td style=text-align:center>234.10m</td><td style=text-align:center>351.15m</td></tr><tr><td style=text-align:center>B -> T6</td><td style=text-align:center>556.40m</td><td style=text-align:center>278.20m</td></tr><tr><td style=text-align:center>A -> T7</td><td style=text-align:center>726.40m</td><td style=text-align:center>1089.60m</td></tr><tr><td style=text-align:center>B -> T8</td><td style=text-align:center>916.40m</td><td style=text-align:center>458.20m</td></tr><tr><td style=text-align:center>A -> T9</td><td style=text-align:center>1086.40m</td><td style=text-align:center>1629.60m</td></tr></tbody></table><p>The table below shows a simple throughput estimation by calculating the average cycle time of the tugger and the expected station utilization.
The estimation assumes &ldquo;full truck loads&rdquo;, always completely loading at one stock (either <code>A</code> or <code>B</code>), and fully unloading at a station (<code>T1</code> - <code>T9</code>).</p><table><thead><tr><th style=text-align:left>Throughput estimation</th><th style=text-align:center></th></tr></thead><tbody><tr><td style=text-align:left>Max throughput 24h</td><td style=text-align:center>60/h x 24h = 1440</td></tr><tr><td style=text-align:left>Demand / product</td><td style=text-align:center>9.5 units</td></tr><tr><td style=text-align:left>Demand / time</td><td style=text-align:center>9.5 / 60s = 0.16/s</td></tr><tr><td style=text-align:left>Average weighted distance</td><td style=text-align:center>811.37m</td></tr><tr><td style=text-align:left>Average driving time</td><td style=text-align:center>81.137s</td></tr><tr><td style=text-align:left>(Un-)loading time 25 units</td><td style=text-align:center>25s</td></tr><tr><td style=text-align:left>Average cycle time</td><td style=text-align:center>(81.137s + 25s) x 2 = 212.274s</td></tr><tr><td style=text-align:left>Delivered units / cycle</td><td style=text-align:center>25</td></tr><tr><td style=text-align:left>Delivered units / time</td><td style=text-align:center>0.12/s</td></tr><tr><td style=text-align:left>Average utilization</td><td style=text-align:center>0.12/s / 0.16/s = 75%</td></tr><tr><td style=text-align:left>Expected throughput per min</td><td style=text-align:center>75% x 60/min = 45/min</td></tr><tr><td style=text-align:left>Expected throughput per 24h</td><td style=text-align:center>~1080/24h</td></tr></tbody></table><p>As we can see, the delivery performance of the tugger represents the limiting factor (bottleneck) of the system, which means that each improvement made here will be directly reflected by a corresponding increase in the overall throughput.<br>For the sake of simplicity, no stochastic model behaviour (such as e.g. randomly distributed loading or movement times) is assumed, hence the simulation model will be deterministic.<br>As stated: the system as a whole is quite abstracted and simplified - but still capturing at least some of the basic complexity inherent to real-world problems.
Will our RL-agent be able to&mldr;</p><ul><li>grasp the underlying mechanics?</li><li>distinguish different product types?</li><li>discover the spots of demand and supply?</li><li>deal with the limits of the tugger&rsquo;s capacity?</li><li>reach the maximal possible throughput?</li></ul><p>We&rsquo;ll find out, but let&rsquo;s first have a look at what the learning environment will look like.</p><h3 id=simulation-model>Simulation Model<a hidden class=anchor aria-hidden=true href=#simulation-model>#</a></h3><p>The <a href=https://github.com/fladdimir/tugger-routing/tree/github/tugger_src/gym_env/des_model>simulation model</a> of the system basically consists of 2 processes, both depicted in the graphic below.</p><p><img loading=lazy src=images/model.jpeg alt=case-study-processes></p><p>On the left side, products pass through the 9 assembly steps (<code>ProductStation</code>, rotated U-shape) before leaving the system, occasionally being blocked by downstream stations or waiting for material at a station.</p><p>On the right side the tugger passes through an infinite cycle of movement and loading/unloading process steps (after initial creation at location <code>A</code> by a <code>TuggerSource</code>):</p><ol><li>the next movement target is chosen and the movement is completed (no actual movement if the next target equals the current location) (<code>TuggerMovement</code>).</li><li>Depending on the current location (being either a stock <code>A</code>/<code>B</code>) or a <code>ProductStation</code>, the next tugger process step is chosen:<ol><li><code>TuggerStock A</code> loading of one unit of <code>A</code> (if tugger-capacity not reached)</li><li><code>TuggerStock B</code> loading of one unit of <code>B</code> (if tugger-capacity not reached)</li><li><code>TuggerStation</code> unloading of one unit of <code>A</code> or <code>B</code> if possible (material required by station is loaded)</li></ol></li></ol><p>Note that even unsuccessful loading or unloading attempts are implemented to take a small, fixed amount of time, so that every possible Gym-step is guaranteed to take at least some simulated time (and a time-constrained episode is guaranteed to reach its end eventually).</p><p>Below you can see a process animation, as well as an animation of a tilemap.
The agent here follows an explicitly defined simple rule of always delivering a complete load of 25 units to the station with the lowest inventory level.
To run the animation just clone the <a href=https://github.com/fladdimir/tugger-routing>repo</a>, run the command, and visit <a href=http://localhost:5000>http://localhost:5000</a>.</p><p>Process animation:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>docker-compose up web-animation-lia-process
</span></span></code></pre></div><p><img loading=lazy src=gifs/lia_process.gif alt=lia-process></p><p>Tilemap animation:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>docker-compose up web-animation-lia
</span></span></code></pre></div><p><img loading=lazy src=gifs/lia_tilemap.gif alt=lia-tilemap></p><h3 id=preparing-the-gym-environment>Preparing the Gym-Environment<a hidden class=anchor aria-hidden=true href=#preparing-the-gym-environment>#</a></h3><p>The <code>TuggerEnv</code> implements the Gym-Env interface and wraps the simulation model to be used for RL-agent training.</p><p>Generic functionalities like the mandatory <code>step</code> and <code>reset</code> functions and related helper methods are inherited and abstract/default parent-methods are overridden in a model-specific way as required (<em>Template-Method Pattern</em>):</p><ul><li><code>initialize_action_and_reward_holder</code> specifies which model blocks&mldr;<ul><li>need access to gym-actions: <code>TilemapMovement</code>, choosing the next movement target based on the supplied target index number</li><li>log achieved rewards: <code>ProductSink</code>, simply counting a reward of 1 for each finished product</li></ul></li><li><code>get_reward</code> specifies how the elapsed time is taken into account for reward calculation</li><li><code>check_if_model_is_done</code> implements a model-specific check whether a certain amount of time has been simulated. One episode is scheduled to take 24h (86400s).</li></ul><p>The <code>render</code> method of the Gym-Env is not implemented, since animations at arbitrary moments in time - whenever a Gym-step is finished - do not make much sense for discrete event simulation environments. The animation is controlled separately.</p><p>The <code>info</code> return value of <code>step</code> is configured to return the number of <code>finished_products</code> which can then be logged.</p><h4 id=observation---action-space>Observation- & Action-Space<a hidden class=anchor aria-hidden=true href=#observation---action-space>#</a></h4><p>The model-specific extraction of the observation from the current model state is done by an instance of a <code>TuggerEnvModelStateConverter</code> which implements the <code>ModelStateConverter</code> &ldquo;interface&rdquo;.</p><p>Specifically, the observation consists of the following information which describes the current state of the system (overall 48 values):</p><ul><li><code>ProductStation</code> observations (5 values x 9 stations = 45 values):<ul><li>current inventory-level (normalized 0-1, counted up to a limit of 10 units)</li><li><em>busy</em>-state (binary)</li><li><em>waiting_for_material</em>-state (binary)</li><li><em>empty</em>-state (binary, whether a product is present or not)</li><li><em>blocked-by-successor</em>-state (binary)</li></ul></li><li><code>TuggerEntity</code> observations (3 values x 1 tugger = 3 values):<ul><li>loaded amount of <code>A</code> (relative to capacity)</li><li>loaded amount of <code>B</code> (relative to capacity)</li><li>current location (index)</li></ul></li></ul><p>Note that parts of a station observation can be seen to be redundant (e.g. a station which is neither <em>busy</em> nor <em>waiting</em> nor <em>empty</em> can only be <em>blocked</em>) - behind lies the rationale that an intelligent algorithm will (hopefully) learn an importance of different components of an observation, so that we do not have to worry about more than providing all potentially useful information.</p><p>The <code>action_space</code> (of type <code>gym.spaces.Discrete</code>) consists of the 11 possible movement targets (9 stations + 2 stocks, encoded by index).</p><h4 id=rewards>Rewards<a hidden class=anchor aria-hidden=true href=#rewards>#</a></h4><p>As stated above, the defined goal of the assembly line is to achieve the best possible throughput of products, which corresponds to producing as many products as possible e.g. during one episode (24h).</p><p>How do we achieve that? Which kind of incentive is suitable to stimulate such a behavior?
The design of appropriate reward functions is known to be a non-trivial matter.
In fact, the design of rewards and incentives even for (arguably more intelligent) humans is a major problem in management and education (remember the last time you studied for passing an exam instead of actually learning useful contents).</p><p>For the environment at hand, we could just think about giving a single reward at the end of each episode, proportionally to the number of achieved products in that fixed amount of time (24h), which would probably properly reflect our aim of maximizing the throughput.
However, the resulting reward would be quite <em>sparse</em> and therefore greatly decelerate learning speed (taking the average duration of a random action, each episode would take more than 1000 actions to complete before an agent sees any reward).</p><p>Another idea would be to reward every successful delivery of material to any station, which would be possible to be completed within 2 steps (movement to the stock & movement to a suitable station consuming the loaded material).
This way we would get less sparse rewards, but also an obvious problem of exploitability, caused by the fact that the delivery of material to one station alone would actually never lead to the completion of any product at all.</p><p>As a compromise, we simply decided to go for a reward of 1 everytime a product completes its final assembly step, which is possible be completed within 12 steps (minimum, not necessarily an optimal strategy).
Even exhibiting a random behavior, this would allow an agent to generate a reward of around 50 during one episode, so that there are sufficient &ldquo;randomly succesful&rdquo; samples to learn from.</p><p>One problem with this reward comes from the fact that the simulated time needed to obtain a reward is not reflected by the reward itself.
Since every gym-step can actually eat up a greatly varying amount of simulation time (from 5 seconds to >100), there is a huge implicit impact on the throughput, which the agent is unaware of.
To solve this problem we introduced &ldquo;costs of time&rdquo;, which means we simply give a small negative reward every step, proportional to the amount of simulated time that passed.
This finally leaves us with the subsequent question of how big these &ldquo;costs&rdquo; should be.
If set too high, they would just overrule any of the few actual rewards at the beginning of the training.
If put too low, there would not be sufficient stimulus to exhibit time-efficient behavior at all.
Again, as a simple compromise, we implemented the costs to grow proportionally with the highest reward seen so far at the end of an episode, which guarantees a certain balance, and rewards increasing time-efficiency.</p><p>The above described reward that we designed is definitely not &ldquo;perfect&rdquo; and also feels a bit like putting too much effort into &ldquo;reward engineering&rdquo; - nevertheless its a first solution our agents can hopefully work with&mldr;</p><h3 id=rl-agent-training--evaluation>RL-Agent Training & Evaluation<a hidden class=anchor aria-hidden=true href=#rl-agent-training--evaluation>#</a></h3><p>The environment presented above is characterized by a <em>Discrete</em> action space and a continuous (<em>Box</em>) observations space.
<a href=https://stable-baselines.readthedocs.io/en/master/guide/algos.html>The stable-baselines documentation</a> lists available RL algorithms and their compatibility.<br>Due to the type of action space, some algorithms are not feasible (i.e. DDPG, SAC, and TD3).</p><p>To train a stable-baselines RL algorithm, the <code>TuggerEnv</code> is vectorized, using a <code>DummyVecEnv</code> and a standard <code>MlpPolicy</code>.
To leverage multiple CPUs for training, it can be desirable to use a <code>SubprocVecEnv</code> (but for simpler logging & analysis we did not go with that one here, instead we did multiple independent training runs in parallel).</p><p>Train an ACER-agent (by default for 10,000 steps only, which should take &lt;1min):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>docker-compose up acer-training
</span></span></code></pre></div><p>Plot performance (might require additional setup for connecting the display):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>docker-compose up acer-plot-training
</span></span></code></pre></div><p>Tilemap-animation of the trained agent (<a href=http://localhost:5000>http://localhost:5000</a>):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>docker-compose up acer-web-animation-tilemap
</span></span></code></pre></div><p>Below we can see an ACER-agent trained for 1m steps:</p><p><img loading=lazy src=gifs/tilemap_acer_1e6.gif alt=tilemap_acer_1e6></p><blockquote><p>As we can see, the agent manages to fully load the 25 units onto the tugger most of the time, seems to target correct (A/B) stations for material unloading, and the choice of stations with a currently low inventory level seems reasonable too!</p></blockquote><p>But how does the overall performance look like?</p><h3 id=performance-comparison>Performance Comparison<a hidden class=anchor aria-hidden=true href=#performance-comparison>#</a></h3><p>For comparison we trained four algorithms (ACER, ACKTR, DQN, and PPO2) with standard settings for both 1 and 3 mio. (Gym-)steps.
Training took up to 2.5 hours (DQN, 3mio. steps) on a 2.9GHz Intel i9, using a single-process DummyVecEnv as explained above.<br>The following graph shows the number of produced products per episode (24h) over the course of the training run for each algorithm, as well as the performance of the deterministic lowest-inventory heuristics (yellow line; always delivering a complete load of 25 units to the station with the currently lowest inventory), and the average performance of fully random actions (turquoise line, measured over 100 episodes).</p><p><img loading=lazy src=graphs/training_progress.png alt=performance-comparision-graph></p><ul><li>As we can see, all of the algorithms manage to increase the number of produced products per episode significantly above the level reached by random actions (turquoise line at the bottom), indicating successful learning progress.</li><li>Furthermore, none of the trained algorithms reaches the performance of the lowest-inventory-heuristics (yellow line at the top).</li><li>The lowest-inventory-heuristics performance reaches the estimated maximum possible throughput of the system (estimated to appr. 1080/episode). This strategy can therefore be considered to be close to a global optimum.</li><li>During training, a complete breakdown in performance can occur. Most prominently: ACER_3mio. (blue line, episode 260, no recovery at all). Other algorithms show drops in performance as well but seem to recover better (e.g. ACKTR - green, PPO2 - pink).</li><li>The best-performing RL algorithm (ACER trained for 1mio. steps, orange line) reached a maximum throughput of 856 products / episode (78% of the near-optimal heuristics performance).</li></ul><p>The number of episodes varies due to the variable number of Gym-steps per episode (24h of simulated time), depending on the simulated time each Gym-step needs.
The small number of episodes of the ACER_3mio. training is explained by the up to 17277 Gym-steps per episode, occurring from episode 260 on.
Each step of such an episode takes only 5 seconds (the minimum possible time of all available Gym-steps, &ldquo;achieved&rdquo; by a repeated visit of the same location).
This behavior might be caused by the defined negative reward per step, proportional to the amount of simulated time the step needed.
Appearently, the agent does not remember how to generate a positive reward and only tries to maximize the short-term reward by minimizing the step-time.
Obviously this behavior does not lead to any successful delivery, let alone completion of any product.</p><p>It is worth to be mentioned that all training runs were done with default algorithm settings, and that the evaluation of different hyperparameters is strongly <a href=https://stable-baselines.readthedocs.io/en/master/guide/rl_tips.html>recommended</a> for performance optimization.
Thus, it might not be improbable for an RL agent to close the performance gap towards the theoretically reachable optimum.</p><h2 id=summing-up>Summing Up<a hidden class=anchor aria-hidden=true href=#summing-up>#</a></h2><p>Short version: Our best RL agent reached about 78% of the best possible performance inside our production-logistics environment.</p><p>Ok, now is this good or bad?</p><p>Well, one could be disappointed by the fact that our agent was not able to reach the performance of a hand-coded heuristics approach.<br>But did we believe when we started that we could get a generic piece of code to cope with the non-trivial relations of our specific and fairly complex environment? Certainly not!<br>And this was just a first shot - we did not yet start with hyperparameter tuning or the evaluation of alternative rewards.</p><p>What do your experiences with reinforcement learning look like?<br>Which logistics problems did you solve with RL?<br>Did you spot a bug somewhere in the code or do you want to suggest an improvement?<br>Or do you have questions concerning the presented implementation/toolstack?</p><p>Just feel free to drop us a note, thanks for reading!</p><p><a href=https://www.linkedin.com/in/wladimir-hofmann-9065a714b/>Wladimir Hofmann</a> - <a href=https://www.linkedin.com/in/clemens-lennart-schwarz-609815186/>Clemens L. Schwarz</a> - <a href=https://www.linkedin.com/in/fredrik-branding-064014163/>Fredrik Branding</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://fladdimir.github.io/tags/python/>Python</a></li><li><a href=https://fladdimir.github.io/tags/simpy/>SimPy</a></li><li><a href=https://fladdimir.github.io/tags/reinforcement-learning/>Reinforcement Learning</a></li><li><a href=https://fladdimir.github.io/tags/openai-gym/>OpenAI Gym</a></li><li><a href=https://fladdimir.github.io/tags/slides/>Slides</a></li></ul><nav class=paginav><a class=prev href=https://fladdimir.github.io/post/csa-streetmap/><span class=title>« Prev</span><br><span>Urban Logistics Network Simulation in Python</span></a>
<a class=next href=https://fladdimir.github.io/post/casymda/><span class=title>Next »</span><br><span>Introducing Casymda</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://fladdimir.github.io/>wh</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>