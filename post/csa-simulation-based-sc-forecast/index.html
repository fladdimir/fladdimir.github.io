<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Real Time Simulation-based Supply Chain Analytics | wh</title>
<meta name=keywords content="Python,SimPy,AWS,LocalStack,Terraform,Slides"><meta name=description content="Proof of Concept using Casymda on AWS, ft. Terraform and LocalStack"><meta name=author content><link rel=canonical href=https://fladdimir.github.io/post/csa-simulation-based-sc-forecast/><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://fladdimir.github.io/icon.png><link rel=icon type=image/png sizes=16x16 href=https://fladdimir.github.io/icon.png><link rel=icon type=image/png sizes=32x32 href=https://fladdimir.github.io/icon.png><link rel=apple-touch-icon href=https://fladdimir.github.io/apple-touch-icon.png><link rel=mask-icon href=https://fladdimir.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script data-goatcounter=https://fmghio.goatcounter.com/count async src=//gc.zgo.at/count.js></script><meta property="og:title" content="Real Time Simulation-based Supply Chain Analytics"><meta property="og:description" content="Proof of Concept using Casymda on AWS, ft. Terraform and LocalStack"><meta property="og:type" content="article"><meta property="og:url" content="https://fladdimir.github.io/post/csa-simulation-based-sc-forecast/"><meta property="og:image" content="https://fladdimir.github.io/post/csa-simulation-based-sc-forecast/featured.jpg"><meta property="article:section" content="post"><meta property="article:published_time" content="2020-10-27T00:00:00+00:00"><meta property="article:modified_time" content="2020-10-27T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://fladdimir.github.io/post/csa-simulation-based-sc-forecast/featured.jpg"><meta name=twitter:title content="Real Time Simulation-based Supply Chain Analytics"><meta name=twitter:description content="Proof of Concept using Casymda on AWS, ft. Terraform and LocalStack"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://fladdimir.github.io/post/"},{"@type":"ListItem","position":2,"name":"Real Time Simulation-based Supply Chain Analytics","item":"https://fladdimir.github.io/post/csa-simulation-based-sc-forecast/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Real Time Simulation-based Supply Chain Analytics","name":"Real Time Simulation-based Supply Chain Analytics","description":"Proof of Concept using Casymda on AWS, ft. Terraform and LocalStack","keywords":["Python","SimPy","AWS","LocalStack","Terraform","Slides"],"articleBody":"In his highly interesting, recently published PhD thesis (German), Toni Donhauser from the University Erlangen-Nürnberg gives an excellent example on how a production-synchronous digital twin can be used for automated, simulation-based order scheduling in masonry plants.\nAs a core feature, the developed simulation allows to initialize the work-in-process of the manufacturing system to precisely mirror the current state and create accurate short-term forecasts, which serve as a basis for comparing alternatives and optimizing production plans in case of unexpected disruptions. Tecnomatix Plant Simulation (Siemens) is used for the implementation of the simulation model. Manufacturing data is fetched via the built-in OPC-UA interface from an OPC server and via ODBC from an MS Access database. Simulation runs can be triggered manually by an operator using a management application written in C#.\nSince Plant Simulation is known for extensive features as well as for extensive licensing fees, this blog post will present an alternative implementation of such a production-synchronous digital twin, based on open-source frameworks and building on easy-to-operate, pay-per-use AWS infrastructure.\nThe complete setup can be deployed and tested locally using Docker, LocalStack and Terraform (no AWS account required).\nGet the repo from github: https://github.com/fladdimir/csa-simulation-based-sc-forecast\nSlides: https://fladdimir.github.io/presentations/des-on-aws.html\nPaper on researchgate\nScenario \u0026 Scope The chart below shows a fictive and simplified order manufacturing process, serving as a minimal example to illustrate how a digital twin of the system can be implemented.\nAfter being created, orders are received and accepted by the company (“ingest”-step), and the order-specific raw material is ordered (“order_material”), leaving the order waiting until the corresponding material arrives (“wait_for_material”). When the material is delivered, the order proceeds to a queue (“wait_for_sop”), waiting to be processed in a capacity-constrained “production”-step, which is only able to process one order at a time. Eventually, the finished order gets delivered to the customer and leaves the system.\nWhenever material for an order is requested, an initial estimated time of arrival (ETA) is assigned. However, unexpected supplier-specific process deviations or other delivery problems may introduce delays at any point in time, so that ETA-updates are possible during this step. Since the production step uses a capacity-constrained resource and represents a possible bottleneck of the system, any unplanned under-utilization here may delay every upcoming order and diminish the system throughput (depending on how tight the schedule looks like). Therefore, it is desirable to be able to quantify the effects of any shift in time as soon as an ETA-update for an order occurs.\nSynchronized Digital Twin: Concept and Implementation The next figure shows a simple event-processing pipeline, able to ingest defined events and to persist the system state (event tracking), which in turn enables the simulation-based creation of forecasts for expected order completions times and delays (event analytics). A simple web-dashboard will be used to visualize the results.\n1. Publishing events of data producers During the processing of an order in the physical system, data producers such as sensors and IoT-devices are capturing information on the progress, i.e. events of state-changes as e.g. start or finish of the production step of an order. These order updates are published to a defined endpoint where they are collected and processed (2.). While those events would actually be happening in the physical manufacturing system, a simulation model might be used to create test-data for the digital twin (see the post on Virtual Commissioning for another example of this use-case for simulation).\n2. Capturing events with AWS Kinesis Kinesis is an AWS service for continuous buffering and real-time processing of streaming data. A Kinesis stream decouples data producers and consumers and consists of a configurable number of shards, each of which is able to ingest up to 1 MB or 1000 records of data per second. Each record is put into one shard based on it’s specified partition key value, which gets important since in-order processing of records is guaranteed only on partition key level.\nIn the described scenario in-order processing becomes critical for ETA-updates of orders, since the message of an expected delay must not be processed before an earlier submitted update.\nNew records can be put to the stream e.g. using the AWS SDK, which is available for various languages, including Python which is used for the emulated test client.\n3. Processing events with AWS Lambda Lambda is the function-as-a-service offer of AWS, which allows to run code on-demand, paying for the number of invocations as well as for execution time. Lambda functions can easily be integrated with other services such as SQS and DynamoDB. Since AWS provisions the function runtime on-demand, the short cold-start times of NodeJS and Python make them a popular choice for implementing lambdas, while “heavier” alternatives such as Java are less common (the JVM would need multiple invocations for the JIT-compilation to boost performance).\nThe lambda implemented for processing order updates is simple and just updates the corresponding item of the affected order in a specified DynamoDB table with data from the event provided as part of the invocation.\n4. Persisting the system state with DynamoDB DynamoDB is used as a fast, flexible and managed NoSQL database. While this type of database by design lacks some of the amenities of relational databases (such as proper means to enforce referential integrity on the database level, or the availability of sophisticated ORMs and schema management tools), it is fine for our simple use-case which just involves updating single items and basic queries. DynamoDB requires a hashkey and optionally a partition key, both of which are used in combination to uniquely identify a stored item. For orders the string id can be used as the hashkey. A nice feature of DynamoDB is the option to enable streams, automatically providing information on table-updates. This way, order ETA-updates can trigger new forecasts.\n5. Simulating the future AWS allows to use Lambda functions as DynamoDB stream event consumers, so that simulation runs can forecast future order completion times on every state change.\nFor each run, the complete system state is fetched from the DynamoDB (which might actually need multiple requests, since a single scan might only return a page of up to 1 MB of data).\nBased on the registered process timestamps, the currently relevant process step of each order can be identified.\nThe simulation model is generated from the process diagram shown above using Casymda. For the sake of simplicity of this proof of concept, processing times are assumed to be deterministic (even though stochastic behavior could be easily modeled, it would require averaging multiple runs). Model blocks are implemented to account for already elapsed processing time of work-in-process-entities at the start of the simulation (one of the possibilities to initialize online simulation models discussed in the often-cited paper of Hanisch and Tolujew, 2005, further explored by Hotz, 2007). During the execution, forecast metrics are collected in form of predicted process step completion times. Currently, AWS allows Lambda function executions to take up to 15 minutes, so that even complex models can be run this way. However, frequent and long running calculations might make it more attractive to create a dedicated service.\n6. + 7. Forecast persistence and visualization At the end of each run, the gathered results are persisted in a second DynamoDB table, from where a dashboard application can access and visualize the data.\nPlotly Dash is a popular framework for analytics web-apps. It enables the quick creation of dynamic dashboards just by writing Python code. Under the hood, it uses flask to serve React websites with plotly charts to a browser. Data queries and analysis are done on the backend using Python. The implemented dashboard just contains a simple gantt-chart (and serves only as a very basic example, leaving lots of room for extension). Automatic dashboard refreshes are implemented using an interval-callback to cyclically poll the database for updates.\nA dashboard’s Docker container could be run on AWS (e.g. ECS/Fargate, but since the free version of LocalStack does not include this it will just be run locally for demonstration).\nResult To run the setup locally from within the cloned repository, Docker and Terraform need to be installed.\nEven though the performance is not comparable to the actual cloud service, LocalStack is an awesome option to serve a multitude of AWS services locally, including Kinesis, Lambda, and DynamoDB. LocalStack can be started in a privileged Docker container, spawning more containers as needed, e.g. for executing Lambdas. It can be started via:\ndocker-compose up localstack Before the Lambda functions can be deployed, the function code and its dependencies need to be packaged:\ndocker-compose up package-ingest-lambda package-simulation-lambda Terraform is a great and widespread tool which can automatically provision infrastructure resources described in configuration files (however, have a look at this article for a more nuanced analysis). To create all required resources, two terraform commands are needed:\ncd terraform terraform init # required once terraform apply # enter 'yes' when prompted to confirm the changes (or use -auto-approve) cd ../ # return to project root (To prevent 404 errors when calling apply after a restart of LocalStack without calling terraform destroy, first delete the terraform.tfstate files next to main.tf.)\nAfter the successfull creation, two more containers can be started - one serving the dashboard and one running a simulation model to emulate real event producers:\ndocker-compose up dashboard emulation Before (re-)starting any test-run, the DynamoDB-tables need to be cleared:\ndocker-compose up truncate-tables http://localhost:8050 should now show the empty dashboard, while http://localhost:5001 should show the generic Casymda web canvas animation controls. To enable automatic refreshes use the switch above the chart on the dashboard.\nWhen starting the emulation, orders will be created at the source and flow through the defined process.\nAt the same time, the dashboard should update with a minor delay and visualize the completion times of the relevant process steps of all orders which are currently present in the system. A vertical line in the chart indicates the point in time when the simulation run started and the forecast was created.\nSample flow 1. The first order is created The simulation forecasts process step completion times as defined in the model: 2. The second order arrives and Order-1 production starts The forecast does not show problems: 3. After some time, an ETA update for the Order-2 material delivery is communicated, and a delay of 1/3 is now expected The forecast shows the announced delay (orange) and the expected shift of the production step of Order-2: 4. Order-1 is finished (and therefore excluded from the forecast), but now Order-3 arrives The forecast reveals an upcoming problem! Caused by the capacity constraint of the production step (max. one order concurrently), the delay of Order-2 (orange) will also prevent to start the of production of Order-3 on time, even though the material is expected to be ready by then (red): 5. When Order-2 is almost finished, a 4th order comes in As the forecast shows, the delay of Order-2 will cascade and also affect Order-4: Complete screen-cast:\nThere should have been a video here but your browser does not seem to support it. While this was just a proof of concept and the presented example would have been easy to calculate by hand, there are plenty of improvements and extensions imaginable.\nLooking at the scenario and business use-case, it would be interesting to add more complexity to the process, such as inventory for raw materials, and different replenishment strategies. Similarly, the impacts of stochastic or planned machine maintenance intervals might be evaluated. Another extension would be to incorporate targets into the process, such as order-specific due dates or throughput goals. This might then ask for additional optimization procedures to determine optimal production control policies (similar to the case presented in the thesis mentioned in the beginning of this article).\nInteresting technical extensions include security aspects such as authentication and authorization of different data producing parties, as well as an integration of the IoT-related services of AWS, which might offer dedicated features to gather data with sensors and edge devices for the digital twin. Concerning the analytics of ingested event data, stream processing solutions such as AWS Kinesis Data Analytics might be useful to identify relevant patterns and trigger forecast and optimization runs only in case of critical process deviations.\n","wordCount":"2030","inLanguage":"en","datePublished":"2020-10-27T00:00:00Z","dateModified":"2020-10-27T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://fladdimir.github.io/post/csa-simulation-based-sc-forecast/"},"publisher":{"@type":"Organization","name":"wh","logo":{"@type":"ImageObject","url":"https://fladdimir.github.io/icon.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://fladdimir.github.io/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://fladdimir.github.io/tags/ title=/tags><span>/tags</span></a></li><li><a href=https://fladdimir.github.io/search title="/search (Alt + /)" accesskey=/><span>/search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://fladdimir.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://fladdimir.github.io/post/>Posts</a></div><h1 class=post-title>Real Time Simulation-based Supply Chain Analytics</h1><div class=post-meta><span title='2020-10-27 00:00:00 +0000 UTC'>October 27, 2020</span>&nbsp;·&nbsp;10 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#scenario--scope>Scenario & Scope</a></li><li><a href=#synchronized-digital-twin-concept-and-implementation>Synchronized Digital Twin: Concept and Implementation</a><ul><li><a href=#1-publishing-events-of-data-producers>1. Publishing events of data producers</a></li><li><a href=#2-capturing-events-with-aws-kinesis>2. Capturing events with AWS Kinesis</a></li><li><a href=#3-processing-events-with-aws-lambda>3. Processing events with AWS Lambda</a></li><li><a href=#4-persisting-the-system-state-with-dynamodb>4. Persisting the system state with DynamoDB</a></li><li><a href=#5-simulating-the-future>5. Simulating the future</a></li><li><a href=#6--7-forecast-persistence-and-visualization>6. + 7. Forecast persistence and visualization</a></li></ul></li><li><a href=#result>Result</a><ul><li><a href=#sample-flow>Sample flow</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p>In his highly interesting, recently published <a href=http://nbn-resolving.de/urn:nbn:de:bvb:29-opus4-145483>PhD thesis (German)</a>, Toni Donhauser from the University Erlangen-Nürnberg gives an excellent example on how a production-synchronous digital twin can be used for automated, simulation-based order scheduling in masonry plants.</p><p>As a core feature, the developed simulation allows to initialize the <em>work-in-process</em> of the manufacturing system to precisely mirror the current state and create accurate short-term forecasts, which serve as a basis for comparing alternatives and optimizing production plans in case of unexpected disruptions.
<a href=https://www.plm.automation.siemens.com/global/de/products/manufacturing-planning/plant-simulation-throughput-optimization.html>Tecnomatix Plant Simulation (Siemens)</a> is used for the implementation of the simulation model.
Manufacturing data is fetched via the built-in OPC-UA interface from an OPC server and via ODBC from an MS Access database.
Simulation runs can be triggered manually by an operator using a management application written in C#.</p><p>Since <em>Plant Simulation</em> is known for extensive features as well as for extensive licensing fees, this blog post will present an alternative implementation of such a production-synchronous digital twin, based on open-source frameworks and building on easy-to-operate, pay-per-use AWS infrastructure.</p><p>The complete setup can be deployed and tested locally using <em>Docker</em>, <em>LocalStack</em> and <em>Terraform</em> (no AWS account required).</p><blockquote><p>Get the repo from github: <a href=https://github.com/fladdimir/csa-simulation-based-sc-forecast>https://github.com/fladdimir/csa-simulation-based-sc-forecast</a></p><p>Slides: <a href=https://fladdimir.github.io/presentations/des-on-aws.html>https://fladdimir.github.io/presentations/des-on-aws.html</a></p><p>Paper <a href=https://www.researchgate.net/publication/359098225_A_brief_introduction_to_deploy_Amazon_Web_Services_for_online_discrete-event_simulation>on researchgate</a></p></blockquote><p><img loading=lazy src=featured.jpg alt=featured></p><h2 id=scenario--scope>Scenario & Scope<a hidden class=anchor aria-hidden=true href=#scenario--scope>#</a></h2><p>The chart below shows a fictive and simplified order manufacturing process, serving as a minimal example to illustrate how a digital twin of the system can be implemented.<br>After being created, orders are received and accepted by the company (<em>&ldquo;ingest&rdquo;</em>-step), and the order-specific raw material is ordered (<em>&ldquo;order_material&rdquo;</em>), leaving the order waiting until the corresponding material arrives (<em>&ldquo;wait_for_material&rdquo;</em>).
When the material is delivered, the order proceeds to a queue (<em>&ldquo;wait_for_sop&rdquo;</em>), waiting to be processed in a capacity-constrained <em>&ldquo;production&rdquo;</em>-step, which is only able to process one order at a time.
Eventually, the finished order gets delivered to the customer and leaves the system.</p><p><img loading=lazy src=img/diagram.png alt=process-flow></p><p>Whenever material for an order is requested, an initial estimated time of arrival (ETA) is assigned.
However, unexpected supplier-specific process deviations or other delivery problems may introduce delays at any point in time, so that ETA-updates are possible during this step.
Since the production step uses a capacity-constrained resource and represents a possible bottleneck of the system, any unplanned under-utilization here may delay every upcoming order and diminish the system throughput (depending on how tight the schedule looks like).
Therefore, it is desirable to be able to quantify the effects of any shift in time as soon as an ETA-update for an order occurs.</p><hr><h2 id=synchronized-digital-twin-concept-and-implementation>Synchronized Digital Twin: Concept and Implementation<a hidden class=anchor aria-hidden=true href=#synchronized-digital-twin-concept-and-implementation>#</a></h2><p>The next figure shows a simple event-processing pipeline, able to ingest defined events and to persist the system state (<em>event tracking</em>), which in turn enables the simulation-based creation of forecasts for expected order completions times and delays (<em>event analytics</em>).
A simple web-dashboard will be used to visualize the results.</p><p><img loading=lazy src=img/concept.png alt=event-processing-pipeline></p><h3 id=1-publishing-events-of-data-producers>1. Publishing events of data producers<a hidden class=anchor aria-hidden=true href=#1-publishing-events-of-data-producers>#</a></h3><p>During the processing of an order in the physical system, data producers such as sensors and IoT-devices are capturing information on the progress, i.e. events of state-changes as e.g. start or finish of the production step of an order.
These order updates are published to a defined endpoint where they are collected and processed (2.).
While those events would actually be happening in the physical manufacturing system, a simulation model might be used to create test-data for the digital twin (see the <a href=../csa-vcom/>post on <em>Virtual Commissioning</em></a> for another example of this use-case for simulation).</p><p><img loading=lazy src=./img/icons/kinesis.png alt=kinesis></p><h3 id=2-capturing-events-with-aws-kinesis>2. Capturing events with AWS Kinesis<a hidden class=anchor aria-hidden=true href=#2-capturing-events-with-aws-kinesis>#</a></h3><p><a href=https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html>Kinesis</a> is an AWS service for continuous buffering and real-time processing of streaming data.
A Kinesis <em>stream</em> decouples data producers and consumers and consists of a configurable number of <em>shards</em>, each of which is able to ingest up to 1 MB or 1000 records of data per second.
Each record is put into one shard based on it&rsquo;s specified <em>partition key</em> value, which gets important since in-order processing of records is <a href=https://aws.amazon.com/blogs/compute/new-aws-lambda-scaling-controls-for-kinesis-and-dynamodb-event-sources/>guaranteed only on partition key level</a>.<br>In the described scenario in-order processing becomes critical for ETA-updates of orders, since the message of an expected delay must not be processed before an earlier submitted update.<br>New records can be put to the stream e.g. using the AWS SDK, which is available for various languages, including Python which is used for the emulated test client.</p><p><img loading=lazy src=./img/icons/lambda.png alt=lambda></p><h3 id=3-processing-events-with-aws-lambda>3. Processing events with AWS Lambda<a hidden class=anchor aria-hidden=true href=#3-processing-events-with-aws-lambda>#</a></h3><p><a href="https://aws.amazon.com/lambda/?nc1=h_ls">Lambda</a> is the function-as-a-service offer of AWS, which allows to run code on-demand, paying for the number of invocations as well as for execution time.
Lambda functions can easily be integrated with other services such as SQS and DynamoDB.
Since AWS provisions the function runtime on-demand, the short cold-start times of NodeJS and Python make them a popular choice for implementing lambdas, while &ldquo;heavier&rdquo; alternatives such as Java are less common (the JVM would need multiple invocations for the JIT-compilation to boost performance).<br>The lambda implemented for processing order updates is simple and just updates the corresponding item of the affected order in a specified DynamoDB table with data from the event provided as part of the invocation.</p><p><img loading=lazy src=./img/icons/dynamodb.png alt=dynamodb></p><h3 id=4-persisting-the-system-state-with-dynamodb>4. Persisting the system state with DynamoDB<a hidden class=anchor aria-hidden=true href=#4-persisting-the-system-state-with-dynamodb>#</a></h3><p><a href=https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html>DynamoDB</a> is used as a fast, flexible and managed NoSQL database.
While this type of database by design lacks some of the amenities of relational databases (such as proper means to enforce referential integrity on the database level, or the availability of sophisticated ORMs and schema management tools), it is fine for our simple use-case which just involves updating single items and basic queries.
DynamoDB requires a hashkey and optionally a partition key, both of which are used in combination to uniquely identify a stored item.
For orders the string id can be used as the hashkey.
A nice feature of DynamoDB is the option to enable <em>streams</em>, automatically providing information on table-updates. This way, order ETA-updates can trigger new forecasts.</p><p><img loading=lazy src=./img/icons/lambda.png alt=lambda></p><h3 id=5-simulating-the-future>5. Simulating the future<a hidden class=anchor aria-hidden=true href=#5-simulating-the-future>#</a></h3><p>AWS allows to use Lambda functions as DynamoDB stream event consumers, so that simulation runs can forecast future order completion times on every state change.<br>For each run, the complete system state is fetched from the DynamoDB (which might actually need multiple requests, since a single scan might only return a page of up to 1 MB of data).<br>Based on the registered process timestamps, the currently relevant process step of each order can be identified.<br>The simulation model is generated from the process diagram shown above using <a href=../casymda/>Casymda</a>.
For the sake of simplicity of this proof of concept, processing times are assumed to be deterministic (even though stochastic behavior could be easily modeled, it would require averaging multiple runs).
Model blocks are implemented to account for already elapsed processing time of <em>work-in-process</em>-entities at the start of the simulation (one of the possibilities to initialize <em>online simulation</em> models discussed in the often-cited paper of <a href=https://informs-sim.org/wsc05papers/222.pdf>Hanisch and Tolujew, 2005</a>, further explored by <a href=https://d-nb.info/987148664/34>Hotz, 2007</a>).
During the execution, forecast metrics are collected in form of predicted process step completion times.
Currently, AWS allows Lambda function executions to take up to 15 minutes, so that even complex models can be run this way.
However, frequent and long running calculations might make it more attractive to create a dedicated service.</p><p><img loading=lazy src=./img/icons/plotly_dash_icon.png alt=dash></p><h3 id=6--7-forecast-persistence-and-visualization>6. + 7. Forecast persistence and visualization<a hidden class=anchor aria-hidden=true href=#6--7-forecast-persistence-and-visualization>#</a></h3><p>At the end of each run, the gathered results are persisted in a second DynamoDB table, from where a dashboard application can access and visualize the data.<br><a href=https://github.com/plotly/dash>Plotly Dash</a> is a popular framework for analytics web-apps.
It enables the quick creation of dynamic dashboards just by writing Python code.
Under the hood, it uses <a href=https://flask.palletsprojects.com/en/1.1.x/>flask</a> to serve <a href=https://dash.plotly.com/react-for-python-developers>React</a> websites with <a href=https://github.com/plotly/plotly.js/>plotly charts</a> to a browser.
Data queries and analysis are done on the backend using Python.
The implemented dashboard just contains a simple gantt-chart (and serves only as a very basic example, leaving lots of room for extension).
Automatic dashboard refreshes are implemented using an <em>interval</em>-callback to cyclically poll the database for updates.<br>A dashboard&rsquo;s Docker container could be run on AWS (e.g. ECS/Fargate, but since the free version of LocalStack does not include this it will just be run locally for demonstration).</p><hr><h2 id=result>Result<a hidden class=anchor aria-hidden=true href=#result>#</a></h2><p>To run the setup locally from within the cloned repository, Docker and Terraform need to be installed.<br>Even though the performance is not comparable to the actual cloud service, <a href=https://github.com/localstack/localstack>LocalStack</a> is an awesome option to serve a multitude of AWS services locally, including Kinesis, Lambda, and DynamoDB.
LocalStack can be started in a privileged Docker container, spawning more containers as needed, e.g. for executing Lambdas.
It can be started via:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>docker-compose up localstack
</span></span></code></pre></div><p>Before the Lambda functions can be deployed, the function code and its dependencies need to be packaged:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>docker-compose up package-ingest-lambda package-simulation-lambda
</span></span></code></pre></div><p><a href=https://www.terraform.io/>Terraform</a> is a great and widespread tool which can automatically provision infrastructure resources described in configuration files (however, have a look at <a href=https://itnext.io/things-i-wish-i-knew-about-terraform-before-jumping-into-it-43ee92a9dd65>this article</a> for a more nuanced analysis).
To create all required resources, two terraform commands are needed:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl><span class=nb>cd</span> terraform
</span></span><span class=line><span class=cl>terraform init <span class=c1># required once</span>
</span></span><span class=line><span class=cl>terraform apply <span class=c1># enter &#39;yes&#39; when prompted to confirm the changes (or use -auto-approve)</span>
</span></span><span class=line><span class=cl><span class=nb>cd</span> ../ <span class=c1># return to project root</span>
</span></span></code></pre></div><p>(To prevent 404 errors when calling <code>apply</code> after a restart of LocalStack without calling <code>terraform destroy</code>, first delete the <code>terraform.tfstate</code> files next to <code>main.tf</code>.)</p><p>After the successfull creation, two more containers can be started - one serving the dashboard and one running a simulation model to emulate real event producers:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>docker-compose up dashboard emulation
</span></span></code></pre></div><p>Before (re-)starting any test-run, the DynamoDB-tables need to be cleared:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>docker-compose up truncate-tables
</span></span></code></pre></div><p><a href=http://localhost:8050>http://localhost:8050</a> should now show the empty dashboard, while <a href=http://localhost:5001>http://localhost:5001</a> should show the generic Casymda web canvas animation controls.
To enable automatic refreshes use the switch above the chart on the dashboard.</p><p>When starting the emulation, orders will be created at the source and flow through the defined process.<br>At the same time, the dashboard should update with a minor delay and visualize the completion times of the relevant process steps of all orders which are currently present in the system.
A vertical line in the chart indicates the point in time when the simulation run started and the forecast was created.</p><hr><h3 id=sample-flow>Sample flow<a hidden class=anchor aria-hidden=true href=#sample-flow>#</a></h3><h4 id=1-the-first-order-is-created>1. The first order is created<a hidden class=anchor aria-hidden=true href=#1-the-first-order-is-created>#</a></h4><p><img loading=lazy src=img/sc/e1.png alt=e1></p><p>The simulation forecasts process step completion times as defined in the model:
<img loading=lazy src=img/sc/g1.png alt=g1></p><hr><h4 id=2-the-second-order-arrives-and-_order-1_-production-starts>2. The second order arrives and <em>Order-1</em> production starts<a hidden class=anchor aria-hidden=true href=#2-the-second-order-arrives-and-_order-1_-production-starts>#</a></h4><p><img loading=lazy src=img/sc/e2.png alt=e1></p><p>The forecast does not show problems:
<img loading=lazy src=img/sc/g2.png alt=g1></p><hr><h4 id=3-after-some-time-an-eta-update-for-the-_order-2_-material-delivery-is-communicated-and-a-delay-of-13-is-now-expected>3. After some time, an ETA update for the <em>Order-2</em> material delivery is communicated, and a delay of 1/3 is now expected<a hidden class=anchor aria-hidden=true href=#3-after-some-time-an-eta-update-for-the-_order-2_-material-delivery-is-communicated-and-a-delay-of-13-is-now-expected>#</a></h4><p><img loading=lazy src=img/sc/e3.png alt=e1></p><p>The forecast shows the announced delay (orange) and the expected shift of the production step of <em>Order-2</em>:
<img loading=lazy src=img/sc/g3.png alt=g1></p><hr><h4 id=4-_order-1_-is-finished-and-therefore-excluded-from-the-forecast-but-now-_order-3_-arrives>4. <em>Order-1</em> is finished (and therefore excluded from the forecast), but now <em>Order-3</em> arrives<a hidden class=anchor aria-hidden=true href=#4-_order-1_-is-finished-and-therefore-excluded-from-the-forecast-but-now-_order-3_-arrives>#</a></h4><p><img loading=lazy src=img/sc/e4.png alt=e1></p><p>The forecast reveals an upcoming problem! Caused by the capacity constraint of the production step (max. one order concurrently), the delay of <em>Order-2</em> (orange) will also prevent to start the of production of <em>Order-3</em> on time, even though the material is expected to be ready by then (red):
<img loading=lazy src=img/sc/g4.png alt=g1></p><hr><h4 id=5-when-_order-2_-is-almost-finished-a-4th-order-comes-in>5. When <em>Order-2</em> is almost finished, a 4th order comes in<a hidden class=anchor aria-hidden=true href=#5-when-_order-2_-is-almost-finished-a-4th-order-comes-in>#</a></h4><p><img loading=lazy src=img/sc/e5.png alt=e1></p><p>As the forecast shows, the delay of <em>Order-2</em> will cascade and also affect <em>Order-4</em>:
<img loading=lazy src=img/sc/g5.png alt=g1></p><hr><p>Complete screen-cast:</p><video style=width:100% autoplay loop controls src=img/sc/peek5.mp4 type=video/mp4>
There should have been a video here but your browser does not seem
to support it.</video><hr><p>While this was just a proof of concept and the presented example would have been easy to calculate by hand, there are plenty of improvements and extensions imaginable.</p><p>Looking at the scenario and business use-case, it would be interesting to add more complexity to the process, such as inventory for raw materials, and different replenishment strategies.
Similarly, the impacts of stochastic or planned machine maintenance intervals might be evaluated.
Another extension would be to incorporate targets into the process, such as order-specific due dates or throughput goals.
This might then ask for additional optimization procedures to determine optimal production control policies (similar to the case presented in the thesis mentioned in the beginning of this article).</p><p>Interesting technical extensions include security aspects such as authentication and authorization of different data producing parties, as well as an integration of the IoT-related services of AWS, which might offer dedicated features to gather data with sensors and edge devices for the digital twin.
Concerning the analytics of ingested event data, stream processing solutions such as <em>AWS Kinesis Data Analytics</em> might be useful to identify relevant patterns and trigger forecast and optimization runs only in case of critical process deviations.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://fladdimir.github.io/tags/python/>Python</a></li><li><a href=https://fladdimir.github.io/tags/simpy/>SimPy</a></li><li><a href=https://fladdimir.github.io/tags/aws/>AWS</a></li><li><a href=https://fladdimir.github.io/tags/localstack/>LocalStack</a></li><li><a href=https://fladdimir.github.io/tags/terraform/>Terraform</a></li><li><a href=https://fladdimir.github.io/tags/slides/>Slides</a></li></ul><nav class=paginav><a class=prev href=https://fladdimir.github.io/post/csa4j+cs/><span class=title>« Prev</span><br><span>Block-based Modeling with SimPy - in Java & C#</span>
</a><a class=next href=https://fladdimir.github.io/post/csa-vcom/><span class=title>Next »</span><br><span>Logistics Process Models for Automated Integration Testing</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://fladdimir.github.io/>wh</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>