<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts | Wladimir Hofmann | Homepage</title><link>/post/</link><atom:link href="/post/index.xml" rel="self" type="application/rss+xml"/><description>Posts</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 21 Mar 2020 00:00:00 +0000</lastBuildDate><image><url>img/map[gravatar:%!s(bool=false) shape:circle]</url><title>Posts</title><link>/post/</link></image><item><title>Using A Digital Twin for Assembly Line Supply Strategy Planning</title><link>/post/tugger-routing/</link><pubDate>Sat, 21 Mar 2020 00:00:00 +0000</pubDate><guid>/post/tugger-routing/</guid><description>&lt;p>&lt;em>Artificial Intelligence&lt;/em>, &lt;em>Digital Twins&lt;/em>, &lt;em>Industry 4.0&lt;/em>&amp;hellip; you might have &lt;em>heard&lt;/em> these buzzwords before.&lt;br>
But wouldn&amp;rsquo;t it be cool to &lt;em>apply&lt;/em> them, solving an actual problem?&lt;br>
To learn about practical application rather than abstract concepts?&lt;br>
To know which challenges to master, and which pitfalls to avoid?&lt;/p>
&lt;p>Great, so this article is for &lt;em>you&lt;/em>!&lt;br>
It will..&lt;/p>
&lt;ul>
&lt;li>describe how to use reinforcement learning to solve a production-logistics problem&lt;/li>
&lt;li>explain basic concepts of reinforcement learning and industrial digital twins&lt;/li>
&lt;li>present an in-depth case-study on assembly line supply strategy planning&lt;/li>
&lt;/ul>
&lt;p>And it will include code, ready for execution on &lt;em>your&lt;/em> machine. Docker-based, no further installation needed.&lt;/p>
&lt;blockquote>
&lt;p>Get the repo:
&lt;a href="https://github.com/fladdimir/tugger-routing">https://github.com/fladdimir/tugger-routing&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>&lt;em>Disclaimer:&lt;/em>
This article is the result of leisure-time work. It represents our individual perspectives on the discussed topics. It&amp;rsquo;s a report capturing our personal experiences, rather than a purely scientific study. The content, concepts, and their implementation can probably be improved in virtually any imaginable way.&lt;br>
We still hope that you enjoy reading.&lt;/p>
&lt;!-- omit in toc -->
&lt;h2 id="outline">Outline&lt;/h2>
&lt;ul>
&lt;li>
&lt;a href="#introduction--basics">Introduction &amp;amp; Basics&lt;/a>
&lt;ul>
&lt;li>
&lt;a href="#reinforcement-learning">Reinforcement Learning&lt;/a>&lt;/li>
&lt;li>
&lt;a href="#digital-twins-and-discrete-event-simulation">Digital Twins and Discrete Event Simulation&lt;/a>&lt;/li>
&lt;li>
&lt;a href="#wrapping-a-des-model-in-a-gym-environment">Wrapping a DES-Model in a Gym-Environment&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;a href="#case-study">Case-Study&lt;/a>
&lt;ul>
&lt;li>
&lt;a href="#scenario">Scenario&lt;/a>&lt;/li>
&lt;li>
&lt;a href="#simulation-model">Simulation Model&lt;/a>&lt;/li>
&lt;li>
&lt;a href="#preparing-the-gym-environment">Preparing the Gym-Environment&lt;/a>&lt;/li>
&lt;li>
&lt;a href="#rl-agent-training--evaluation">RL-Agent Training &amp;amp; Evaluation&lt;/a>&lt;/li>
&lt;li>
&lt;a href="#performance-comparison">Performance Comparison&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;a href="#summing-up">Summing Up&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="introduction--basics">Introduction &amp;amp; Basics&lt;/h2>
&lt;p>Reinforcement learning represents an emerging technique from machine learning.
It can autonomously derive complex action sequences in dynamic environments and is successfully applied in various fields, e.g. from
&lt;a href="https://openai.com/blog/" target="_blank" rel="noopener">robotics and gaming&lt;/a>.
Instead of explicitly defining a specific solution strategy for a problem, we can just provide an &lt;em>environment&lt;/em>. A self-learning &lt;em>agent&lt;/em> will then autonomously discover successful strategies just by interaction.&lt;/p>
&lt;p>Needless to say, there is nothing new under the moon and
&lt;a href="https://www.researchgate.net/publication/326039529_Optimization_of_global_production_scheduling_with_deep_reinforcement_learning" target="_blank" rel="noopener">previous studies&lt;/a> show the general feasibility of using RL for solving production-logistics problems.&lt;/p>
&lt;p>So why do we think that there is the need for yet another article about this very topic?&lt;/p>
&lt;p>First, there is a lot of active development in &lt;em>RL&lt;/em>, as well as in the application of &lt;em>Digital Twins&lt;/em> in production/logistics.
We believe that there lies even more potential in integrating these concepts.
Furthermore, we found the often derogatory-treated &lt;em>&amp;ldquo;low-level implementation work&amp;rdquo;&lt;/em> to be an actual obstacle for making progress in this challenging and highly inter-disciplinary area of applied research.
This contribution therefore strives to show a working example based on a tool-stack which seamlessly integrates two of the most popular open-source software packages from their respective areas:
&lt;a href="https://stable-baselines.readthedocs.io/en/master/" target="_blank" rel="noopener">stable-baselines&lt;/a> for &lt;em>RL&lt;/em> and
&lt;a href="https://simpy.readthedocs.io/en/latest/" target="_blank" rel="noopener">SimPy&lt;/a> for implementing &lt;em>Digital Twins&lt;/em>.&lt;/p>
&lt;h3 id="reinforcement-learning">Reinforcement Learning&lt;/h3>
&lt;p>If you still ask yourself what RL is capable of, we definitely recommend to have a look at
&lt;a href="https://openai.com/blog/emergent-tool-use/" target="_blank" rel="noopener">what the guys from openai are doing&lt;/a>.&lt;/p>
&lt;p>Admittedly, thats probably a quite sophisticated and highly engineered example, but it breaks down to a simple interaction between an &lt;em>agent&lt;/em> and an &lt;em>environment&lt;/em>.
Technically, this interaction is defined by an interface (or abstract base-class as Python likes to put it), which is part of the
&lt;a href="https://gym.openai.com/docs/" target="_blank" rel="noopener">gym-package&lt;/a>.&lt;/p>
&lt;p>The graphic below illustrates the exchange of information between agent and environment.
First, the agent calls the environment&amp;rsquo;s &lt;em>step&lt;/em> method, providing the action to be executed.
The environment then processes the action and returns:&lt;/p>
&lt;ul>
&lt;li>the new state of the system (&lt;em>observation&lt;/em>),&lt;/li>
&lt;li>the &lt;em>reward&lt;/em> which occured during the step (might be zero),&lt;/li>
&lt;li>a &lt;em>done&lt;/em> value potentially indicating the end of an episode (and the need for a subsequent &lt;em>reset&lt;/em>)&lt;/li>
&lt;li>and an &lt;em>info&lt;/em>-object (might contain additional information e.g. for logging purposes).&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="diagrams/gym_rl_environment_interface.jpg" alt="Gym-RL-Environment-Interface">&lt;/p>
&lt;p>The interface also prescribes some more stuff, such as the formats of &lt;em>action-space&lt;/em> and &lt;em>observation_space&lt;/em>, as well as &lt;em>render&lt;/em> and &lt;em>reset&lt;/em> behavior.&lt;br>
The various RL algorithms provided by the
&lt;a href="https://pypi.org/project/stable-baselines/" target="_blank" rel="noopener">stable-baselines&lt;/a>-package are ready to work with environments implementing this gym-interface.
All that is left to do is creating a compliant environment - and in the next section we will show how this can be achieved in the domain of logistics.&lt;/p>
&lt;h3 id="digital-twins-and-discrete-event-simulation">Digital Twins and Discrete Event Simulation&lt;/h3>
&lt;p>Frankly, &lt;em>Digital Twin&lt;/em> is probably the most overused buzzword of all the &amp;ldquo;Lostistics 4.0 / Industry 4.0&amp;rdquo; stuff that is out there.
Even though we could not resist to put it into the title, from now on we&amp;rsquo;ll prove that we can do better and use the more specific term &amp;ldquo;Discrete Event Simulation&amp;rdquo; (DES) instead.&lt;/p>
&lt;p>Why DES? Discrete Event Simulation is one of the widespread tools for analysis and design of logistics systems.
Today&amp;rsquo;s applications go beyond the traditional usage for systems planning.
They include more operational use-cases such as virtual commissioning or short-term forecasts.
Simulation models are getting integrated tightly into other IT-systems.
This allows to increase process transparency and to improve our means to analyze, control, and optimize system performance in real-time.
Doesn&amp;rsquo;t this sound pretty close to what &lt;em>Digital Twins&lt;/em> always promise? ;)&lt;/p>
&lt;p>Most industrial simulation uses are still based on commercial packages.&lt;br>
However, there are a couple of open-source alternatives, which are typically closer to general-purpose language programming. Even though they tend to lack some convenient commercial features, there are upsides such as better scalability and simplified interfacing.&lt;/p>
&lt;p>Related to Python we became aware of two popular DES packages:
&lt;a href="https://www.salabim.org/" target="_blank" rel="noopener">Salabim&lt;/a> and
&lt;a href="https://simpy.readthedocs.io/en/latest/" target="_blank" rel="noopener">SimPy&lt;/a>.
Both are not only free and open-source, but even built on top of the standard library of one of the world&amp;rsquo;s most popular programming languages - let&amp;rsquo;s see what we can get out of that!&lt;/p>
&lt;!-- omit in toc -->
&lt;h4 id="casymda-package">Casymda-Package&lt;/h4>
&lt;p>Based on &lt;em>SimPy&lt;/em>, we added bits of complementing functionality to gain some of the modeling convenience of commercial &amp;ldquo;block-based&amp;rdquo; DES-packages.&lt;br>
&lt;a href="https://pypi.org/project/casymda/" target="_blank" rel="noopener">&lt;em>Casymda&lt;/em>&lt;/a> facilitates the usage of bpmn-process-descriptions to generate corresponding simulation-model python-code.
.bpmn-files (basically xml) can easily be created with the
&lt;a href="http://www.bpmn.io" target="_blank" rel="noopener">Camunda-Modeler&lt;/a>.&lt;br>
This graphical modeling helps to maintain an overview of the high-level model-structure.
Generated Casymda-models also include a generic, token-like animation of simulated processes out-of-the-box, ready to be run in a web-browser.
For presentation and debugging, animations can be paused and their speed can be changed dynamically.
Solely animation-related events are not scheduled if the simulation is run without visualization.
This maximizes the execution speed - which becomes especially important related to RL, when a high number of runs is necessary.&lt;br>
Further features of Casymda include simulated movements along shortest paths in a 2D-tilemap-space, and gradual typing for development convenience (checkout
&lt;a href="https://github.com/microsoft/pyright" target="_blank" rel="noopener">pyright&lt;/a> if you are using
&lt;a href="https://code.visualstudio.com/docs/python/editing" target="_blank" rel="noopener">vscode&lt;/a>).&lt;/p>
&lt;p>For more info on &lt;em>Casymda&lt;/em> have a look at
&lt;a href="https://github.com/fladdimir/casymda" target="_blank" rel="noopener">the repo&lt;/a> or
&lt;a href="https://casymda.github.io/page/Webpage/Startpage.html" target="_blank" rel="noopener">the (German) website&lt;/a>.&lt;/p>
&lt;h3 id="wrapping-a-des-model-in-a-gym-environment">Wrapping a DES-Model in a Gym-Environment&lt;/h3>
&lt;p>To be able to train an RL-agent inside a simulation model, we need to make the model implementing the Gym-interface described above.&lt;/p>
&lt;p>The following diagram illustrates the coupling concept:&lt;/p>
&lt;p>&lt;img src="diagrams/sim_env_wrapper.jpg" alt="Gym-environment wrapper for a simulation model">&lt;/p>
&lt;p>When the &lt;code>step&lt;/code> function of the Gym-Environment is called (&lt;code>1&lt;/code>), the provided action is propagated to the relevant block of the simulation model (&lt;code>1.1&lt;/code>).
This is realized with help of an &lt;code>ActionHolder&lt;/code>, so that a consuming piece of decision logic can dispatch according to the received information.&lt;/p>
&lt;p>Subsequently, the simulation is executed until a &lt;code>next_action_needed&lt;/code>-Event is triggered by the simulation model (&lt;code>1.2&lt;/code>).
This is indicating the end of the current step and the need for another action of the agent.&lt;/p>
&lt;blockquote>
&lt;p>One &lt;em>Gym-step&lt;/em> can thus comprise an arbitrary number of discrete &lt;em>SimPy-steps&lt;/em>, each of which can in turn take an arbitrary amount of simulated time.&lt;/p>
&lt;/blockquote>
&lt;p>Rewards are managed with help of a &lt;code>RewardHolder&lt;/code> object, which is wired into the relevant blocks of the simulation model during environment initialization.
At the end of each step, occured rewards are collected (&lt;code>1.3&lt;/code>). Depending on the type of the optimization problem to solve, a post-processing of collected rewards can be applied (e.g. taking into account the amount elapsed time, so that an agent can learn time-efficient behavior).&lt;/p>
&lt;p>To check whether an episode ended (the &lt;em>done&lt;/em> part of the returned information), the current state of the model is checked against configured &lt;code>done_criteria&lt;/code> (&lt;code>1.4&lt;/code>).
These can contain e.g. some goals to be reached or a certain amount of time to be simulated.&lt;/p>
&lt;p>To provide the agent with an observation, a model-specific &lt;code>ModelStateToObservationConverter&lt;/code> is used to collect relevant information from the model.
The created observation conforms to the defined &lt;code>observation_space&lt;/code> (&lt;code>1.5&lt;/code>).
This step could include e.g. counting the number of entities in different queues or checking inventory levels and creating a NumPy-array out of this information.&lt;/p>
&lt;p>Finally, collected information is returned to the agent (&lt;code>2&lt;/code>), which can learn based on the reward and decide for the next action.&lt;/p>
&lt;p>Having the basics covered, let&amp;rsquo;s see how we get this to work.&lt;/p>
&lt;h2 id="case-study">Case-Study&lt;/h2>
&lt;p>Back in August of last year at the MIM2019 in Berlin, we had the chance to attend an interesting talk of two Bavarian guys presenting their research on
&lt;a href="https://www.sciencedirect.com/science/article/pii/S2405896319315010" target="_blank" rel="noopener">improving the tour-building for in-plant milk-run systems&lt;/a>.
These internal deliveries are commonly used for assembly line supply, and the tours are typically following a very rigid plan.
Given the fact that the actual demand at the line tends to vary, their research revealed quite a lot of potential to decrease delivery lead times and to increase systems&amp;rsquo; utilization - just by making the tour-planning more dynamic.&lt;/p>
&lt;p>Based on this setting we constructed an abstracted and simplified version of an assembly line with a corresponding material supply system to provide a playground for reinforcement learning algorithms.
The repository is provided
&lt;a href="https://github.com/fladdimir/tugger-routing" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;h3 id="scenario">Scenario&lt;/h3>
&lt;p>The image below shows a schematic layout plan of the system:&lt;/p>
&lt;p>&lt;img src="images/layout_50perc.png" alt="case-study-layout">&lt;/p>
&lt;p>Unfinished products enter the system on the upper right (&lt;code>I&lt;/code>) and are assembled sequentially at 9 different stations, arranged in U-shape (&lt;code>I-IX&lt;/code>).
Finished products leave the system after the last assembly step (&lt;code>IX&lt;/code>).&lt;br>
Stations require a certain amount of resource of either type &lt;code>A&lt;/code> or &lt;code>B&lt;/code> to be present in the station&amp;rsquo;s inventory before an assembly step can start.&lt;br>
Each station can only hold one product at a time, and finished products can only be forwarded once the following station is empty (thus multiple upstream stations holding already finished products may be blocked by downstream stations which are still processing a product or waiting for material before being able to start processing).&lt;br>
Material is supplied by a tugger, able to carry a limited discrete amount (&amp;ldquo;boxes&amp;rdquo;). The tugger can load material at a stock (&lt;code>A&lt;/code> and/or &lt;code>B&lt;/code>, located at the bottom).
1 discrete unit of material (&amp;ldquo;box&amp;rdquo;) can be loaded/unloaded at a time.
The goal of the assembly line is achieving the maximal throughput, which also correlates with small lead-times of products.&lt;/p>
&lt;p>Assumptions:&lt;/p>
&lt;ul>
&lt;li>material can only be loaded at the stocks (&lt;code>A&lt;/code> and &lt;code>B&lt;/code>), each of which holds an infinite amount of material, so that the tugger never waits for material at a loading site&lt;/li>
&lt;li>material can only be unloaded at a station actually requiring this type of material (hence a tugger cannot unload a box of &lt;code>A&lt;/code> at a station which needs &lt;code>B&lt;/code> for assembly)&lt;/li>
&lt;li>the inventory capacity at the stations (&lt;code>I-IX&lt;/code>) is infinte, therefore the tugger never waits at an unloading site (otherwise livelocks could occur where a tugger cannot unload material whereever it moves)&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th align="left">System parameters&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td align="left">Takt-time: processing time per station per product&lt;/td>
&lt;td align="center">60s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">Demand per product of stations type &lt;code>A&lt;/code>&lt;/td>
&lt;td align="center">1.5 units&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">Demand per product of stations type &lt;code>B&lt;/code>&lt;/td>
&lt;td align="center">0.5 units&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">Tugger movement speed&lt;/td>
&lt;td align="center">10 m/s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">Tugger capacity&lt;/td>
&lt;td align="center">25 units&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">Amount of material (un-)loaded per step&lt;/td>
&lt;td align="center">5 units&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">Time needed per (un-)loading step&lt;/td>
&lt;td align="center">5s&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Distances between stocks and stations (higher demands cause more frequent tours):&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th align="center">Relation&lt;/th>
&lt;th align="center">Simple&lt;/th>
&lt;th align="center">Demand-weighted&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td align="center">A -&amp;gt; T1&lt;/td>
&lt;td align="center">1096.40m&lt;/td>
&lt;td align="center">1644.60m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">B -&amp;gt; T2&lt;/td>
&lt;td align="center">926.40m&lt;/td>
&lt;td align="center">463.20m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">A -&amp;gt; T3&lt;/td>
&lt;td align="center">736.40m&lt;/td>
&lt;td align="center">1104.60m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">B -&amp;gt; T4&lt;/td>
&lt;td align="center">566.40m&lt;/td>
&lt;td align="center">283.20m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">A -&amp;gt; T5&lt;/td>
&lt;td align="center">234.10m&lt;/td>
&lt;td align="center">351.15m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">B -&amp;gt; T6&lt;/td>
&lt;td align="center">556.40m&lt;/td>
&lt;td align="center">278.20m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">A -&amp;gt; T7&lt;/td>
&lt;td align="center">726.40m&lt;/td>
&lt;td align="center">1089.60m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">B -&amp;gt; T8&lt;/td>
&lt;td align="center">916.40m&lt;/td>
&lt;td align="center">458.20m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">A -&amp;gt; T9&lt;/td>
&lt;td align="center">1086.40m&lt;/td>
&lt;td align="center">1629.60m&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The table below shows a simple throughput estimation by calculating the average cycle time of the tugger and the expected station utilization.
The estimation assumes &amp;ldquo;full truck loads&amp;rdquo;, always completely loading at one stock (either &lt;code>A&lt;/code> or &lt;code>B&lt;/code>), and fully unloading at a station (&lt;code>T1&lt;/code> - &lt;code>T9&lt;/code>).&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th align="left">Throughput estimation&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td align="left">Max throughput 24h&lt;/td>
&lt;td align="center">60/h x 24h = 1440&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">Demand / product&lt;/td>
&lt;td align="center">9.5 units&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">Demand / time&lt;/td>
&lt;td align="center">9.5 / 60s = 0.16/s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">Average weighted distance&lt;/td>
&lt;td align="center">811.37m&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">Average driving time&lt;/td>
&lt;td align="center">81.137s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">(Un-)loading time 25 units&lt;/td>
&lt;td align="center">25s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">Average cycle time&lt;/td>
&lt;td align="center">(81.137s + 25s) x 2 = 212.274s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">Delivered units / cycle&lt;/td>
&lt;td align="center">25&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">Delivered units / time&lt;/td>
&lt;td align="center">0.12/s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">Average utilization&lt;/td>
&lt;td align="center">0.12/s / 0.16/s = 75%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">Expected throughput per min&lt;/td>
&lt;td align="center">75% x 60/min = 45/min&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">Expected throughput per 24h&lt;/td>
&lt;td align="center">~1080/24h&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>As we can see, the delivery performance of the tugger represents the limiting factor (bottleneck) of the system, which means that each improvement made here will be directly reflected by a corresponding increase in the overall throughput.&lt;br>
For the sake of simplicity, no stochastic model behaviour (such as e.g. randomly distributed loading or movement times) is assumed, hence the simulation model will be deterministic.&lt;br>
As stated: the system as a whole is quite abstracted and simplified - but still capturing at least some of the basic complexity inherent to real-world problems.
Will our RL-agent be able to&amp;hellip;&lt;/p>
&lt;ul>
&lt;li>grasp the underlying mechanics?&lt;/li>
&lt;li>distinguish different product types?&lt;/li>
&lt;li>discover the spots of demand and supply?&lt;/li>
&lt;li>deal with the limits of the tugger&amp;rsquo;s capacity?&lt;/li>
&lt;li>reach the maximal possible throughput?&lt;/li>
&lt;/ul>
&lt;p>We&amp;rsquo;ll find out, but let&amp;rsquo;s first have a look at what the learning environment will look like.&lt;/p>
&lt;h3 id="simulation-model">Simulation Model&lt;/h3>
&lt;p>The
&lt;a href="https://github.com/fladdimir/tugger-routing/tree/github/tugger_src/gym_env/des_model" target="_blank" rel="noopener">simulation model&lt;/a> of the system basically consists of 2 processes, both depicted in the graphic below.&lt;/p>
&lt;p>&lt;img src="images/model.jpeg" alt="case-study-processes">&lt;/p>
&lt;p>On the left side, products pass through the 9 assembly steps (&lt;code>ProductStation&lt;/code>, rotated U-shape) before leaving the system, occasionally being blocked by downstream stations or waiting for material at a station.&lt;/p>
&lt;p>On the right side the tugger passes through an infinite cycle of movement and loading/unloading process steps (after initial creation at location &lt;code>A&lt;/code> by a &lt;code>TuggerSource&lt;/code>):&lt;/p>
&lt;ol>
&lt;li>the next movement target is chosen and the movement is completed (no actual movement if the next target equals the current location) (&lt;code>TuggerMovement&lt;/code>).&lt;/li>
&lt;li>Depending on the current location (being either a stock &lt;code>A&lt;/code>/&lt;code>B&lt;/code>) or a &lt;code>ProductStation&lt;/code>, the next tugger process step is chosen:
&lt;ol>
&lt;li>&lt;code>TuggerStock A&lt;/code> loading of one unit of &lt;code>A&lt;/code> (if tugger-capacity not reached)&lt;/li>
&lt;li>&lt;code>TuggerStock B&lt;/code> loading of one unit of &lt;code>B&lt;/code> (if tugger-capacity not reached)&lt;/li>
&lt;li>&lt;code>TuggerStation&lt;/code> unloading of one unit of &lt;code>A&lt;/code> or &lt;code>B&lt;/code> if possible (material required by station is loaded)&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;p>Note that even unsuccessful loading or unloading attempts are implemented to take a small, fixed amount of time, so that every possible Gym-step is guaranteed to take at least some simulated time (and a time-constrained episode is guaranteed to reach it&amp;rsquo;s end eventually).&lt;/p>
&lt;p>Below you can see a process animation, as well as an animation of a tilemap.
The agent here follows an explicitly defined simple rule of always delivering a complete load of 25 units to the station with the lowest inventory level.
To run the animation just clone
&lt;a href="https://github.com/fladdimir/tugger-routing" target="_blank" rel="noopener">the repository&lt;/a>.&lt;/p>
&lt;p>Process animation:&lt;/p>
&lt;pre>&lt;code class="language-sh">docker-compose up web-animation-lia-process
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="gifs/lia_process.gif" alt="lia-process">&lt;/p>
&lt;p>Tilemap animation:&lt;/p>
&lt;pre>&lt;code class="language-sh">docker-compose up web-animation-lia
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="gifs/lia_tilemap.gif" alt="lia-tilemap">&lt;/p>
&lt;h3 id="preparing-the-gym-environment">Preparing the Gym-Environment&lt;/h3>
&lt;p>The &lt;code>TuggerEnv&lt;/code> implements the Gym-Env interface and wraps the simulation model to be used for RL-agent training.&lt;/p>
&lt;p>Generic functionalities like the mandatory &lt;code>step&lt;/code> and &lt;code>reset&lt;/code> functions and related helper methods are inherited and abstract/default parent-methods are overridden in a model-specific way as required (&lt;em>Template-Method Pattern&lt;/em>):&lt;/p>
&lt;ul>
&lt;li>&lt;code>initialize_action_and_reward_holder&lt;/code> specifies which model blocks&amp;hellip;
&lt;ul>
&lt;li>need access to gym-actions: &lt;code>TilemapMovement&lt;/code>, choosing the next movement target based on the supplied target index number&lt;/li>
&lt;li>log achieved rewards: &lt;code>ProductSink&lt;/code>, simply counting a reward of 1 for each finished product&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>get_reward&lt;/code> specifies how the elapsed time is taken into account for reward calculation&lt;/li>
&lt;li>&lt;code>check_if_model_is_done&lt;/code> implements a model-specific check whether a certain amount of time has been simulated. One episode is scheduled to take 24h (86400s).&lt;/li>
&lt;/ul>
&lt;p>The &lt;code>render&lt;/code> method of the Gym-Env is not implemented, since animations at arbitrary moments in time - whenever a Gym-step is finished - do not make much sense for discrete event simulation environments. Animation is therefore controlled seperately.&lt;/p>
&lt;p>The &lt;code>info&lt;/code> return value of &lt;code>step&lt;/code> is configured to return the number of &lt;code>finished_products&lt;/code> which can then be logged.&lt;/p>
&lt;!-- omit in toc -->
&lt;h4 id="observation---action-space">Observation- &amp;amp; Action-Space&lt;/h4>
&lt;p>The model-specific extraction of the observation from the current model state is done by an instance of a &lt;code>TuggerEnvModelStateConverter&lt;/code> which implements the &lt;code>ModelStateConverter&lt;/code> &amp;ldquo;interface&amp;rdquo;.&lt;/p>
&lt;p>Specifically, the observation consists of the following information which describes the current state of the system (overall 48 values):&lt;/p>
&lt;ul>
&lt;li>&lt;code>ProductStation&lt;/code> observations (5 values x 9 stations = 45 values):
&lt;ul>
&lt;li>current inventory-level (normalized 0-1, counted up to a limit of 10 units)&lt;/li>
&lt;li>&lt;em>busy&lt;/em>-state (binary)&lt;/li>
&lt;li>&lt;em>waiting_for_material&lt;/em>-state (binary)&lt;/li>
&lt;li>&lt;em>empty&lt;/em>-state (binary, whether a product is present or not)&lt;/li>
&lt;li>&lt;em>blocked-by-successor&lt;/em>-state (binary)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>TuggerEntity&lt;/code> observations (3 values x 1 tugger = 3 values):
&lt;ul>
&lt;li>loaded amount of &lt;code>A&lt;/code> (relative to capacity)&lt;/li>
&lt;li>loaded amount of &lt;code>B&lt;/code> (relative to capacity)&lt;/li>
&lt;li>current location (index)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Note that parts of a station observation can be seen to be redundant (e.g. a station which is neither &lt;em>busy&lt;/em> nor &lt;em>waiting&lt;/em> nor &lt;em>empty&lt;/em> can only be &lt;em>blocked&lt;/em>) - behind lies the rationale that an intelligent algorithm will (hopefully) learn an importance of different components of an observation, so that we do not have to worry about more than providing all potentially useful information.&lt;/p>
&lt;p>The &lt;code>action_space&lt;/code> (of type &lt;code>gym.spaces.Discrete&lt;/code>) consists of the 11 possible movement targets (9 stations + 2 stocks, encoded by index).&lt;/p>
&lt;!-- omit in toc -->
&lt;h4 id="rewards">Rewards&lt;/h4>
&lt;p>As stated above, the defined goal of the assembly line is to achieve the best possible throughput of products, which corresponds to producing as many products as possible e.g. during one episode (24h).&lt;/p>
&lt;p>How do we achieve that? Which kind of incentive is suitable to stimulate such a behavior?
The design of appropriate reward functions is known to be a non-trivial matter.
In fact, the design of rewards and incentives even for (arguably more intelligent) humans is a major problem in education and management (remember the last time you studied for passing an exam instead of actually learning useful contents ;).&lt;/p>
&lt;p>For the environment at hand, we could just think about giving a single reward at the end of each episode, proportionally to the number of achieved products in that fixed amount of time (24h), which would probably properly reflect our aim of maximizing the throughput.
However, the resulting reward would be quite &lt;em>sparse&lt;/em> and therefore greatly decelerate learning speed (taking the average duration of a random action, each episode would take more than 1000 actions to complete before an agent sees any reward).&lt;/p>
&lt;p>Another idea would be to reward every successful delivery of material to any station, which would be possible to be completed within 2 steps (movement to the stock &amp;amp; movement to a suitable station consuming the loaded material).
This way we would get less sparse rewards, but also an obvious problem of exploitability, caused by the fact that the delivery of material to one station alone would actually never lead to the completion of any product at all.&lt;/p>
&lt;p>As a compromise, we simply decided to go for a a reward of 1 everytime a product completes its final assembly step, which is possible be completed within 12 steps (minimum, not necessarily an optimal strategy).
Even exhibiting a random behavior, this would allow an agent to generate a reward of around 50 during one episode, so that there are sufficient &amp;ldquo;randomly succesful&amp;rdquo; samples to learn from.&lt;/p>
&lt;p>One problem of this reward is that the simulated time needed to obtain a reward is not reflected in the reward itself.
Since every gym-step can actually eat up a greatly varying amount of simulation time (from 5 seconds to &amp;gt;100), there is a huge implicit impact on the throughput, which the agent is unaware of.
To solve this problem we introduced &amp;ldquo;costs of time&amp;rdquo;, which means we simply give a small negative reward every step, proportional to the amount of simulated time that passed.
This finally leaves us with the subsequent question of how big these &amp;ldquo;costs&amp;rdquo; should be.
If set too high, they would just overrule any of the few actual rewards in the beginning of the training.
If put too low, there would not be sufficient stimulus to exhibit time-efficient behavior at all.
Again, as a simple compromise, we implemented the costs to grow proportionally with the highest reward seen so far at the end of an episode, which guarantees a certain balance, and rewards increasing time-efficiency.&lt;/p>
&lt;p>The above described reward that we designed is definitely not &amp;ldquo;perfect&amp;rdquo; and also feels a bit like putting too much effort into &amp;ldquo;reward engineering&amp;rdquo; - nevertheless its a first solution our agents can hopefully work with&amp;hellip;&lt;/p>
&lt;h3 id="rl-agent-training--evaluation">RL-Agent Training &amp;amp; Evaluation&lt;/h3>
&lt;p>The environment presented above is characterized by a &lt;em>Discrete&lt;/em> action space and a continuous (&lt;em>Box&lt;/em>) observations space.
&lt;a href="https://stable-baselines.readthedocs.io/en/master/guide/algos.html" target="_blank" rel="noopener">The stable-baselines documentation&lt;/a> lists available RL algorithms and their compatibility.&lt;br>
Due to the type of action space, some algorithms are not feasible (i.e. DDPG, SAC, and TD3).&lt;/p>
&lt;p>To train a stable-baselines RL algorithm, the &lt;code>TuggerEnv&lt;/code> is vectorized, using a &lt;code>DummyVecEnv&lt;/code> and a standard &lt;code>MlpPolicy&lt;/code>.
To leverage multiple CPUs for training, it can be desirable to use a &lt;code>SubprocVecEnv&lt;/code> (but for simpler logging &amp;amp; analysis we did not go with that one here, instead we did multiple independent training runs in parallel).&lt;/p>
&lt;p>Train an ACER-agent (by default for 10,000 steps only, which should take &amp;lt;1min):&lt;/p>
&lt;pre>&lt;code class="language-sh">docker-compose up acer-training
&lt;/code>&lt;/pre>
&lt;p>Plot performance (might require additional steps for connecting the display):&lt;/p>
&lt;pre>&lt;code class="language-sh">docker-compose up acer-plot-training
&lt;/code>&lt;/pre>
&lt;p>Tilemap-animation of the trained agent (visit &lt;a href="http://localhost:5000">http://localhost:5000&lt;/a>):&lt;/p>
&lt;pre>&lt;code class="language-sh">docker-compose up acer-web-animation-tilemap
&lt;/code>&lt;/pre>
&lt;p>Below we can see an ACER-agent trained for 1m steps:&lt;/p>
&lt;p>&lt;img src="gifs/tilemap_acer_1e6.gif" alt="tilemap_acer_1e6">&lt;/p>
&lt;blockquote>
&lt;p>As we can see, the agent manages to fully load the 25 units onto the tugger most of the time, seems to target correct (A/B) stations for material unloading, and the choice of stations with a currently low inventory level seems reasonable too!&lt;/p>
&lt;/blockquote>
&lt;p>But how does the overall performance look like?&lt;/p>
&lt;h3 id="performance-comparison">Performance Comparison&lt;/h3>
&lt;p>For comparison we trained four algorithms (ACER, ACKTR, DQN, and PPO2) with standard settings for both 1 and 3 mio. (Gym-)steps.
Training took up to 2.5 hours (DQN, 3mio. steps) on a 2.9GHz Intel i9, using a single-process DummyVecEnv as explained above.&lt;br>
The following graph shows the number of produced products per episode (24h) over the course of the training run for each algorithm, as well as the performance of the deterministic lowest-inventory heuristics (yellow line; always delivering a complete load of 25 units to the station with the currently lowest inventory), and the average performance of fully random actions (turquoise line, measured over 100 episodes).&lt;/p>
&lt;p>&lt;img src="graphs/training_progress.png" alt="performance-comparision-graph">&lt;/p>
&lt;ul>
&lt;li>As we can see, all of the algorithms manage to increase the number of produced products per episode significantly above the level reached by random actions (turquoise line at the bottom), indicating successful learning progress.&lt;/li>
&lt;li>Furthermore, none of the trained algorithms reaches the performance of the lowest-inventory-heuristics (yellow line at the top).&lt;/li>
&lt;li>The lowest-inventory-heuristics performance reaches the estimated maximum possible throughput of the system (estimated to appr. 1080/episode). This strategy can therefore be considered to be close to a global optimum.&lt;/li>
&lt;li>During training, a complete breakdown in performance can occur. Most prominently: ACER_3mio. (blue line, episode 260, no recovery at all). Other algorithms show drops in performance as well but seem to recover better (e.g. ACKTR - green, PPO2 - pink).&lt;/li>
&lt;li>The best-performing RL algorithm (ACER trained for 1mio. steps, orange line) reached a maximum throughput of 856 products / episode (78% of the near-optimal heuristics performance).&lt;/li>
&lt;/ul>
&lt;p>The number of episodes varies due to the variable number of Gym-steps per episode (24h of simulated time), depending on the simulated time each Gym-step needs.
The small number of episodes of the ACER_3mio. training is explained by the up to 17277 Gym-steps per episode, occuring from episode 260 on.
Each step of such an episode takes only 5 seconds (the minimum possible time a repeated visit of the same location takes).
This behavior might be caused by the defined negative reward per step, proportional to the amount of simulated time the step needed.
Appearently, the agent does not remember how to generate a positive reward and only tries to maximize the short-term reward by minimizing the step-time.
Obviously this behavior does not lead to any successful delivery, let alone completion of any product.&lt;/p>
&lt;p>It is worth to be mentioned that all training runs were done with default algorithm settings, and that the evaluation of different hyperparameters is strongly
&lt;a href="https://stable-baselines.readthedocs.io/en/master/guide/rl_tips.html" target="_blank" rel="noopener">recommended&lt;/a> for performance optimization. Thus, it might not be improbable for an RL agent to close the performance gap towards the theoretically reachable optimum.&lt;/p>
&lt;h2 id="summing-up">Summing Up&lt;/h2>
&lt;p>Short version: Our best RL agent reached about 78% of the best possible performance inside our production-logistics environment.&lt;/p>
&lt;p>Ok, now is this good or bad?&lt;/p>
&lt;p>Well, one could be disappointed by the fact that our agent was not able to reach the performance of a hand-coded heuristics approach.&lt;br>
But did we believe when we started out that we could get a generic piece of code to cope with the non-trivial relations of our specific and fairly complex environment? Certainly not!&lt;br>
And this was just a first shot - we did not even start with hyperparameter tuning or the evaluation of alternative rewards.&lt;/p>
&lt;p>What do your experiences with reinforcement learning look like?&lt;br>
Which logistics problems did you solve with RL?&lt;br>
Did you spot a bug somewhere in the code or do you want to suggest an improvement?&lt;br>
Or do you have questions concerning the presented implementation/toolstack?&lt;/p>
&lt;p>Just feel free to drop us a note, thanks for reading!&lt;/p>
&lt;p>
&lt;a href="https://www.linkedin.com/in/wladimir-hofmann-9065a714b/" target="_blank" rel="noopener">Wladimir Hofmann&lt;/a> -
&lt;a href="https://www.linkedin.com/in/clemens-lennart-schwarz-609815186/" target="_blank" rel="noopener">Clemens L. Schwarz&lt;/a> -
&lt;a href="https://www.linkedin.com/in/fredrik-branding-064014163/" target="_blank" rel="noopener">Fredrik Branding&lt;/a>&lt;/p></description></item></channel></rss>